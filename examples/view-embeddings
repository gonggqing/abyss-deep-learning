#!/usr/bin/env python3
import argparse
import os
import sys

import numpy as np
import PIL.Image
try:
    import tensorflow.compat.v1 as tf
    tf.disable_v2_behavior()
except:
    import tensorflow as tf
from pycocotools.coco import COCO
import cv2

try:
    from tensorboard.plugins import projector
except:
    from tensorflow.contrib.tensorboard.plugins import projector
#from abyss_deep_learning.keras.tensorboard import produce_embeddings_tsv
import json
import pandas as pd
from tqdm import tqdm
from scipy.stats import chi2

from abyss_deep_learning.utils import imread

def images_to_sprite(data):
    """Creates the sprite image along with any necessary padding

    Args:
      data: NxHxW[x3] tensor containing the images.

    Returns:
      data: Properly shaped HxWx3 image with any necessary padding.
    """
    if len(data.shape) == 3:
        data = np.tile(data[..., np.newaxis], (1, 1, 1, 3))
    data = data.astype(np.float32)
    min = np.min(data.reshape((data.shape[0], -1)), axis=1)
    data = (data.transpose(1, 2, 3, 0) - min).transpose(3, 0, 1, 2)
    max = np.max(data.reshape((data.shape[0], -1)), axis=1)
    data = (data.transpose(1, 2, 3, 0) / max).transpose(3, 0, 1, 2)
    # Inverting the colors seems to look better for MNIST
    #data = 1 - data

    n = int(np.ceil(np.sqrt(data.shape[0])))
    padding = ((0, n ** 2 - data.shape[0]), (0, 0), (0, 0)) + ((0, 0), ) * (data.ndim - 3)
    data = np.pad(data, padding, mode='constant', constant_values=0)
    # Tile the individual thumbnails into an image.
    data = data.reshape(
        (n, n) + data.shape[1:]).transpose((0, 2, 1, 3) + tuple(range(4, data.ndim + 1)))
    data = data.reshape((n * data.shape[1], n * data.shape[3]) + data.shape[4:])
    data = (data * 255).astype(np.uint8)
    return data

def save_metadata_to_tsv(save_path, meta_df):
    meta_df.to_csv(save_path, sep='\t', index=False)

def save_embeddings(log_dir, features, metadata=None, sprite=None, sprite_shape=None):
    """
    Save the embeddings to the tensorboard directory
    Args:
        log_dir: (str) The tensorboard directory
        features: (np.ndarray) The features to visualise
        metadata: (pd.DataFrame) The metadata associated with the features
        sprite: (np.ndarray) The sprite image
        sprite_shape: (tuple) The shape of each sprite image.

    Returns:

    """
    metadata_path = os.path.join(log_dir, 'metadata.tsv')
    features = tf.Variable(features, name='features')
    if metadata is not None:
        save_metadata_to_tsv(metadata_path, metadata)
    # produce_embeddings_tsv(os.path.join(log_dir, 'metadata.tsv'), headers=['label'], labels=labels)

    #if labels is not None:
    #    with open(metadata, 'w') as metadata_file:
    #        for row in labels:
    #            metadata_file.write('%d\n' % row)
    if sprite is not None:
        PIL.Image.fromarray(sprite).save(os.path.join(log_dir, 'sprite.png'))
    with tf.Session() as sess:
        saver = tf.train.Saver([features])
        sess.run(features.initializer)
        saver.save(sess, os.path.join(log_dir, 'features.ckpt'))

        config = projector.ProjectorConfig()
        # One can add multiple embeddings.
        embedding = config.embeddings.add()
        embedding.tensor_name = features.name
        if metadata is not None:
            embedding.metadata_path = metadata_path
        if sprite is not None:
            embedding.sprite.image_path = os.path.join(log_dir, 'sprite.png')
            embedding.sprite.single_image_dim.extend(sprite_shape)
        # Saves a config file that TensorBoard will read during startup.
        projector.visualize_embeddings(tf.summary.FileWriter(log_dir), config)

def say(*args):
    print(*args, file=sys.stderr)

def image_arg_to_coco(image_arg, do_patches=False, from_video=False, patch_size=[512,512]):
    """Gets the image argument and converts it to a COCO format dataset.
    The image argument can sequence of images, or a sequence of videos

    Args:
        image_arg (list): The image/video arguments
        do_patches (bool): Whether to extract patches from the given image_arg.
        from_video (bool): Whether to extract frames from the given image_arg.

    Returns:
        COCO: The coco dataset

    """
    image_list = []
    if do_patches:
        # we will subsample the images, and create multiple patches per image
        n = 0
        for _, ipath in enumerate(tqdm(image_arg)):
            with PIL.Image.open(ipath) as img:
                width, height = img.size
                # work out sub-sampling stats
                num_patches = (width // patch_size[0]) * (height // patch_size[1])
                for m in range(num_patches):
                    patch_path = os.path.splitext(ipath)[0] + \
                                                    '.' + str(patch_size[0] *  np.mod(m, width // patch_size[0])) + \
                                                    '.' + str(patch_size[0] * ( m // (width // patch_size[1]))) + '.png'
                    image = {
                        'filename': ipath,
                        'path': os.path.basename(patch_path),
                        'height': patch_size[0],
                        'width': patch_size[1],
                        'id': n
                    }
                    image_list.append(image)
                    n = n + 1
        dataset = {'images': image_list, 'categories': [], 'annotations': []}
        coco = COCO()
        coco.dataset = dataset
        coco.createIndex()

    elif from_video:
        # we will subsample the videos, and ripping the frames
        n = 0
        for _, ipath in enumerate(tqdm(image_arg)):
            cap = cv2.VideoCapture(ipath)
            fps = cap.get(cv2.CAP_PROP_FPS)
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) # this is an issue, since cv2 DOES NOT RETURN THE CORRECT FRAME COUN
            duration = frame_count / fps
            # TODO: dont hardcode this
            n_frames = int(fps * 10) # this gets a frame every 10 seconds
            n_frames = int(1)  # this gets a frame every 10 seconds
            for frame in range(0, frame_count - 1, n_frames):
                patch_path = os.path.splitext(ipath)[0] + \
                             '.' + str(frame) + '.png'
                image = {
                    'filename': ipath,
                    'path': os.path.basename(patch_path),
                    'height': patch_size[1],
                    'width': patch_size[0],
                    'id': n
                }
                image_list.append(image)
                n = n + 1
            cap.release()

        dataset = {'images': image_list, 'categories': [], 'annotations': []}
        coco = COCO()
        coco.dataset = dataset
        coco.createIndex()

    else:
        # input is assumed to be a list of images, and a coco will be created for it
        for n, ipath in enumerate(image_arg):
            with PIL.Image.open(ipath) as img:
                width, height = img.size
            image = {
                'filename': os.path.basename(ipath),
                'path': ipath,
                'height': height,
                'width': width,
                'id': n
            }
            image_list.append(image)
        dataset = {'images': image_list, 'categories': [], 'annotations': []}
        coco = COCO()
        coco.dataset = dataset
        coco.createIndex()
    return coco

def get_data_from_coco(coco, args, load_images=True):
    """
    Creates a dictionary of data from the COCO file. Includes extracting category information.
    Args:
        coco: (pycocotools.coco.COCO) The COCO format dataset
        args: (ArgumentParser) The command line arguments
        load_images: (bool) Whether to load the images.

    Returns:
        dict: A dictionary containing the data extracted from the COCO file.

    """
    img_list = []
    hots = []
    img_ids = []
    paths = []
    is_labelled_list = []
    labelled_class_list = []

    # TODO: make this configurable
    class_map = '/mnt/pond/scratch/cctv-ml-experiments/fault-detection/057.fault-no-fault/fault-detection.category-map.json'
    raw_cat_map = json.load(
        open(class_map, 'r'))  # Load the caption map - caption_map should live on place on servers
    cat_map = {}
    for k, v in raw_cat_map.items():
        cat_map[int(k)] = v

    for image in tqdm(coco.loadImgs(coco.getImgIds())):
        if load_images:
            img = imread(image['path'], size=args.image_size, dtype=np.uint8)
            hot = np.zeros([len(coco.getCatIds())])
            for ann in coco.loadAnns(coco.getAnnIds(imgIds=[image['id']])):
                hot[ann['category_id']] = 1

            if len(coco.loadAnns(coco.getAnnIds(imgIds=[image['id']]))) > 0:
                is_labelled_list.append(True)

            img_list.append(img)
            hots.append(hot)
            img_ids.append(image['id'])
            paths.append(image['path'])
            labelled_class_list.append(cat_map[ann['category_id']])

        else:  # This is needed for patches
            paths.append(image['path'])
            img_ids.append(image['id'])
            is_labelled_list.append(False)

    data = {
        'imgs': img_list,
        'ImgIds': img_ids,
        'Path': paths,
        'Labelled': is_labelled_list
    }
    if load_images:
        data['class'] = labelled_class_list

    hots = np.asarray(hots)
    cid_to_name = {cat['id']:cat['name'] for cat in coco.loadCats(coco.getCatIds())}
    for k,v in cid_to_name.items():
        data[v] = hots[:, k]

    return data

def create_metadata_df(data, args):
    """
    Creates the metadata dataframe from the dictionary extracted from the coco file
    Args:
        data: (dict) The dictionary of data extracted from the coco file.
        args: (ArgumentParser) The command line arguments.

    Returns:
        pd.DataFrame: A dataframe containing the relevant information

    """
    meta = data
    meta['FileName'] = [os.path.basename(path) for path in data['Path']]
    try:
        del meta['imgs']
        del meta['Path']
    except KeyError:
        pass

    meta_df = pd.DataFrame.from_dict(meta)

    meta_df = add_additional_metadata(meta_df, args)
    return meta_df

def add_additional_metadata(meta_df, args):
    """
    Adds additional metadata and matches it to the existing CSV by spherical name

    Args:
        meta_df: (pd.DataFrame) the metadata df of the predicted data.
        args: (ArgumentParser) The command line arguments.

    Returns:
        pd.DataFrame:

    """
    if not args.metadata_csv:
        return meta_df
    add_df = pd.read_csv(args.metadata_csv)
    # add_df.loc[:, 'filename_index'] = add_df['# PhotoID'].apply(lambda x: x[:-4]) # quick and dirty

    # Check if metadata is extracted using the '# PhotoID' method. # TODO this method doesn't work.
    if '# PhotoID' in add_df.columns.values:
        def split_extension_on_row(row):
            just_fname = os.path.splitext(row['# PhotoID'])[0]
            return just_fname

        add_df.loc[:, 'filename_index'] = add_df.apply(split_extension_on_row, axis=1)

    # Gets the spherical name from the full filename
    def index_column_from_row(row):
        return "_".join(row['FileName'].split('.')[0].split('_')[:-1])

    # Set the spherical name
    meta_df.loc[:, 'filename_index'] = meta_df.apply(index_column_from_row, axis=1)

    output_df = meta_df.merge(add_df, left_on='filename_index', right_on='SphericalName', how='left', )

    return output_df

def compute_distance(vectorlist1, vectorlist2, method='euclidean', inverse_covariance_matrix=None):
        '''
        compute the distance between two vectors.
        :param vector1: a list of vectors
        :param vector2: another list of vectors
        :param method: method of distance computation. Choices are: ['euclidean', 'mahalanobis']
        :return: the distance between the two vectors
        '''
        # TODO: bregman distance/divergence ? - http://mark.reid.name/blog/meet-the-bregman-divergences.html
        if method is 'euclidean':
            # implementation: https://stackoverflow.com/questions/1401712/how-can-the-euclidean-distance-be-calculated-with-numpy
            return [np.linalg.norm(vectorlist1[count].squeeze() - vectorlist2[count].squeeze()) for count, _ in enumerate(tqdm(vectorlist1))]
        elif method is 'mahalanobis':  # a measure of the distance between a point P and a distribution D.
            # implementation: https://stackoverflow.com/questions/27686240/calculate-mahalanobis-distance-using-numpy-only
            e = vectorlist1 - vectorlist2
            D = np.sqrt(np.sum(np.dot(e, inverse_covariance_matrix) * e, axis=1))
            if np.any(np.isnan(D)):
                import warnings
                warnings.warn("A computed distance in nan. This probably means that your covariance matrix is "
                              "semi-definite. You can fix this by including more observations when computing the "
                              "covariance matrix.")
            return D
        else:
            raise(ValueError, "No other distance methods are implemented yet. Please use \'euclidean\', or "
                              "\'mahalanobis\' only.")

def compute_feature_stats(features, labels=None):
    '''
    compute the centroids and covariance matrices for each class
    :param features: [N, M], where N is the number of features, and M is the feature vector length
    :param labels: [N, 1], where N is the number of features
    :return: (class_centroids, class_covariances), where:
        class_centroids - [C, M], where is the centroid of of features in each class, and C is number of classes
        class_covariances - [C, M, M], where is the convariance matrix of features for each class
    '''
    class_centroids = []
    class_covariances = []
    if labels is None:
        labels = np.zeros((len(features), 1))
    for c in set(labels):
        # compute the mean embedding vector (centroid) for each class, and the covariance matrix
        centroid = np.mean(features[labels == c], axis=0)
        centroid_cov = np.cov(features[labels == c].T)
        if centroid.shape[0] == 0:
            raise (ValueError, 'Something bad happened. There are no instances of class ' + str(c) + ' in '
                                                                                                     'the labels!')
        if not np.all(np.linalg.eigvals(centroid_cov) > 0):
            import warnings
            warnings.warn('A computed covariance matrix is not postive definite! This is likely because there is not'
                          'enough examples to properly estimate it. It is highly recommended that more features are used'
                          'for computing the class statistics.')

        class_centroids.append(centroid)
        class_covariances.append(centroid_cov)
    return class_centroids, class_covariances

def get_principal_components_per_class(convariance_matrix_per_class, percent_variance_to_capture=90):
    '''
    find the components in features that explain XX% of the variance.
     This is an estimate of the number of degrees of freedom.
    :param convariance_matrix_per_class:
    :param percent_variance_to_capture: percent of variance to capture by the most significant features.
    :return:
    '''
    principal_components_idx = []
    for n in range(len(convariance_matrix_per_class)):
        correlation = np.corrcoef(convariance_matrix_per_class[n])
        eig_vals, eig_vecs = np.linalg.eig(correlation)
        eig_vals_sorted = np.sort(eig_vals)[::-1]
        explained_variance = [(i / sum(eig_vals)) * 100 for i in eig_vals_sorted]
        if n == 0:  # quick and dirty, only estimate the deg freedom once, since having different ones will skew distance calculations
            num_degrees_freedom = np.sum(np.cumsum(explained_variance) <= percent_variance_to_capture)
            if num_degrees_freedom is 0:
                import warnings
                warnings.warn(("The percent variance to capture is too low. No variance was captured at all. At least "
                              "one degree of freedom explains % 5.2f \% of the variance. At least one degree of freedom"
                               "is instead selected" %(np.cumsum(explained_variance)[0])))
                num_degrees_freedom = 1

        principal_components_idx.append(np.sort(np.argpartition(eig_vals, -num_degrees_freedom)[
                                         -num_degrees_freedom:]))  # get the top N, linear time sort, worst case

    return principal_components_idx

def pool_features(features, pooling=None):
    '''
    Pool features in required, plus does some checks to make sure the features are vectors
    :param features:
    :param pooling: method of pooling, options are 'avg, 'max', or None
    :return: list of optionally pooled feature vectors
    '''
    if len(features.shape) > 2:
        raise ValueError(
            'It looks like the extracted features are not the right shape; they are not a vector. '
            'This is probably because you are not using pooling to vectorise them. Try: --pooling "avg" '
            'or --pooling "max".')
    else:
        if pooling is 'avg':
            features = np.mean(np.mean(features, axis=1), axis=1)  # compute the mean embedding vector across all pixels
        elif pooling is 'max':
            features = np.max(np.max(features, axis=1), axis=1)  # compute the max embedding vector across all pixels
        elif pooling is not None:
            raise ValueError("Only avg,max pooling methods are supported")

    return features

def main(args):
    thumbs_list = []
    features_list = []
    df_list = []
    predictions = None

    if args.images[0].endswith('.txt'):
        args.images = [l.rstrip('\n') for l in open(args.images[0]).readlines()]

    say("Loading model...")
    if args.model_definition is not None:
        model = tf.keras.models.model_from_json(open(args.model_definition).read(), custom_objects={'tf': tf})
        model.load_weights(args.model_weights)
    else:
        model = tf.keras.models.load_model(args.model_weights, custom_objects={'tf': tf})

    layer_name = args.layer_name
    if layer_name is not None:
        output = model.get_layer(layer_name).output
        model = tf.keras.models.Model(inputs=model.input,
                                  outputs=[model.output, output]) # get the logits, plus the layer_name as output
    say("Model loaded!")

    if args.coco:
        say("")
        say("Predicting on images in coco dataset.")
        coco = COCO(args.coco) # Load coco
        say("Loading images...")
        coco_data = get_data_from_coco(coco, args, load_images=True) # Get the data from coco
        images = np.array(coco_data['imgs'], dtype=np.uint8)
        say("Creating thumbnails of {:d} images".format(len(images)))
        thumbs = np.array([
            np.array(PIL.Image.fromarray(image).resize(args.thumb_size))
            for image in images], dtype=np.uint8) # Create the thumbnails
        images = images.astype(np.float32) / 127.5 - 1 # Rescale images

        say("Predicting on coco images...")
        model_output = model.predict(images)

        if isinstance(model.output, list): # unpack list
            predictions = model_output[0].argmax(axis=1)
            features = model_output[1]
        else:
            features = model_output

        # Get mean features
        features = pool_features(features, args.pooling)

        if predictions is not None:
            coco_data['prediction'] = predictions
            if args.compute_class_centroids: # compute the centroids for each class
                say("Computing class centroids and covariances...")
                class_centroids, class_covariances = compute_feature_stats(features, np.asarray(coco_data["class"]))

                # save them
                np.savetxt(os.path.join(args.tensorboard_dir, 'class-centroids.csv'), class_centroids,
                           delimiter=',')
                class_covariances = np.asarray(class_covariances)
                np.savetxt(os.path.join(args.tensorboard_dir, 'class-covariances.csv'),
                           class_covariances.reshape(class_covariances.shape[0],
                                                     class_covariances.shape[1] * class_covariances.shape[2]),
                           delimiter=',')

                say("Computing centroid distances and probabilities...")
                distances = np.zeros((len(class_covariances), len(predictions)))
                main_features_idx = get_principal_components_per_class(class_covariances,
                                                                       percent_variance_to_capture=80)
                class_principal_covariances = []
                for m, covariance in enumerate(class_covariances):
                    # re-sample the covariance matrix using only the principal components?
                    # NOTE: Is this mathematically meaningful? Should instead re-sample from the total convariance
                    # matrix? Is THAT mathematically meaningful?
                    for n, c in enumerate(set(coco_data["class"])):
                        if n == m:
                            # compute the distances using the principal components only
                            class_principal_covariances.append(np.linalg.inv(np.cov(
                                                                features[np.asarray(coco_data["class"]) == c][:,
                                                                main_features_idx[m]].T)))
                            distances[m] = compute_distance(features[:, main_features_idx[m]],
                                                            np.tile(class_centroids[m][main_features_idx[m]],
                                                                    (len(features[:, main_features_idx[m]]), 1)),
                                                            method='mahalanobis',
                                                            inverse_covariance_matrix=class_principal_covariances[m])

                # compute the probability using chi-squared, with the degrees of freedom as the number of main components
                probability_in_class_distribution = np.around(chi2.cdf(np.min(distances, axis=0),
                                                                       len(main_features_idx[n])), decimals=4)
                coco_data['distance-from-centroid'] = np.min(distances, axis=0)
                coco_data['probability-out-of-distribution'] = probability_in_class_distribution

        coco_meta_df = create_metadata_df(coco_data, args)

        # Add to lists
        df_list.append(coco_meta_df)
        features_list.append(features)
        thumbs_list.append(thumbs)

    if args.images and args.from_cube_face:  # Cube face different prediction pipeline
        assert args.compute_class_centroids is not True, 'Not get implemented! Do not use.'
        say("")
        say("Predicting on unlabelled cube faces...")
        patches_coco = image_arg_to_coco(args.images, do_patches=True, patch_size=args.image_size)
        patches_data = get_data_from_coco(patches_coco, args, load_images=False)
        thumbs = np.zeros((len(patches_coco.loadImgs(patches_coco.getImgIds())),
                           args.thumb_size[0], args.thumb_size[1],3))
        features = np.zeros((len(patches_coco.loadImgs(patches_coco.getImgIds())), int(output.shape[-1])))
        cache_file_name = ''
        cnt = 0
        for image in tqdm(patches_coco.loadImgs(patches_coco.getImgIds())):
            cache_path = image['path'].split('.')
            if cache_file_name is not image['filename']:
                cache_file_name = image['filename']
                img = imread(image['filename'], dtype=np.uint8)

            crop_index = np.asarray([[int( cache_path[-3]),
                                      int( cache_path[-3]) + int( args.image_size[0])],
                                    [int( cache_path[-2]),
                                     int( cache_path[-2]) + int( args.image_size[1])]])

            patch = img[crop_index[0,0] : crop_index[0,1],
                        crop_index[1,0] : crop_index[1,1]]
            thumbs[cnt, :, :] = np.array(PIL.Image.fromarray(patch).resize(args.thumb_size), dtype=np.uint8)
            model_output = model.predict(np.expand_dims(patch, axis=0).astype(np.float32) / 127.5 - 1)
            if isinstance(model.output, list): # unpack it
                prediction = model_output[0]
                feature = model_output[1]
            else:
                feature = model_output

            # Get mean features
            features[cnt, :] = pool_features(feature, args.pooling)
            cnt = cnt + 1

        # Get metadata df
        patches_meta_df = create_metadata_df(patches_data, args)

        # Add to lists
        df_list.append(patches_meta_df)
        features_list.append(features)
        thumbs_list.append(thumbs)

    elif args.images and args.from_video:  # process video-frames
        say("")
        say("Predicting on unlabelled videos.")
        say("creating coco...")
        video_coco = image_arg_to_coco(args.images, from_video=True, patch_size=args.image_size)
        frames_data = get_data_from_coco(video_coco, args, load_images=False)
        thumbs = np.zeros((len(frames_data['Path']), args.thumb_size[1], args.thumb_size[0], 3))
        features = np.zeros((len(frames_data['Path']), int(output.shape[-1])))
        if isinstance(model.output, list):
            predictions = np.zeros((len(frames_data['Path'])))
        cache_file_name = ''
        cnt = 0
        say("Predicting...")
        # could do with some serious speed improvements. Best way is to extract in parallel and process in batches
        for image in tqdm(video_coco.loadImgs(video_coco.getImgIds())):
            if cache_file_name is not image['filename']:
                if cnt != 0:  # quick and dirty saving of distances per video
                    np.savetxt(os.path.join(args.tensorboard_dir,
                                            cache_file_name.split('.')[-2].split('/')[-1] + '.distance.csv'),
                        np.min( [compute_distance(features[vid_cnt_begin:cnt, main_features_idx[m]],
                                                            np.tile(class_centroids[m][main_features_idx[m]],
                                                                    (len(features[vid_cnt_begin:cnt, main_features_idx[m]]), 1)),
                                                            method='mahalanobis',
                                                            inverse_covariance_matrix=class_principal_covariances[m])
                             for m, centroid in enumerate(class_centroids)], axis=0), delimiter=',')

                    np.savetxt(os.path.join(args.tensorboard_dir,
                                            cache_file_name.split('.')[-2].split('/')[-1] + '.probability.csv'),
                               np.around(chi2.cdf(np.min([compute_distance(features[vid_cnt_begin:cnt, main_features_idx[m]],
                                                            np.tile(class_centroids[m][main_features_idx[m]],
                                                                    (len(features[vid_cnt_begin:cnt, main_features_idx[m]]), 1)),
                                                            method='mahalanobis',
                                                            inverse_covariance_matrix=class_principal_covariances[m])
                                       for m, centroid in enumerate(class_centroids)], axis=0), len(main_features_idx[n])), decimals=4),
                               delimiter=',')  # quick and dirty saving of probabilities per video

                vid_cnt_begin = cnt # quick and dirty
                cache_file_name = image['filename']
                cap = cv2.VideoCapture(cache_file_name)
            cache_path = image['path'].split('.')
            
            cap.set(cv2.CAP_PROP_POS_FRAMES, int(cache_path[1]))
            _, frame = cap.read()
            try: # quick and dirty
                frame = PIL.Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB).copy()) # convert from cv2 to PIL format
            except:
                frame = PIL.Image.new('RGB', (args.image_size[0], args.image_size[1])) # quick and dirty
                say("Unexpectedly failed to grab a frame; possibly reached the end of video. Continuing...")

            thumbs[cnt, :, :] = np.array(frame.resize((args.thumb_size)), dtype=np.uint8)
            model_output = model.predict(np.expand_dims(frame.resize(args.image_size),
                                                        axis=0).astype(np.float32) / 127.5 - 1)
            if isinstance(model.output, list):
                # unpack list
                prediction = model_output[0].argmax()
                feature = model_output[1]
            else:
                feature = model_output

            # Get mean features
            features[cnt, :] = pool_features(feature, args.pooling)
            predictions[cnt] = prediction
            cnt = cnt + 1

        # Get metadata df
        if predictions is not None:
            frames_data['prediction'] = predictions
            if args.compute_class_centroids:
                say("Computing embedding distances...")
                # compute the distances from the closest centroid
                distances = np.min([compute_distance(features[:, main_features_idx[m]],
                                         np.tile(class_centroids[m][main_features_idx[m]],
                                                 (len(features[:, main_features_idx[m]]), 1)),
                                         method='mahalanobis',
                                         inverse_covariance_matrix=np.linalg.inv(np.cov(
                                             features[:, main_features_idx[m]].T)))
                        for m, centroid in enumerate(class_centroids)], axis=0)
                # compute the probability using chi-squared, with the degrees of freedom as the number of main components
                probability_in_class_distribution = np.around(chi2.cdf(distances, len(main_features_idx[n])), decimals=4)
                frames_data['centroid-distance'] = distances
                frames_data['probability-out-of-distribution'] = probability_in_class_distribution

        patches_meta_df = create_metadata_df(frames_data, args)

        # Add to lists
        df_list.append(patches_meta_df)
        features_list.append(features)
        thumbs_list.append(thumbs)

    elif args.images:  # No cube faces
        assert args.compute_class_centroids is not True, 'Not get implemented! Do not use.'
        say("Predicting on images")
        # Load coco
        say("creating coco...")
        coco = image_arg_to_coco(args.images, do_patches=False)
        # Get the data from coco
        image_data = get_data_from_coco(coco, args, load_images=True)
        # Load images
        images = np.array(image_data['imgs'], dtype=np.uint8)
        # Create thumbnails
        thumbs = np.array([
            np.array(PIL.Image.fromarray(image).resize(args.thumb_size))
            for image in images], dtype=np.uint8)
        # Rescale images
        images = images.astype(np.float32) / 127.5 - 1
        # Predict features
        model_output = model.predict(images)
        if isinstance(model.output, list):
            predictions = model_output[0]
            features = model_output[1]
        else:
            features = model_output
        # Get mean features
        features = pool_features(features, args.pooling)

        # Get metadata df
        images_meta_df = create_metadata_df(image_data, args)

        # Add to lists
        df_list.append(images_meta_df)
        features_list.append(features)
        thumbs_list.append(thumbs)

    # combine all lists into combined
    combined_meta_df = pd.concat(df_list)
    combined_features = np.vstack(features_list)
    combined_thumbs = np.vstack(thumbs_list)

    if args.save_embeddings_to_csv:
        say('Saving embeddings to csv...')
        np.savetxt(os.path.join(args.tensorboard_dir, 'embeddings-features.csv'), combined_features)

    say("Saving embeddings...")
    os.makedirs(args.tensorboard_dir, exist_ok=True)
    save_embeddings(
        args.tensorboard_dir, combined_features,
        metadata=combined_meta_df,
        sprite=images_to_sprite(np.asarray(combined_thumbs)) if combined_thumbs is not None else None,
        sprite_shape=args.thumb_size if combined_thumbs is not None else None)

    say('Done!')

    say("To view embeddings, launch tensorboard from the %s directory with: " %args.tensorboard_dir)
    say("tensorboard --logdir ./")
    say("You may need to edit paths in the projector_config.pbtxt file so they point to the correct places. ")

def get_args(cmd_line=None):
    '''Get args from the command line args'''
    parser = argparse.ArgumentParser(
        description="""View the embeddings of a layer and display the results on the TensorBoard Projector.
        
        example (CCTV oriented) - python3 ~/src/abyss/deep-learning/examples/view-embeddings ./CCTV.outputs/ --images video-list.txt --model-weights /mnt/pond/processed/abyss-internal/cctv/classification/models/fault-no-fault/20190909.fault-no-fault/models/best_model.h5 --model-definition /mnt/pond/processed/abyss-internal/cctv/classification/models/fault-no-fault/20190909.fault-no-fault/models/model-definition.json --image-size 320,240 --layer-name global_average_pooling2d_1 --from-video --thumb-size=64,48 --coco coco.json --compute-class-centroids

        """)
    parser.add_argument(
        "tensorboard_dir", help="Where to write the tensorboard embeddings")
    parser.add_argument(
        "--images", nargs='+', help="images, COCO JSON, or list of capture stub directories to cluster")
    parser.add_argument("--coco", help="Add a COCO json as a labelled dataset")
    parser.add_argument(
        "--image-size", help="CSV height, width to load and resize each image to feed into the network", default=None)
    parser.add_argument(
        "--thumb-size", default='256,256', help="CSV height, width for the TensorBoard thumbnails")
    parser.add_argument(
        "--image-dir", help="The directory to relatively path the images with (COCO only)")
    parser.add_argument(
        "--model-definition", help="The model definition .json")
    parser.add_argument(
        "--model-weights", help="path to model weights file, saved in keras .h5 format", required=True)
    parser.add_argument(
        "--layer-name", help="The layer to calculate the embeddings on, e.g. 'decoder_conv1_pointwise_BN'", required=True)
    parser.add_argument(
        "--pooling", help="The type of pooling to use, options are {avg,max}")
    parser.add_argument(
        "--metadata-csv", help="Additional metadata contained in a .csv file, indexed by filename")
    parser.add_argument(
        "--save-embeddings-to-csv", help="Optionally, save embeddings as a csv.", action='store_true')
    parser.add_argument(
        "--from-cube-face", help="Optionally, sub-divide (tile) input images (i.e., cube-faces), so embeddings can be "
                             "computed for unlabelled cube-faces.", action='store_true')
    parser.add_argument(
        "--from-video", help="Optionally, extract frames from a video, so embeddings can be "
                             "computed for unlabelled videos.", action='store_true')
    parser.add_argument(
        "--compute-class-centroids", help="Optionally, compute the embeddings centroids for each labelled class.",
        action='store_true')
    args = parser.parse_args(args=cmd_line)

    assert args.from_video is not True or args.from_cube_face is not True, "Error! Both --from-video and " \
                                                                            "--from_cube-face cannot be used together."
    assert args.compute_class_centroids is True and args.coco is not None, \
        "Error! If using --compute-class-centroids, a coco fo labelled data much also be supplied using --coco argument."
    if args.image_size is not None:
        args.image_size = tuple(int(i) for i in args.image_size.split(','))
    args.thumb_size = tuple(int(i) for i in args.thumb_size.split(',')) \
        if args.thumb_size else tuple(i // 2 for i in args.image_size)
    return args

if __name__ == "__main__":
    main(get_args())