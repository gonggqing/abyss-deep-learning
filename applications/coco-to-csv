#!/usr/bin/env python3
import argparse
import collections
import copy
import csv
import json
import signal
import sys
from contextlib import redirect_stdout

import pandas as pd
from pycocotools.coco import COCO

signal.signal(signal.SIGPIPE, signal.SIG_DFL)

DESCRIPTION = \
    """
YO

Convert COCO json file and output csv formatted COCO data to stdout.

Can perform join operations between different coco sections based on arguments given to positional argument section and 
optional argument -f/--fields

examples
    General usage:
        cat coco.json | coco-to-csv annotations -f id,bbox,area,categories/name,categories/supercategory,images/file_name > annotations.csv
        cat coco.json | coco-to-csv categories -f id,name --index-from 1 > categories.csv
        cat coco.json | coco-to-csv annotations -f id,bbox, --bbox-position absolute > annotations.csv

    Retinanet usage:
        cat coco.json | coco-to-csv annotations -w retinanet > training.csv
        cat coco.json | coco-to-csv categories -w retinanet > class_mappings.csv
"""


def main(args: argparse.Namespace = None):
    coco = load_coco(sys.stdin.read())
    section = coco.dataset[args.section]
    csv_writer = csv.writer(sys.stdout)

    dataset = copy.deepcopy(coco.dataset)
    # Create common keys between different sections in coco dataset so that pd.merge has a key to merge on
    for i in dataset.get('annotations', []):
        try:
            i['annotation_id'] = i.pop('id')
        except KeyError:
            pass
    for i in dataset.get('categories', []):
        try:
            i['category_id'] = i.pop('id')
        except KeyError:
            pass
    for i in dataset.get('images', []):
        try:
            i['image_id'] = i.pop('id')
        except KeyError:
            pass
        try:
            i['license_id'] = i.pop('license')
        except KeyError:
            pass
    for i in dataset.get('licenses', []):
        try:
            i['license_id'] = i.pop('id')
        except KeyError:
            pass
    # info section is not valid to merge with other sections
    if args.section != 'info':
        # Add original unique id to section of interest
        for i, j in zip(section, dataset[args.section]):
            j['id'] = i['id']

    section = dataset[args.section]
    fields = []
    joined = set()

    # Iterate through fields and check to see if there are nested keys i.e. annotations/bbox or images/path
    for field in args.fields:
        # Nested key. Currently supports nesting to two levels
        if len(field) == 2:
            # Expand values
            section_to_join, field_to_index = field

            # Extract field of interest
            fields.append(field_to_index)

            # If section has already been joined, skip it
            if section_to_join in joined:
                continue

            # Skip merge on info key
            if section_to_join == 'info':
                say("skipping merge on info section as it has no common keys with any other section",
                    verbose=args.verbose)
                continue

            joined.add(section_to_join)
            say("extra section found, trying to merge between {section} and {section_to_join}"
                .format(section_to_join=section_to_join, section=args.section), verbose=args.verbose)

            # Create data frame for each section
            i, j = pd.DataFrame(section), pd.DataFrame(dataset[section_to_join])

            # Find common key to merge on between the two sections
            intersection = set(section[0]).intersection(set(dataset[section_to_join][0]))
            if len(intersection) > 1:
                die(",ore than one common key [{keys}] was found in the join for {section} and {section_to_join}"
                    .format(keys=list(intersection), section=args.section, section_to_join=section_to_join))
            elif len(intersection) != 1:
                die("no common key was found in the join between {section} and {section_to_join}"
                    .format(section=args.section, section_to_join=section_to_join))

            common_key = intersection.pop()
            df = pd.merge(i, j, on=common_key)

            # Convert data frame back to list
            section = df.to_dict('records')

        # Extract field of interest
        elif len(field) == 1:
            fields.append(*field)
        else:
            say("nesting up to {num} levels is not supported".format(num=len(field)), verbose=args.verbose)

    fixed_headers = get_header(section)

    if args.output_fields:
        if fields:
            print(",".join(fields))
        else:
            print(','.join(fixed_headers))
        sys.exit(0)

    # Find bbox index in list
    try:
        bbox_index = fields.index('bbox')
    except ValueError:
        bbox_index = None

    # Find key index for category id
    if args.index_from is not None:
        if args.section == 'annotations':
            key = 'category_id'
        elif args.section == 'categories':
            key = 'id'
        else:
            key = None

        try:
            key_index = fields.index(key)
        except ValueError:
            key_index = None
        except TypeError:
            key_index = None

        min_value = min(coco.getCatIds())
        offset = args.index_from - min_value

    # Unpack info section
    if isinstance(section, dict):
        # Print headers
        if args.header:
            headers = fixed_headers
            if fields:
                headers = [header for header in fields if header in section]
                non_headers = list(set(fields) - set(headers))
                if not headers:
                    die('cannot find keys {fields} in dict {section}'
                        .format(fields=fields, section=args.section))
                if non_headers:
                    say('cannot find keys {fields} in dict {section}'
                        .format(fields=non_headers, section=args.section), verbose=args.verbose)
                headers = fields
            csv_writer.writerow(headers)

        # Print values
        if fields:
            valid_fields = [field for field in fields if field in section]
            if valid_fields:
                # If field is not in the dictionary, substitute for whitespace
                values = [section.get(field, '') for field in fields]
                csv_writer.writerow(values)

                # Warning message
                non_values = list(set(fields) - set(valid_fields))
                if non_values:
                    say('cannot find keys {fields} in dict {section}'
                        .format(fields=non_values, section=args.section), verbose=args.verbose)
            else:
                # No valid field can be found to index into the dictionary
                die('cannot find keys {fields} in dict {section}'.format(fields=valid_fields, section=args.section))
        else:
            values = [section[field] if field in section else '' for field in fixed_headers]
            csv_writer.writerow(values)

    # Unpack images, annotations, categories, or licenses section
    elif isinstance(section, list):
        # Print headers
        if args.header:
            if fields:
                # Find fields that are not valid keys
                headers = [field for field in fields if field in section[0]]
                non_headers = list(set(fields) - set(headers))

                # Warning prints
                if not headers:
                    die('cannot find keys {fields} in dict {section}'.format(fields=fields, section=args.section))
                if non_headers:
                    say('cannot find keys {fields} in dict {section}'
                        .format(fields=non_headers, section=args.section), verbose=True)
                headers = fields.copy()
            else:
                headers = fixed_headers

            # Substitute bbox for either x,y,width,height or x1,y1,x2,y2
            if bbox_index is not None:
                headers[bbox_index] = ['x', 'y', 'width', 'height'] \
                    if args.bbox_position == 'relative' \
                    else ['x1', 'y1', 'x2', 'y2']

            csv_writer.writerow(flatten(headers))

        # Print values in fields. If field is not valid, output empty string
        for entry in section:
            if fields:
                valid_fields = [field for field in fields if field in entry]
                non_fields = list(set(fields) - set(valid_fields))
                if valid_fields:
                    values = [entry.get(field, '') for field in fields]
                    if bbox_index is not None:
                        x, y, width, height = bbox = values[bbox_index]
                        if args.bbox_position == 'absolute':
                            bbox[2] = x + width
                            bbox[3] = y + height
                        values[bbox_index] = bbox

                    if args.index_from is not None and key_index is not None:
                        values[key_index] += offset

                    csv_writer.writerow(flatten(values))
                else:
                    # Error print
                    die('cannot find keys {fields} in dict {section}'.format(fields=fields, section=args.section))

                if non_fields:
                    # Warning print
                    say('cannot find keys {fields} in dict {section}'
                        .format(fields=non_fields, section=args.section), verbose=args.verbose)
            else:
                values = [entry[field] if field in entry else '' for field in fixed_headers]
                csv_writer.writerow(values)

    sys.exit()


def get_header(section_):
    priority_dict = {
        "id": 0,
        "annotation_id": 10,
        "category_id": 20,
        "image_id": 30,
        "score": 40,
        "bbox": 50,
        "area": 60,
        "segmentation": 70,
        "path": 80,
        "width": 90,
        "height": 91,
        "name": 100
    }
    if isinstance(section_, dict):
        return list(map(lambda x: str(x), section_))
    elif isinstance(section_, list):
        return sorted(set(flatten([map(lambda x: str(x), section_[i]) for i in range(len(section_))])),
                      key=lambda y: priority_dict[y] if y in priority_dict else sys.maxsize)


def say(*args, verbose=False, **kwargs):
    if verbose:
        print(*args, file=sys.stderr, **kwargs)


def die(*args, **kwargs):
    say("error: ", *args, verbose=True, **kwargs)
    sys.exit(1)


def get_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description=DESCRIPTION, formatter_class=argparse.RawDescriptionHelpFormatter)
    parser.add_argument('section',
                        nargs='?',
                        default='annotations',
                        choices=['annotations', 'categories', 'info', 'images', 'licenses'],
                        help="General fields to extract from COCO json to csv; default: %(default)s",
                        )
    parser.add_argument('-f', '--fields', type=str, help="Comma separated values of COCO fields to extract from the "
                                                         "json file and export to a csv format. Performs a join "
                                                         "between two different sections if a '/' character is given "
                                                         "separating section and field")
    parser.add_argument('-w', '--what', choices=['retinanet', 'yolo3'],
                        help="Format the csv output to be compatible with one "
                             "of the supported options")
    parser.add_argument('-p', '--bbox-position', type=str, default='relative', choices=['absolute', 'relative'],
                        help="Output values of bbox as either x1,y1,x2,y2 or x,y,width,height; default: %(default)s")
    parser.add_argument('-i', '--index-from', type=int, help="Change category indexing values to start "
                                                             "from specified value; default: %(default)s")
    parser.add_argument('-head', '--header', '--field-names', action='store_true',
                        help="The first row of the csv output will contain the field names")
    parser.add_argument('-o', '--output-fields', action='store_true', help="Output to stdout what fields will be "
                                                                           "retrieved from the coco file")
    parser.add_argument('-v', '--verbose', action='store_true', help="More output to stderr")
    args = parser.parse_args()

    if args.what == 'retinanet':
        if args.section == 'annotations':
            # todo: this will not work, if path field is not present in images: it will output empty field for path
            #       according to coco spec, images/file_name is basename of the file, while we need the full path
            #       it is unclear which field to use (e.g. coco_url may be the right candidate)
            #       please don't hack this code, ask the authors what is a better way
            #       for a quick workaround you always can run coco-to-csv with explicit --fields, something like:
            #       coco-to-csv --fields 'images/coco_url,bbox,categories/name'
            args.fields = 'images/path,bbox,categories/name'  # args.fields = 'images/file_name,bbox,categories/name'
            args.bbox_position = 'absolute'
        elif args.section == 'categories':
            args.fields = 'name,id'
            args.index_from = 0
    if args.what == 'yolo3':
        die("yolo3: todo")

    try:
        args.fields = list(map(lambda x: x.split('/'), args.fields.split(',')))
    except AttributeError:
        args.fields = []

    return args


def flatten(l: collections.Iterable) -> list:
    """
    Flatten an iterable into a single list recursively

    Args:
        l: iterable item

    Returns: list of flattened elements

    """
    result = []
    for el in l:
        if isinstance(el, collections.Iterable) and not isinstance(el, (str, bytes)):
            result.extend(flatten(el))
        else:
            result.append(el)
    return result


class Verbose:
    @staticmethod
    def write(line):
        line = line.strip()
        if line:
            say(line)


def load_coco(json_buffer: str) -> COCO:
    with redirect_stdout(Verbose):
        coco = COCO()
        coco.dataset = json.loads(json_buffer)
        coco.createIndex()
    return coco


if __name__ == '__main__':
    main(args=get_args())
