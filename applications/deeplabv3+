#!/bin/bash
# Deeplab V3+ train, eval, export utility
SCRIPTNAME="$(basename $0)"
ARGPARSE_DESCRIPTION="Deeplab v3+ application for initialising dataset, training, evaluating and exporting the frozen inference graph.
Examples:

deeplabv3+ init train.json /tmp/project1 \\\\
    --eval-dataset val.json \\\\
    --image-dir /path/to/voc/JPEGImages \\\\
    --deeplab-dir $HOME/tensorflow_models/research/deeplab

deeplabv3+ train /tmp/lol/tmp/project1/tfrecord /tmp/project1 \\\\
    --deeplab-dir $HOME/src/tensorflow_models/research/deeplab \\\\
    --train-steps 1000 --batch-size 2 --train-crop-size 513 \\\\
    --validate --eval-crop-size 1025
"
source "$(dirname $0)/argparse.bash" || exit 1 # This imports functions info, error, optional_flag, bool_flag, assert
argparse "$@" <<EOF || exit 1
parser.add_argument('operation', help='One of {init, train, eval, export}')
parser.add_argument('dataset', help='The path to the coco JSON (for init) or tfrecord (all others) to train')
parser.add_argument('logdir', help='Path to the log directory used for all operations')
parser.add_argument('--debug', action='store_true', help='Sets bash -x flag')
parser.add_argument('--deeplab-dir', help='The path to tensorflow/models/deeplab', default='$PWD')
parser.add_argument('--verbose', '-v', action='store_true', help='Verbose output to stderr')
parser.add_argument('--image-dir', help='[init] The path to the image directory for the COCO JSON (init only)', default='$PWD')
parser.add_argument('--image-format', help='[init] The image format of the files (init only)', default='jpg')
parser.add_argument('--eval-dataset', help='[init] The path to the image directory for the COCO JSON (init only)')
parser.add_argument('--learning-rate', type=float, help='[train] The base learning rate (default 1e-4, use 1e-3 for scratch training)', default=1e-4)
parser.add_argument('--model', help='[train] One of {xception_65, mobilenet_v2}', default='xception_65')
parser.add_argument('--reinitialize-head', action='store_true', help='[train] Clear the class specific layers to allow for a transfer learning to a new class set.')
parser.add_argument('--train-crop-size', help='[train] The crop size to use in training and evaluation', default=513)
parser.add_argument('--train-freeze-batch-norm', action='store_false', help='[train] Whether or not to freeze the batch norms')
parser.add_argument('--train-steps', type=int, help='[train] The number of epochs to train for', default=0)
parser.add_argument('--validate', action='store_true', help='[train] Simultenously run validation after every checkpoint.')
parser.add_argument('--num-train-gpus', help='[train] The number of GPUs used for training.')
parser.add_argument('--batch-size', help='[train, eval] The batch size to use in training and evaluation', default=2)
parser.add_argument('--checkpoint', help='[train, eval] The path to the checkpoint to load before training or evaluation')
parser.add_argument('--eval-crop-size', help='[eval] The crop size to use in evaluation, must be at least as big as the largest image in val set', default=513)
parser.add_argument('--export-crop-size', help='[export] The crop size to use when exporting, must be at least as big as the largest image to be deployed to', default=513)
parser.add_argument('--export-checkpoint-number', type=int, default=0, help='[export] When exporting a model, the checkpoint number to use, e.g. for model.ckpt-5000, this would be 5000')
EOF

set -e
[[ ! -z $DEBUG ]] && set -x || true

function patch_dataset_file(){
 local file="$DEEPLAB_DIR/datasets/segmentation_dataset.py"
 assert "-f '$DATASET/dataset_meta.json'" "Dataset meta file not found."
 NUM_CLASSES=$(jq '.train.num_classes' "$DATASET/dataset_meta.json")
 TRAIN_SET_SIZE=$(jq '.train.set_size' "$DATASET/dataset_meta.json")
 VAL_SET_SIZE=$(jq '.val.set_size' "$DATASET/dataset_meta.json")
 NUM_CLASSES=$(( 1 + NUM_CLASSES ))
 [[ "$VAL_SET_SIZE" == 'null' ]] && VAL_SET_SIZE=0
 info "Patching $file with $NUM_CLASSES classes and train/val size of $TRAIN_SET_SIZE/$VAL_SET_SIZE"
 [[ $VALIDATE == yes ]] && assert "$VAL_SET_SIZE > 0" "Val set is empty but --validate was given."
 export NUM_CLASSES
 export TRAIN_SET_SIZE
 export VAL_SET_SIZE
 perl -p -e 's/\$\{([^}]+)\}/defined $ENV{$1} ? $ENV{$1} : $&/eg' <"$file.template" >"$file"
}

function do_install(){
 error "TODO: install"
}

function do_init(){
 # if [[ ! -f "$DATASET" ]] ; then error "[$LINENO] dataset $DATASET does not exist"; fi
 # if [[ ! -z "$EVAL_DATASET" && ! -f "$EVAL_DATASET" ]] ; then error "[$LINENO] validation dataset $EVAL_DATASET does not exist"; fi
 # if [[ ! -d "$IMAGE_DIR" ]] ; then error "[$LINENO] dataset $IMAGE_DIR does not exist"; fi
 # if [[ ! -d "$LOGDIR" ]] ; then error "[$LINENO] dataset $LOGDIR does not exist"; fi
 assert "-f '$DATASET'" "[$LINENO] dataset $DATASET does not exist"
 assert "-z '$EVAL_DATASET' || -f '$EVAL_DATASET'" "[$LINENO] validation dataset $EVAL_DATASET does not exist"
 assert "-d '$IMAGE_DIR'" "[$LINENO] dataset $IMAGE_DIR does not exist"
 assert "-d '$LOGDIR'" "[$LINENO] dataset $LOGDIR does not exist"


 # Go to the train logdir and create the VOC dataset

 cd "$LOGDIR"
 [[ ! -z $EVAL_DATASET ]] && VAL_CMD="val,$EVAL_DATASET,$IMAGE_DIR"
 coco-to-tfrecord "train,$DATASET,$IMAGE_DIR" "$VAL_CMD" --deeplab-dir "$DEEPLAB_DIR" --output-dir "$LOGDIR" --image-format $IMAGE_FORMAT
 info "init success"
}

function copy_models(){
if [[ ! -d $LOGDIR/checkpoints ]]; then
  mkdir -p $LOGDIR/checkpoints
fi
inotifywait -m -e moved_to -e create "$LOGDIR" --format "%f" | while read f
do
    echo $f
    # check if the file is a .sh file
    if [[ $f = *"model.ckpt"* ]]; then
      # Sleep to allow for all models to be copied in
      sleep 15
      # Copy the models - use rsync instead?
      cp -r $LOGDIR/*"model.ckpt"* $LOGDIR/checkpoints
    fi
done
}



function do_train(){
 info "Training:\n DATASET=${DATASET}\nLOGDIR=${LOGDIR}"
 assert "$TRAIN_STEPS > 0" "[$LINENO] --train-steps must be provided"
 assert "$BATCH_SIZE > 0" "Invalid batch size of 0"
 if [[ $TRAIN_FREEZE_BATCH_NORM = yes ]]; then
  assert "$BATCH_SIZE > 1" "If you are fine tuning batch_norm you MUST have a batch size > 1."
  if [[ $BATCH_SIZE < 12 ]]; then info "WARNING: Batch size of at least 12-16 is recommended"; fi
 fi
 patch_dataset_file


 info "Beginning training for $TRAIN_STEPS steps"
  PYTHONPATH=$PYTHONPATH CUDA_VISIBLE_DEVICES=$(cut -d, -f1 <<< $CUDA_VISIBLE_DEVICES) \
  python3 "${DEEPLAB_DIR}"/train.py \
  --atrous_rates 12  --atrous_rates 18  --atrous_rates 6 \
  --base_learning_rate "$LEARNING_RATE" \
  --dataset_dir "${DATASET}" \
  --decoder_output_stride 4 \
  $(bool_flag fine_tune_batch_norm "$TRAIN_FREEZE_BATCH_NORM") \
  --logtostderr \
  --model_variant "$MODEL" \
  --output_stride 16 \
  --save_summaries_images \
  --save_summaries_secs 60 \
  --train_batch_size "${BATCH_SIZE}" \
  --train_crop_size "${TRAIN_CROP_SIZE}" --train_crop_size "${TRAIN_CROP_SIZE}" \
  --train_logdir "$LOGDIR" \
  --train_split "train" \
  --training_number_of_steps "$TRAIN_STEPS" \
  $(optional_flag num_clones "$NUM_TRAIN_GPUS") \
  $(bool_flag initialize_last_layer "$REINITIALIZE_HEAD") \
  --last_layers_contain_logits_only \
  --save_interval_secs 300 \
  $(optional_flag tf_initial_checkpoint "$CHECKPOINT") &
 PID_TRAIN=$!

 # This is used to copy the models to $LOGDIR/checkpoints, which stops deeplab overwriting them
 copy_models &


 if [ ! -z "$VALIDATE" ] ; then
  do_eval 0 &
  PID_VAL=$!
  sleep 2
  if ! kill -0 $PID_VAL >/dev/null; then
   kill $PID_TRAIN
  fi
 fi
 wait $PID_TRAIN
 kill $PID_VAL

 info "Training finished."
}

function do_eval(){
 local num_eval=$1
 local cuda_visible_devices=$CUDA_VISIBLE_DEVICES

 if (( num_eval == 0 )); then
  # Simultaneously for --validate, must use different GPU
  cuda_visible_devices=$(cut -d, -f2 <<< $CUDA_VISIBLE_DEVICES)
 else
  cuda_visible_devices=$(cut -d, -f1 <<< $CUDA_VISIBLE_DEVICES)
 fi

 info "Evaluating:\n DATASET=${DATASET}\nLOGDIR=${LOGDIR}"

 PYTHONPATH=$PYTHONPATH CUDA_VISIBLE_DEVICES=$cuda_visible_devices \
 python3 "${DEEPLAB_DIR}/eval.py" \
  --logtostderr \
  --eval_split="val" \
  --model_variant="$MODEL" \
  --atrous_rates=6 \
  --atrous_rates=12 \
  --atrous_rates=18 \
  --output_stride=16 \
  --decoder_output_stride=4 \
  --eval_crop_size "${EVAL_CROP_SIZE}" --eval_crop_size "${EVAL_CROP_SIZE}" \
  --checkpoint_dir="${LOGDIR}" \
  --eval_logdir="${LOGDIR}" \
  --dataset_dir="${DATASET}" \
  --model_variant "$MODEL" \
  --max_number_of_evaluations=${num_eval}
 local return_code=$?
 info "Eval finished."
 return $?
}

function do_export(){
 local cuda_visible_devices=$CUDA_VISIBLE_DEVICES

 # If the checkpoint number is 0, remove it from the path
 if ((EXPORT_CHECKPOINT_NUMBER == 0)); then
   CHECKPOINT_PATH=$LOGDIR/model.ckpt
 else
   CHECKPOINT_PATH=$LOGDIR/model.ckpt-$EXPORT_CHECKPOINT_NUMBER
 fi

 info "Exporting checkpoint ${CHECKPOINT_PATH} to ${LOGDIR}/export/frozen_inference_graph_$EXPORT_CHECKPOINT_NUMBER.pb"

 # Make the log directory
 mkdir -p ${LOGDIR}/export
 PYTHONPATH=$PYTHONPATH CUDA_VISIBLE_DEVICES=$cuda_visible_devices \
 python3 ${DEEPLAB_DIR}/export_model.py \
  --checkpoint_path="${CHECKPOINT_PATH}" \
  --export_path="${LOGDIR}/export/frozen_inference_graph_$EXPORT_CHECKPOINT_NUMBER.pb" \
  --logtostderr \
  --atrous_rates=6 \
  --atrous_rates=12 \
  --atrous_rates=18 \
  --output_stride=16 \
  --decoder_output_stride=4 \
  --crop_size ${EXPORT_CROP_SIZE} --crop_size ${EXPORT_CROP_SIZE} \
  --model_variant "$MODEL" \
  --inference_scales=1.0
 local return_code=$?
 info "Export finished."
 return $return_code
}




trap ctrl_c INT
function ctrl_c() {
 if [[ ! -z $PID_TRAIN ]]; then
   info "killing process: train ($PID_TRAIN)"
   kill $PID_TRAIN
 fi
 if [[ ! -z $PID_VAL ]]; then
   info "killing process: validate ($PID_VAL)"
   kill $PID_VAL
 fi
}
####
assert "-d '$DEEPLAB_DIR'" "[$LINENO] Deeplab dir not found"
assert "$(basename $(dirname "$DEEPLAB_DIR") ) = research" "[$LINENO] Deeplab dir not in tensorflow_models/research folder"
assert "-e '$DATASET'" "[$LINENO] dataset '$DATASET' does not exist"
assert "-d '$LOGDIR'" "[$LINENO] log dir '$LOGDIR' does not exist"

PYTHONPATH="$PYTHONPATH:$DEEPLAB_DIR/..:$DEEPLAB_DIR/../slim"

[[ "$OPERATION" == "install" ]] && do_install
[[ "$OPERATION" == "init" ]] && do_init
[[ "$OPERATION" == "train" ]] && do_train
[[ "$OPERATION" == "eval" ]] && do_eval 1
[[ "$OPERATION" == "export" ]] && do_export
