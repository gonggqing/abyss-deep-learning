#!/usr/bin/env python3.6
"""Train a segmentation network on COCO JSON datasets.
Images are loaded as RGB uint8, optionally augmented, then converted to
tensorflow standard format (float32 in range [-1:1]) before being fed to
the network.

Attributes:
    CONFIG_INSTALL_PATH (str): The path to the config file when loading this module.

"""

import argparse
import json
import logging
import os
import sys
import copy

import abyss_deep_learning.imgaug as abyss_imgaug
import abyss_deep_learning.keras.zoo.segmentation.deeplab_v3_plus.util as deeplab_util
import numpy as np
import tensorflow as tf
from abyss_deep_learning.datasets.coco import ImageSemanticSegmentationDataset
from abyss_deep_learning.keras.utils import lambda_gen
from imgaug.augmentables.segmaps import SegmentationMapsOnImage
from keras_contrib.callbacks import CyclicLR
from tensorboard.plugins.hparams import api as hp


CONFIG_INSTALL_PATH = os.path.join(
    os.path.dirname(os.path.dirname(sys.argv[0])),
    "etc/abyss/ml/segmentation/training/config.json")


def xpath_get(dictionary, path):
    elem = dictionary
    for x in path.strip("/").split("/"):
        elem = elem.get(x)
    return elem


def xpath_set(dictionary, path, value):
    elem = dictionary
    parts = path.strip("/").split("/")
    for x in parts:
        child = elem.get(x)
        if not isinstance(child, dict):  # Leaf
            if x != parts[-1]:  # If leaf element is not leaf path, create
                elem[x] = dict()
                elem = elem[x]
                continue
            elem[x] = value
            return
        elem = child

def config_to_hparams(config):
    """
    Converts the training configuration file to a flat key:plain-data type dictionary that is necessary for passing to hparams.
    Replaces dictionary hierarchy with '/'.

    Args:
        config: (dict) The training configuration file.

    Returns:
        (dict): The flattened dictionary.

    """
    training_config = copy.deepcopy(config)
    def flatten_configuration(cfg):
        flat_cfg = {}
        queue = []
        queue.extend(list(cfg.items()))
        while len(queue) > 0:
            next_pair = queue.pop(0)
            if next_pair[1] is None:
                continue
            if isinstance(next_pair[1], dict):
                for k, v in next_pair[1].items():
                    queue.append((next_pair[0] + '/' + k, v))
            elif isinstance(next_pair[1], list) or isinstance(next_pair[1], tuple):
                for n, v in enumerate(next_pair[1]):
                    queue.append((next_pair[0] + '[%d]' % n, v))
            else:
                flat_cfg[next_pair[0]] = next_pair[1]
        return flat_cfg

    try:
        for key in ['training', 'validation']:
            del training_config[key]['augmentation']
    except KeyError as error:
        pass
    flat_cfg = flatten_configuration(training_config)
    return flat_cfg


class ScalarLogger(tf.keras.callbacks.Callback):
    def __init__(self, scalar_tensor, name):
        self.tensor = scalar_tensor
        self.name = name

    def on_epoch_end(self, epoch, logs):
        logs[self.name] = tf.keras.backend.get_value(self.tensor)


class BestMetricCallback(tf.keras.callbacks.Callback):
    """
    Logs the best value a metric has reported during the training run. This is used for easier visualisation for hparams.
    """
    def __init__(self, metric, target='max'):
        """
        Args:
            metric: (str) the target metric, e.g. val_accuracy
            target: (str) whether best performance is minimising or maximising the metric. Defaults to max.
        """
        super(BestMetricCallback, self).__init__()
        self.metric = metric
        self.best = None
        if target == 'max':
            self.mult = 1
        elif target == 'min':
            self.mult = -1
        else:
            raise ValueError("Target options are min or max")
    def on_epoch_end(self, epoch, logs=None):
        """
        Adds the best recorded metric to the logs
        """
        recorded_metric = logs[self.metric]
        if self.best is None:
            self.best = recorded_metric
        elif recorded_metric * self.mult > self.best * self.mult:
            self.best = recorded_metric
        logs[self.metric + '_best'] = self.best


class AnnotationClassMapper:
    """Maps category to classes given an explicit mapping.

    Attributes:
        category_to_class_map (dict): Mapping from category_id to class_id
    """

    def __init__(self, category_to_class_map):
        self.category_to_class_map = category_to_class_map

    def filter(self, annotation):
        return True

    def translate(self, annotation):
        output = dict(annotation)
        output['category_id'] = self.category_to_class_map[annotation['category_id']]
        return output


def get_args(cmd_line=None):
    """Construct arguments from command line.

    Returns:
        argparse.namespace: The arguments for this application

    Args:
        cmd_line (str, optional): Command line string to use instead of
            reading from sys.argv.
    """
    parser = argparse.ArgumentParser(
        formatter_class=argparse.RawTextHelpFormatter,
        description=f"""
        Train a segmentation network.
        Configurations are specified in three ways, which override eachother:
        * A default configuration file ({CONFIG_INSTALL_PATH}).
        * An optional configuration file (specified by --config).
        * Optional command line arguments.
        The final configuration used is then saved in the output directory as
        `config.train.json`.

        Usage:

        # Train a model, initializing new heads (defined in config.json:/model).
            train_segmenter.py config.json \\
            --training-dataset coco.train_set_A.json \\
            --validation-dataset coco.val.json \\

        # Continue training a model (given the same config.json, and model.h5)
            train_segmenter.py config.json \\
            --training-dataset coco.train_set_B.json \\
            --training-weights model.h5 \\
            --validation-dataset coco.val.json \\
            """)
    # Note: Arguments which should be structured under xpath for CLI override
    #       must separate levels using double underscores in the `dest` argument.
    # Example: (training__learning_rate -> /training/learning_rate)
    parser.add_argument(
        '--config',
        help="Specifies the path to the config JSON for the model.")
    parser.add_argument(
        "--model-dir", default="model",
        help="The directory to place the models.")
    parser.add_argument(
        '--training-dataset', type=str, dest="training__dataset",
        help="Path to the coco dataset")
    parser.add_argument(
        "--model-weights", type=str, dest="model__weights",
        help=("Load a standard pretrained model (`pascal_voc` (21 classes), `cityscapes` (19))."
              "Weights overridden by --training-weights if specified."))
    parser.add_argument(
        "--training-weights", type=str, dest="training__weights",
        help=("Path to model weights to load."))
    parser.add_argument(
        "--training-learning_rate", dest="training__optimizer__config__learning_rate",
        help="The training learning rate.", type=float)
    parser.add_argument(
        "--validation-dataset", type=str, dest="validation__dataset",
        help="Path to the validation coco dataset")
    parser.add_argument("--seed", default=0, type=int, help="Random seed (0 default)")
    parser.add_argument(
        '--parallel', '-P', default=['/gpu:0'], action='append', help="Compute devices to use, eg '/gpu:0', '/cpu:0'")
    parser.add_argument("--hparams-metric",
                        help="Logs the best value of this metric at every epoch. \
                        Specifying this argument will enable logging of hparams.")
    parser.add_argument("--hparams-function", choices=['min', 'max'],
                        help="For keep-best-metric, whether this metric should be maxmised or minimised. \
                        I.e. accuracy should be maximised, loss should be minimised")
    parser.add_argument(
        '--verbose', '-v',
        action='store_const', const=logging.INFO,
        dest='loglevel', help="verbose output to stderr")
    args = parser.parse_args(cmd_line)
    args.parallel = list(dict.fromkeys(args.parallel))  # remove duplicates
    if args.hparams_metric and not args.hparams_function:
        raise ValueError("If --hparams-metric is given, --hparams-function must also be given")
    return args


def get_class_mapping(dataset, categories_config=None):
    """Return a class map for the given dataset and categories
        json mapping.

    Args:
        dataset (adl.datasets.coco.CocoInterface): The dataset
            to perform the mapping on.
        categories_config (dict): The categories configuration
            dict mapping `coco name` to `class id`.

    Returns:
        AnnotationClassMapper: Dataset translator to provide to
            the dataset.
    """
    category_map = {  # Maps category name to category_id
        category['name']: category['id']
        for category in dataset.coco.cats.values()
    }

    category_to_class_map = {
        category_map[pair['name']]: pair['id']
        for pair in categories_config
    }
    logging.info('category_to_class_map')
    logging.info(category_to_class_map)
    return AnnotationClassMapper(category_to_class_map)


def setup_dataset(coco_path, model_config, process_config, categories_config, args):
    """Set up a COCO dataset given the model, process (train/val),
        and categories configuration.

    Args:
        coco_path (str): Description
        model_config (dict): Model configuration dict.
        process_config (dict): Training or validation configuration dict.
        categories_config (dict): Categories configuration dict.

    Returns:
        adl.datasets.coco.CocoInterface: The adl.datasets COCO dataset.
        tf.data.Dataset: The tf.keras compatible dataset.
    """
    def hack_dataset(generator):
        """A hack to get generators to work with tf.keras datasets.

        Args:
            generator: A generator yielding (input, targets) tuples.

        Returns:
            tf.keras.datasets.Dataset: A tf.keras compatible dataset.
        """
        def wrap_callable():
            return generator
        return wrap_callable

    def preprocess_inputs(image, targets):
        """Preprocess RGB uint8 inputs: apply augmentation and convert image
        in to tf image format [-1:1] float32.

        Args:
            image (np.array): The input image
            targets (np.array): The input targets

        Returns:
            image: The image to send to the network.
            targets: The targets to send to the network.
        """
        nonlocal augmentation
        if augmentation is not None:
            image, targets = augmentation(
                image=image,
                segmentation_maps=SegmentationMapsOnImage(targets, image.shape))
            targets = targets.get_arr()
            # TODO: Support higher depth images (move from PIL to ???)
            return image.astype('float32') / 127.5 - 1, targets
        return image, targets

    augmentation = None
    if process_config['augmentation']:
        augmentation = abyss_imgaug.sequential_from_dicts(
            process_config['augmentation'])
        logging.info("Using augmentation:")
        logging.info([i for i in augmentation])
    output_shapes = (
        model_config['input_shape'],
        model_config['input_shape'][:-1] + (model_config['classes'],))
    dataset = ImageSemanticSegmentationDataset(
        coco_path, num_classes=model_config['classes'])
    if categories_config is not None:
        translator = get_class_mapping(dataset, categories_config)
        dataset.translator = translator

    generator = dataset.generator(endless=True)
    generator = lambda_gen(generator, func=preprocess_inputs)
    generator = tf.data.Dataset.from_generator(
        hack_dataset(generator),
        output_types=(tf.float32, tf.float32),
        output_shapes=output_shapes)\
        .prefetch(2 * len(args.parallel) * process_config['batch_size'])\
        .batch(process_config['batch_size'])
    return dataset, generator


def load_config(args):
    """Load the default configuration, and update it with the config file.

    Args:
        args (argparse.namespace): The arguments to the application.

    Returns:
        dict: The configuration dict.
    """

    # First, load the installed config

    logging.info(f"Loading installed config file: {CONFIG_INSTALL_PATH}.")
    try:
        with open(CONFIG_INSTALL_PATH, 'r') as file:
            config = json.load(file)
    except OSError:
        logging.error(f"Couldn't find installed config file: {CONFIG_INSTALL_PATH}.")
        raise

    # Second, override with user config if present.
    if args.config:
        logging.info(f"Loading user config: {args.config}")
        with open(args.config, 'r') as file:
            user_config = json.load(file)
            for section in ['model', 'training', 'validation']:  # dicts
                config[section].update(user_config[section])
            for section in ['categories']:  # lists
                config[section] = list(user_config[section])
    # Third, override with command line arguments
    for key, value in args.__dict__.items():
        if value is None:
            continue
        xpath_key = "/" + key.replace("__", "/")
        try:
            xpath_set(config, xpath_key, value)
        except KeyError:
            logging.error(f"CLI override failed: {xpath_key} = {value}")

    # Fix config types, needed for deeplab.
    config['model']['input_shape'] = tuple(config['model']['input_shape'])
    return config


def setup_schedule(config, steps_per_epoch):
    schedule_config = dict(config['training']['schedule'])
    schedule_type = schedule_config['type']
    schedule = None
    del schedule_config['type']
    if schedule_type == 'cyclic':
        if 'epochs_per_cycle' in schedule_config:  # Set by epoch instead of steps
            schedule_config['step_size'] = schedule_config['epochs_per_cycle'] * steps_per_epoch
            del schedule_config['epochs_per_cycle']
            schedule = CyclicLR(**schedule_config)
    elif schedule_type == 'power':
        schedule = tf.keras.callbacks.LearningRateScheduler(
            lambda epoch: (
                schedule_config['initial_lr']
                * schedule_config['base']
                ** (-epoch / schedule_config['period'])))
    elif schedule_type == 'plateau':
        schedule = tf.keras.callbacks.ReduceLROnPlateau(**schedule_config)
    else:
        logging.error(f"Unknown training schedule {schedule_type}.")
    return schedule


def setup_model_util(config):
    """Given the application config, set up the model utilities module.

    Args:
        config (dict): Application configuration.

    Returns:
        module: The abyss.keras.zoo model utilities module.

    Raises:
        ValueError: If an unknown model is given.
    """
    # Setup model utilities
    return {
        "deeplab": deeplab_util,
    }[config['model']['type']]
    raise ValueError(f"Unknown model type `{config['model']['type']}`")


def main(args):
    """Train a segmentation network given the program args.

    Args:
        args (argparse.Namespace): Program args from get_args()

    Returns:
        int: Error code. 0 if no errors occurred.
    """
    np.random.seed(args.seed)
    tf.random.set_seed(args.seed)

    logging.basicConfig(
        format='%(filename)s: %(asctime)s.%(msecs)d: %(levelname)s: %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S',
        level=args.loglevel)
    logging.info('--verbose enabled')
    logging.info(args)

    # Load config and model utils
    config = load_config(args)
    with open("config.trained.json", "w") as file:
        file.write(
            json.dumps(
                config, sort_keys=True,
                indent=4, separators=(', ', ': ')))
    model_util = setup_model_util(config)

    # Set up dataset
    training_dataset, gen_train = setup_dataset(
        config['training']['dataset'], config['model'],
        config['training'], config['categories'], args)
    validation_steps, gen_val = config['validation']['steps'], None

    if 'validation' in config:
        logging.info("Using validation.")
        validation_dataset, gen_val = setup_dataset(
            config['validation']['dataset'], config['model'],
            config['validation'], config['categories'], args)
        if validation_steps is None:
            validation_steps = len(validation_dataset.data_ids) \
                // config['validation']['batch_size'] // len(args.parallel)
    steps_per_epoch = len(training_dataset.data_ids) \
        // config['training']['batch_size'] // len(args.parallel)

    # Set up training
    logging.info('Training on compute devices:')
    logging.info(args.parallel)
    if len(args.parallel) == 1:
        strategy = tf.distribute.OneDeviceStrategy(device=args.parallel[0])
    elif len(args.parallel) >= 2:
        strategy = tf.distribute.MirroredStrategy(devices=args.parallel)
    else:
        print('Invalid compute device selection!')
        exit(1)

    with strategy.scope():
        # Set up training
        if config['training']['new_heads']:
            logging.info("Creating new head.")
            model_args = dict(config['model'])
            del model_args['type']
            model = model_util.make_model(**model_args)
        else:
            logging.info("Loading model.")
            # TODO? Support for tensorflow version < 1.14 (load model from json+h5)
            model = tf.keras.models.load_model(
                args.training__weights, compile=False, custom_objects={'tf': tf})

        # Start training
        optimizer = tf.keras.optimizers.deserialize(config['training']['optimizer'])

        for layer in model.layers:
            if type(layer).__name__.startswith('BatchNormalization'):
                layer.trainable = layer.trainable and config['training']['train_batch_norm']

        freeze = config['training']['freeze'] if 'freeze' in config['training'] else None
        if freeze:
            layer_names = [layer.name for layer in model.layers]
            if freeze not in layer_names:
                logging.error(f"Error freezing model layers: could not find layer {freeze}")
            layer_index = layer_names.index(freeze)
            logging.info(f"Freezing layers before {freeze} ({layer_index})")
            for layer in model.layers[:layer_index]:
                logging.info(f"Freezing layer: {layer.name}")
                layer.trainable = False

        model.compile(
            optimizer=optimizer,
            loss=config['training']['loss'],
            metrics=[
                'categorical_accuracy',
                tf.keras.metrics.Recall(),
                tf.keras.metrics.Precision(),
            ])

        # tf.keras.metrics.MeanIoU(num_classes=config['model']['classes'])
        callbacks=[]
        if args.hparams_metric:
            callbacks.append(BestMetricCallback(args.hparams_metric, args.hparams_function))
            callbacks.append(hp.KerasCallback('.', config_to_hparams(config)))

        callbacks.extend([
            ScalarLogger(model.optimizer.lr, "learning_rate"),
            tf.keras.callbacks.TensorBoard('.', write_graph=False),
            tf.keras.callbacks.ModelCheckpoint(
                filepath=os.path.join(args.model_dir, 'model_{epoch:03d}_{val_loss:.3f}.h5'),
                save_best_only=True, save_weights_only=False, verbose=1),
            tf.keras.callbacks.ModelCheckpoint(
                filepath=os.path.join(args.model_dir, 'weights_{epoch:03d}_{val_loss:.3f}.h5'),
                save_best_only=True, save_weights_only=True, verbose=1),
            tf.keras.callbacks.TerminateOnNaN(),
            tf.keras.callbacks.CSVLogger('./logs.csv')
        ])

        if 'schedule' in config['training'] and config['training']['schedule']:
            callbacks.append(setup_schedule(config, steps_per_epoch))

        os.makedirs(args.model_dir, exist_ok=True)
        with open(os.path.join(args.model_dir, "model_def.json"), 'w') as file:
            file.write(model.to_json())

        model.fit(
            gen_train,
            steps_per_epoch=steps_per_epoch,
            validation_data=gen_val,
            validation_steps=validation_steps,
            epochs=config['training']['epochs'],
            callbacks=callbacks,
            initial_epoch=0,
            verbose=1)
    return 0


if __name__ == '__main__':
    exit(main(get_args()))
