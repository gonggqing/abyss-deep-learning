#!/usr/bin/env python3

"""
Initiated on 2019-01-10


"""

import argparse
import os
import sys
import time

import keras
import numpy as np
import tensorflow as tf

# Change these to absolute imports if you copy this script outside the keras_retinanet package.
from keras_retinanet import models
from keras_retinanet.utils.config import read_config_file, parse_anchor_parameters
from keras_retinanet.utils.keras_version import check_keras_version
from keras_retinanet.utils.image import read_image_bgr, preprocess_image, resize_image

import keras.models
from keras_retinanet import layers
from keras_retinanet.models import assert_training_model
from keras_retinanet.models.retinanet import __build_anchors, retinanet
from keras_retinanet.utils.anchors import AnchorParameters
from keras_retinanet.utils.visualization import draw_box, draw_caption

from abyss_deep_learning.utils import tile_gen

# Allow relative imports when being executed as script.
if __name__ == "__main__" and __package__ is None:
    sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))
    __package__ = "keras_retinanet.bin"


def get_session():
    """ Construct a modified tf session.
    """
    config = tf.ConfigProto()
    config.gpu_options.allow_growth = True
    return tf.Session(config=config)


def retinanet_bbox(
        model=None,
        nms=True,
        class_specific_filter=True,
        nms_threshold=0.5,
        name='retinanet-bbox',
        anchor_params=None,
        **kwargs
):
    """ Construct a RetinaNet model on top of a backbone and adds convenience functions to output boxes directly.

    This model uses the minimum retinanet model and appends a few layers to compute boxes within the graph.
    These layers include applying the regression values to the anchors and performing NMS.

    Args
        model                 : RetinaNet model to append bbox layers to. If None, it will create a RetinaNet model using **kwargs.
        nms                   : Whether to use non-maximum suppression for the filtering step.
        class_specific_filter : Whether to use class specific filtering or filter for the best scoring class only.
        name                  : Name of the model.
        anchor_params         : Struct containing anchor parameters. If None, default values are used.
        *kwargs               : Additional kwargs to pass to the minimal retinanet model.

    Returns
        A keras.models.Model which takes an image as input and outputs the detections on the image.

        The order is defined as follows:
        ```
        [
            boxes, scores, labels, other[0], other[1], ...
        ]
        ```

    NOTE
    This function was pulled from keras_retinanet.models.retinanet.retinant_bbox
    Edited here to allow passthrough of nms_threshold
    """

    # if no anchor parameters are passed, use default values
    if anchor_params is None:
        anchor_params = AnchorParameters.default

    # create RetinaNet model
    if model is None:
        model = retinanet(num_anchors=anchor_params.num_anchors(), **kwargs)
    else:
        assert_training_model(model)

    # compute the anchors
    features = [model.get_layer(p_name).output for p_name in ['P3', 'P4', 'P5', 'P6', 'P7']]
    anchors = __build_anchors(anchor_params, features)

    # we expect the anchors, regression and classification values as first output
    regression = model.outputs[0]
    classification = model.outputs[1]

    # "other" can be any additional output from custom submodels, by default this will be []
    other = model.outputs[2:]

    # apply predicted regression to anchors
    boxes = layers.RegressBoxes(name='boxes')([anchors, regression])
    boxes = layers.ClipBoxes(name='clipped_boxes')([model.inputs[0], boxes])

    # filter detections (apply NMS / score threshold / select top-k)
    detections = layers.FilterDetections(
        nms=nms,
        nms_threshold=nms_threshold,
        class_specific_filter=class_specific_filter,
        name='filtered_detections'
    )([boxes, classification] + other)

    # construct the model
    return keras.models.Model(inputs=model.inputs, outputs=detections, name=name)


def parse_args():
    """ Parse the arguments.
    """
    parser = argparse.ArgumentParser(description='Evaluation script for a RetinaNet network.')
    parser.add_argument('images',
                        help='Text file of image paths',
                        type=str)
    parser.add_argument('--weights',
                        help='Path to RetinaNet model weights.',
                        type=str)
    parser.add_argument('--convert-model',
                        help='Convert the model to an inference model (ie. the input is a training model).',
                        action='store_true')
    parser.add_argument('--backbone',
                        help='The backbone of the model. default is %(default)s',
                        default='resnet50')
    parser.add_argument('--gpu',
                        help='Id of the GPU to use (as reported by nvidia-smi).')
    parser.add_argument('--score-threshold',
                        help='Threshold on score to filter detections with. default is %(default)s',
                        default=0.05,
                        type=float)
    parser.add_argument('--nms-threshold',
                        help='IoU threshold for non-maximum supression for outputs. default is %(default)s',
                        default=0.1,
                        type=float)
    parser.add_argument('--max-detections',
                        help='Max Detections per image. default is %(default)s.',
                        default=100,
                        type=int)
    parser.add_argument('--save-path',
                        type=str,
                        help='Directory for saving images with detections. Individual CSV files will be produced for '
                             'each image that has detections. If no detections are found in the image then the CSV '
                             'file will be empty. If not specified, detections are output to stdout.')
    parser.add_argument('--show',
                        help='Show per-image examples in matplotlib',
                        action='store_true')
    parser.add_argument('--image-min-side',
                        help='Rescale the image so the smallest side is min_side. If omitted, the min value of the '
                             'first images height and width will be used. In this case, '
                             'if the rest of the given images do not have the same dimension, the script will stop.',
                        type=int, )
    parser.add_argument('--image-max-side',
                        help='Rescale the image if the largest side is larger than max_side. If omitted, '
                             'the max value of the first images height and width will be used. In this case, '
                             'if the rest of the given images do not have the same dimension, the script will stop.',
                        type=int, )
    parser.add_argument('--tile-size',
                        type=str,
                        help='Comma separated values for [height],[width] of tiles to crop from each image')
    parser.add_argument('--stride',
                        type=str,
                        help='Comma separated values for [height],[width] of strides to take when tiling the image')
    parser.add_argument('--config',
                        help='Path to a configuration parameters .ini file (only used with --convert-model).')
    parser.add_argument('--remove',
                        help="Remove negative category detections from output as the network outputs fixed size "
                             "detections",
                        action='store_true')
    parser.add_argument('-v', '--verbose',
                        help="More verbose output to stderr",
                        action='store_true')
    args = parser.parse_args()

    try:
        args.tile_size = [int(num) for num in args.tile_size.split(',')]
    except AttributeError:
        pass

    try:
        args.stride = [int(num) for num in args.stride.split(',')]
    except AttributeError:
        pass

    # EXAMPLE USAGE to be added to --help output
    # cat ~/data/anadarko/object-recognition/val_annotations.csv | sed 's/images/\/home\/users\/sba\/data\/anadarko\/object-recognition\/images/g' | cut -d, -f1 | sort | uniq | ~/src/abyss/deep-learning/applications/retinanet-predict --score-threshold=0.5 --image-min-side=1000 --image-max-side=1000 ~/scratch/anadarko/object-recognition/snapshots/resnet50_csv_100.h5 --convert-model > ~/data/anadarko/object-recognition/val_predictions.csv

    return args


def draw_result(boxes, scores, labels, image):
    import webcolors
    import matplotlib.pyplot as plt

    color_strings = ['green', 'blue', 'yellow', 'red']
    color_rgb_list = []
    for color_string in color_strings:
        color_rgb_list.append(tuple(webcolors.name_to_rgb(color_string)))

    labels_to_names = {0: 'PF-G', 1: 'PF-L', 2: 'PF-M', 3: 'PF-H'}
    names_to_labels = {}
    for k, v in labels_to_names.items():
        names_to_labels[v] = k

    # visualize detections
    for box, score, label in zip(boxes[0], scores[0], labels[0]):
        # scores are sorted so we can break
        if score < 0.5:
            break

        #         color = label_color(label)
        b = box.astype(int)
        draw_box(image, b, color=color_rgb_list[label])

        caption = "{} {:.3f}".format(labels_to_names[label], score)
        draw_caption(image, b, caption)

    fig, ax = plt.subplots(1, 1, figsize=(15, 15))

    ax.imshow(image)
    ax.axis('off')
    plt.show()


def main(args=None):
    script_time = time.perf_counter()
    # make sure keras is the minimum required version
    check_keras_version()

    # optionally choose specific GPU
    if args.gpu:
        os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu
    keras.backend.tensorflow_backend.set_session(get_session())

    # make save path
    try:
        os.makedirs(args.save_path, exist_ok=True)
    except TypeError:
        pass

    # optionally load config parameters
    if args.config:
        args.config = read_config_file(args.config)

    # # create the generator
    # generator = create_generator(args)

    # optionally load anchor parameters
    anchor_params = None
    if args.config and 'anchor_parameters' in args.config:
        anchor_params = parse_anchor_parameters(args.config)

    # load the model
    print_verbose('Loading model, this may take a second...', verbose=True)
    model = models.load_model(args.weights, backbone_name=args.backbone)

    # optionally convert the model
    if args.convert_model:
        model = retinanet_bbox(model=model, nms=True, nms_threshold=args.nms_threshold, anchor_params=anchor_params)

    # start evaluation
    with open(args.images) as f:
        images = f.read().split("\n")

    tile_size = args.tile_size
    stride = args.stride
    image_min_side = args.image_min_side
    image_max_side = args.image_max_side
    output_path = "stdout"
    for image_num, image_path in enumerate(images):
        if not image_path:
            continue

        image_time = time.perf_counter()
        image_path = image_path.strip()
        print_verbose("Processing image {image_num} at {image_path}".format(image_num=image_num + 1,
                                                                            image_path=image_path),
                      verbose=True)

        image = read_image_bgr(image_path)
        image_height, image_width = image.shape[:2]

        if tile_size is None:
            tile_size = image_height, image_width

        if stride is None:
            stride = tile_size
            
        # Values used for cropping tiles from the image
        stride_height, stride_width = stride
        tile_height, tile_width = tile_size

        # Generator of tile images
        image_tiles = tile_gen(image, (tile_height, tile_width), (stride_height, stride_width))

        # Stores all results from each tile for one image
        image_results = []
        num_tiles_y = int(np.floor((image_height - tile_height) / stride_height + 1))
        num_tiles_x = int(np.floor((image_width - tile_width) / stride_width + 1))

        # Used to evaluate overlapping bboxes for different tiles
        centroids = []
        tile_indices = []
        for i in range(num_tiles_x * num_tiles_y):
            col = i % num_tiles_x
            row = i // num_tiles_x
            tile_indices.append((col, row))
            centroid_x = col * stride_width + tile_width / 2
            centroid_y = row * stride_height + tile_height / 2
            centroids.append((centroid_y, centroid_x))
        centroids = np.asarray(centroids)

        if args.image_min_side is None:
            if image_min_side is None:
                image_min_side = min(tile_size)
                print_verbose('image min side has been omitted, minimum value of {} will be taken from the first '
                              'image. If other images do not have same dimensions, the script will stop'
                              .format(image_min_side), verbose=True)
            elif image_min_side != min(tile_size):
                print_and_die("inconsistent image dimensions in file given and no tile size has been specified. "
                              "exiting")

        if args.image_max_side is None:
            if image_max_side is None:
                image_max_side = max(tile_size)
                print_verbose('image max side has been omitted, maximum value of {} will be taken from the first '
                              'image. If other images do not have same dimensions, the script will stop'
                              .format(image_max_side), verbose=True)
            elif image_max_side != max(tile_size):
                print_and_die("inconsistent image dimensions in file given and no tile size has been specified. "
                              "exiting")

        for tile_num, tile in enumerate(image_tiles):
            tile_time = time.perf_counter()
            col, row = tile_indices[tile_num]

            # bbox offsets of tile in regards to starting pixel of original image
            offset_x = stride_width * col
            offset_y = stride_height * row

            offsets = np.asarray((offset_x, offset_y, offset_x, offset_y))

            if args.show:
                import cv2
                draw = cv2.cvtColor(tile.copy(), cv2.COLOR_BGR2RGB)

            # Pre-process image for network
            tile, scale = resize_image(preprocess_image(tile),
                                       min_side=image_min_side,
                                       max_side=image_max_side)

            # process image
            boxes, scores, labels = model.predict_on_batch(np.expand_dims(tile, axis=0))

            # correct for image scale
            boxes /= scale

            bbox_score_category_output = np.concatenate((boxes[0, :], scores.T, labels.T), axis=1)

            # visualize detections
            if args.show:
                draw_result(boxes, scores, labels, draw)

            for box_idx in range(bbox_score_category_output.shape[0]):
                # Early exit for max detections per image
                if box_idx >= args.max_detections:
                    break

                # Early exit for invalid detections
                bbox = bbox_score_category_output[box_idx, :]
                if bbox[-1] == -1 and args.remove:
                    break

                bbox[:4] += offsets
                bbox_centroid = np.asarray(((bbox[0] + (bbox[2] - bbox[0]) / 2), bbox[1] + (bbox[3] - bbox[1]) / 2))
                closest_tile_idx = np.argmin(np.linalg.norm(centroids - bbox_centroid, axis=1))
                if closest_tile_idx == tile_num:
                    image_results.append(bbox)

            print_verbose("processing time for tile {}: {}h {}m {}s".format(tile_num + 1,
                                                                            *pretty_time(
                                                                                time.perf_counter() - tile_time)),
                          verbose=True)

        if args.save_path:
            output_path = os.path.join(args.save_path,
                                       os.path.splitext(os.path.basename(image_path))[0] + '.predictions.csv')
            output_stream = open(output_path, 'w')
        else:
            output_stream = sys.stdout
        print_verbose("writing to {}".format(output_path), verbose=True)
        if image_results:
            output = np.vstack(image_results)
            for tile_num in range(output.shape[0]):
                record = output[tile_num, :]
                print('{},{},'.format(image_path, tile_num), end='', file=output_stream)
                np.savetxt(output_stream, record[None, :], delimiter=',', fmt='%.4f')
        if args.save_path:
            output_stream.close()

        print_verbose("processing time for image {}: {}h {}m {}s".format(image_path,
                                                                         *pretty_time(
                                                                             time.perf_counter() - image_time)),
                      verbose=args.verbose)
    print_verbose("processing time for script: {}h {}m {}s".format(*pretty_time(time.perf_counter() - script_time)),
                  verbose=args.verbose)
    sys.exit(0)


def print_and_die(*args, **kwargs):
    print_verbose(*args, verbose=True, **kwargs)
    sys.exit(1)


def print_verbose(*args, verbose=False, **kwargs):
    if verbose:
        print(os.path.basename(__file__) + ': ', *args, **kwargs, file=sys.stderr)


def pretty_time(seconds: float):
    minutes, seconds = divmod(seconds, 60)
    hours, minutes = divmod(minutes, 60)
    return int(hours), int(minutes), seconds


if __name__ == '__main__':
    main(args=parse_args())
