#!/usr/bin/env python3
from __future__ import print_function
from collections import defaultdict
from contextlib import redirect_stdout
from pprint import pprint
import argparse
import importlib
import json
import os
import sys

from scipy.ndimage.measurements import center_of_mass
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

from abyss.utils import JsonNumpyEncoder
from abyss_deep_learning.metrics import result_to_series, calc_image_stats

def sanity_check_masks(dataset, num_images=4):
    # Load and display random samples
    image_ids = np.random.choice(dataset.image_ids, num_images)
    for image_id in image_ids:
        image = dataset.load_image(image_id)
        mask, class_ids = dataset.load_mask(image_id)
        visualize.display_top_masks(
            image, mask, class_ids, dataset.class_names)
        plt.show()


def get_ax(rows=1, cols=1, size=8):
    return plt.subplots(rows, cols, figsize=(size*cols, size*rows))[1]

def display_gt(dataset, image_id, ax=None):
    image = dataset.load_image(image_id)
    mask, class_ids = dataset.load_mask(image_id)
    bbox = utils.extract_bboxes(mask)
    visualize.display_instances(
        image, bbox, mask, class_ids, dataset.class_names, ax=ax)

def calc_position_error(mask1, mask2):
    pos1 = np.array(center_of_mass(mask1 > 0))
    pos2 = np.array(center_of_mass(mask2 > 0))
    return np.sqrt(np.sum((pos1 - pos2) ** 2.0))

def compute_centroid_errors(gt_boxes, gt_class_ids, gt_masks,
                            pred_boxes, pred_class_ids, pred_scores, pred_masks,
                            iou_thresh=0.5):
    raise NotImplementedError()

def calc_stats(dataset):
    TP, FP, FN = dataset['TP'].sum(), dataset['FP'].sum(), dataset['FN'].sum()
    out = {
        'precision': TP / max(1, TP + FP),
        'recall': TP / max(1, TP + FN), # True positive rate
        'F1': 2 * TP / max(1, 2*TP + FP + FN),
    }
    for metric in out.keys():
        if not np.isfinite(out[metric]):
            out[metric] = 0.0
    return out

def calc_dataset_metrics(dataset):
    dataset_stats = []
    for (match, class_id, score_thresh), group in dataset.groupby(('match', 'class_id', 'score_thresh')):
        class_stats = calc_stats(group)
        class_stats['class_id'] = class_id
        class_stats['match'] = match
        class_stats['score_thresh'] = score_thresh
        dataset_stats.append(class_stats)
    return pd.DataFrame(dataset_stats)

def output_metrics(dataset_metrics):
    results = defaultdict(lambda: defaultdict(dict))
    for index, value in dataset_metrics.groupby(('match', 'score_thresh', 'class_id')):
        for i, key in enumerate(index):
            if i == 0:
                nested = results[str(key)]
            elif i == len(index) - 1:
                nested[str(key)] = value[['precision', 'recall', 'F1']].to_dict(orient='list')
            else:
                nested = nested[str(key)]
    return results

def test(config, args):
    config.NUM_CLASSES = args.dataset_test.num_classes
    model = modellib.MaskRCNN(
        mode="inference", config=config, model_dir=args.model_dir)
    if args.weights == 'last':
        model.load_weights(model.find_last()[1], by_name=True)
    elif args.weights is not None:
        model.load_weights(args.weights, by_name=True)
    print("Loaded weights! Beginning testing.", file=sys.stderr)

    class_names = [
        i[1] for i in
        sorted([
            (cat['id'], cat['name'])
            for cat in args.dataset_test.class_info],
            key=lambda x: x[0]
        )
    ]
    match_kinds = ['one-to-one', 'many-to-one', 'many-to-many']
    results = []
    unique_scores = []
    print('Processing through {} images'.format(len(args.dataset_test.image_ids)))
    for image_count, image_id in enumerate(args.dataset_test.image_ids):
        try:
            if args.num_images is not None and image_count > args.num_images:
                break
            # Load image and ground truth data
            image, image_shape, gt_class_ids, gt_bbox, gt_mask = \
                modellib.load_image_gt(
                    args.dataset_test, config, image_id, use_mini_mask=False)
            # molded_images = np.expand_dims(modellib.mold_image(image, config), 0)
            predicted = model.detect([image], verbose=0)[0]
            r = predicted
            predicted = result_to_series(predicted) if len(predicted['class_ids']) else None
            if predicted is not None:
                unique_scores.append(predicted['score'])
            object_gts = result_to_series({
                'rois': gt_bbox[:, :4],
                'masks': gt_mask,
                'class_ids': gt_class_ids
            }) if len(gt_class_ids) else None
            results.append((image_id, predicted, object_gts))

            if args.show:
                if r is not None:
                    plt.figure()
                    plt.subplot(1, 2, 1)
                    
                    visualize.display_instances(
                        image, r['rois'], r['masks'], r['class_ids'], class_names, r['scores']
                    )
                    plt.title('predicted')
                    plt.subplot(1, 2, 2)
                    display_gt(args.dataset_test, image_id)
                    plt.title('ground truth')
                    plt.show()


        except ValueError as e:
            print("GOT AN ERROR", file=sys.stderr)
            print(e, file=sys.stderr)

    unique_scores = np.sort(pd.concat(unique_scores).unique())
    image_stats = []
    for score_thresh in np.linspace(unique_scores[0], unique_scores[-1], 50):
        for matching in match_kinds:
            for image_id, predicted, object_gts in results:
                subset = predicted[predicted['score'] > score_thresh] if predicted is not None else None
                if subset is None or len(subset) == 0:
                    predicted = None
                # print(predicted)
                stats = calc_image_stats(subset, object_gts, iou_thresh=args.iou, matching=matching)
                for class_stats in stats:
                    class_stats['image_id'] = image_id
                    class_stats['score_thresh'] = score_thresh
                image_stats.append(pd.DataFrame(stats))
    
    dataset_stats = pd.concat(image_stats).reset_index(drop=True)
    dataset_stats.to_csv('/tmp/dataset_stats.pkl')

    # print(dataset_stats, file=sys.stderr)
    dataset_metrics = calc_dataset_metrics(dataset_stats)
    dataset_metrics.to_csv('/tmp/dataset_metrics.pkl')
    # print(dataset_metrics, file=sys.stderr)
    dataset_metrics = output_metrics(dataset_metrics)
    print(json.dumps(
        dataset_metrics, sort_keys=True, indent=0, separators=(',', ':'), cls=JsonNumpyEncoder))

def main(args):
    if args.cpu:
        os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
        os.environ["CUDA_VISIBLE_DEVICES"] = ""
    with redirect_stdout(sys.stderr):
        args.dataset_test = dataset_model()
        args.dataset_test.load_coco(
            args.dataset_test_path, image_dir=args.image_dir, class_ids=args.categories)
        args.dataset_test.prepare()
    spec = importlib.util.spec_from_file_location(
        "maskrcnn_config", args.config)
    config_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(config_module)
    config = config_module.InferenceConfig()
    if args.sanity_check:
        print(args, file=sys.stderr)
        config.display()
        sanity_check_masks(args.dataset_test)
    test(config, args)


def get_args():
    '''Get args from the command line args'''
    parser = argparse.ArgumentParser(
        description="Statistical testing of Resnet Mask RCNN")
    parser.add_argument(
        "config", help="Use this config file (see default MaskRCNN.config.py)", default=None)
    parser.add_argument("dataset_test_path",
                        help="Path to the coco JSON for the testing set.")
    parser.add_argument("model_dir", help="Path to save and load models from.")
    parser.add_argument(
        "weights",
        help="Path to pretrained weights, or 'last' to load last model trained.",
        default=None
    )
    parser.add_argument(
        "--categories", help="Only train on images that have this group of categories", default=None)
    parser.add_argument(
        "--cpu", help="Use CPU instead of GPU", action='store_true')
    parser.add_argument(
        "--image-dir",
        help="Base dir of the images referred to relatively from the COCO JSON",
        default=None
    )
    parser.add_argument(
        "--iou", help="IoU threshold for stats.", default=0.5, type=float)
    parser.add_argument(
        "--num-images", help="Number of images to test (default all)", default=None, type=int)
    parser.add_argument(
        "--sanity-check",
        help="Show train and validation datasets to ensure that data is valid.",
        action='store_true'
    )
    parser.add_argument(
        "--show", help="Display outputs interactively.", action='store_true')
    args = parser.parse_args()
    if args.categories != None:
        args.categories = [int(i) for i in args.categories.split(',')]
    return args


if __name__ == '__main__':
    # Put ahead to make --help faster
    ARGS = get_args()

from abyss_deep_learning.abyss_dataset import CocoDataset as dataset_model
import abyss_maskrcnn.coco as coco
import abyss_maskrcnn.model as modellib
import abyss_maskrcnn.utils as utils
import abyss_maskrcnn.visualize as visualize

if __name__ == '__main__':
    main(ARGS)
