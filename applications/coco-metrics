#!/usr/bin/env python3
import argparse
import json
import os
import sys
from collections import defaultdict
from operator import itemgetter
from pathlib import Path
from abyss_deep_learning import metrics

description = """

calculate metrics on predictions vs labels

run coco-metrics <operation> --help for operation option    
"""

tfpn_description = """

take predictions.json, truth.json, output to stdout coco annotations labeled as TP, FP, TN

usage: cat predictions.json | coco-metrics --truth truth.json > tfpn.json

categories in predictions.json and truth.json should be the same, no checks performed
"""

def tfpn(args):
    predictions = json.loads(sys.stdin.read())
    f = open(args.truth)
    truth = json.loads(f.read())
    
    
    # category -> category_id
    
    
    category_ids = []
    for c in truth['categories']: category_ids.append(c['id'])
    images = {}
    if args.bounding_boxes:
        for annotation in predictions['annotations']:
            if annotation['score'] < args.score_threshold: continue
            image_id = annotation['image_id']
            category_id = annotation['category_id']
            if not category_id in category_ids:
                print( 'coco-metrics: expected category id in', category_ids, '; got:', category_id, '; discarded' )
                continue
            if not image_id in images: images[image_id] = {}
            if not category_id in images[image_id]: images[image_id][category_id] = ( [], [] )
            images[image_id][category_id][0].append( annotation['bbox'] ) # todo: make sure coordinate order is correct (x,y vs y,x?)        
        for annotation in truth['annotations']:
            image_id = annotation['image_id']
            if image_id not in images: images[image_id] = {}
            category_id = annotation['category_id']
            if not category_id in images[image_id]: images[image_id][category_id] = ( [], [] )
            images[image_id][category_id][1].append( annotation['bbox'] ) # todo: make sure coordinate order is correct (x,y vs y,x?)        
    annotations = {} # todo: match annotation ids?
    match = eval( args.match() )
    for image_id, image in images.items():
        for i in range( len( category_ids ) ):
            category_id = category_ids[i]
            if len( image[category_id] ) == 0:
                1 == 1
                
                
                # todo
                
                
            else:
                iou = metrics.bbox_iou_matrix( image[category_id][0], image[category_id][1] ) # only to get iou scores; remove, if scores not required
                tp, fp, tn, fn = metrics.tp_fp_tn_fn( image[category_id][0], image[category_id][1], args.iou_threshold, match )
                
                
                # todo:         mark TP
                # todo:         mark FP
                # todo:         append FN
    predictions['categories'] = []
    if args.flat_categories:
        predictions['categories'] += [ { 'name': 'TP', 'supercategory': '', 'id': 0 }
                                     , { 'name': 'FP', 'supercategory': '', 'id': 1 }
                                     , { 'name': 'TN', 'supercategory': '', 'id': 2 }
                                     , { 'name': 'FN', 'supercategory': '', 'id': 3 } ]
    else:
        i = 0
        for c in truth['categories']:
            predictions['categories'] += [ { 'name': 'TP', 'supercategory': c['name'], 'id': i }
                                         , { 'name': 'FP', 'supercategory': c['name'], 'id': i + 1 }
                                         , { 'name': 'TN', 'supercategory': c['name'], 'id': i + 2 }
                                         , { 'name': 'FN', 'supercategory': c['name'], 'id': i + 3 } ]
            i += 4
    predictions['annotations'] = annotations
    json.dump(predictions, sys.stdout, indent=4)

def get_args():
    parser = argparse.ArgumentParser(description=description, formatter_class=argparse.RawDescriptionHelpFormatter)
    subparsers = parser.add_subparsers(title="operations", help="available operations")
    tfpn_parser = subparsers.add_parser('tfpn', description=tfpn_description, help="take predictions.json, truth.json, output to stdout coco annotations labeled as TP, FP, TN", formatter_class=argparse.RawDescriptionHelpFormatter)
    tfpn_parser.set_defaults( func=tfpn )
    tfpn_parser.add_argument( '--bounding-boxes', '--bbox', '-b', action='store_true', help="match bounding boxes" )
    tfpn_parser.add_argument( '--flat-categories', '--flat', action='store_true', help="output just four categories: TP, FP, TN, and FN" )
    tfpn_parser.add_argument( '--iou-threshold', default=0.5, type=float, help="iou threshold; default: %(default)s" )
    tfpn_parser.add_argument( '--match', default='one_to_one', type=str, help="how to match, 'one_to_one' or 'one_to_many'; default: %(default)s" )
    tfpn_parser.add_argument( '--score-threshold', default=0.5, type=float, help="score threshold: default: %(default)s" )
    tfpn_parser.add_argument( '--truth', '-t', type=str, help="ground truth coco.json file" )
    return parser.parse_args()

def main():
    args = get_args()
    args.func(args)
    
if __name__ == '__main__':
    main()
