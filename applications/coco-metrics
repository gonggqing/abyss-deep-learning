#!/usr/bin/env python3
import argparse
import itertools
import json
import numpy as np
import sys
from datetime import datetime

from abyss_deep_learning import metrics

description = """

calculate metrics on predictions vs labels

run coco-metrics <operation> --help for operation option    
"""

tfpn_description = """

take predictions.json, truth.json, output to stdout coco annotations labeled as TP, FP, TN

usage: cat predictions.json | coco-metrics confusion-matrix --truth truth.json --bounding-boxes > confusion_matrix.csv

limitations:
    - categories in predictions.json and truth.json should be the same, no checks performed
    - output annotation ids do NOT match the prediction or ground thruth ids, since there is
      no way to make them unique across output; todo: work out required semantics (e.g.
      optionally output one category, e.g. TP, only)
"""

confusion_matrix_description = """

take predictions.json, truth.json, output to stdout confusion matrix as csv

usage: cat predictions.json | coco-metrics confusion-matrix --truth truth.json -b > confusion-matrix.csv

"""

confusions_description = """

take predictions.json, truth.json, output to stdout coco annotations labeled as confusions among categories with iou as score

usage: cat predictions.json | coco-metrics confusions --truth truth.json -b > confusions.json

"""

def load_annotations( predicted, truth, category_ids, bounding_boxes = False, score = None ):
    images = {}
    if bounding_boxes:
        for annotation in predicted['annotations']:
            if not score is None and 'score' in annotation and annotation['score'] < score: continue
            image_id = annotation['image_id']
            category_id = annotation['category_id']
            if not category_id in category_ids:
                print( 'coco-metrics: expected category id in', category_ids, '; got:', category_id, '; discarded', file = sys.stderr )
                continue
            if not image_id in images: images[image_id] = {}
            if not category_id in images[image_id]: images[image_id][category_id] = ( [], [], [], [] )
            bbox = annotation['bbox']
            images[image_id][category_id][0].append( [ bbox[1], bbox[0], bbox[1] + bbox[3], bbox[0] + bbox[2] ] )
            images[image_id][category_id][1].append( annotation['id'] ) # quick and dirty
        for annotation in truth['annotations']:
            image_id = annotation['image_id']
            if image_id not in images: images[image_id] = {}
            category_id = annotation['category_id']
            if not category_id in images[image_id]: images[image_id][category_id] = ( [], [], [], [] )
            bbox = annotation['bbox']
            images[image_id][category_id][2].append( [ bbox[1], bbox[0], bbox[1] + bbox[3], bbox[0] + bbox[2] ] )
            images[image_id][category_id][3].append( annotation['id'] ) # quick and dirty
        return images
    print( 'coco-metrics: only loading bounding boxes implemented; for bounding boxes, please specify --bounding-boxes', file = sys.stderr )
    sys.exit( 1 )

def plot_confusion_matrix(m, classes, normalized = False):
    import matplotlib.pyplot as plt
    plt.imshow(m, interpolation='nearest', cmap=plt.cm.Blues)
    plt.title('confusion matrix')
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)
    fmt = '.2f' if normalized else 'd'
    thresh = m.max() / 2.
    for i, j in itertools.product(range(m.shape[0]), range(m.shape[1])):
        plt.text(j, i, format(m[i, j], fmt), horizontalalignment="center", color="white" if m[i, j] > thresh else "black")
    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.show()
    
def confusions(args):
    predicted = json.loads(sys.stdin.read())
    f = open(args.truth)
    truth = json.loads(f.read())
    category_ids = []
    for c in truth['categories']: category_ids.append(c['id'])
    images = load_annotations( predicted, truth, category_ids, score = args.score_threshold, bounding_boxes = args.bounding_boxes )
    iou_matrix = metrics.bbox_iou_matrix if args.bounding_boxes else None
    result = {}
    result['categories'] = []
    confusion_category_ids = {}
    i = 0
    for c in [{ 'name': 'BG', 'supercategory': '' }] + truth['categories']:
        for d in [{ 'name': 'BG', 'supercategory': '' }] + truth['categories']:
            result['categories'].append( { 'id': i, 'name': c['name'] + ',' + d['name'], 'supercategory': c['name'] } )
            confusion_category_ids[(c['name'], d['name'])] = i
            i += 1
    result['annotations'] = []
    for image_id, image in images.items():
        p = []
        t = []
        pc = []
        tc = []
        for category_id, annotations in image.items():
            p += annotations[0]
            t += annotations[2]
            pc += [category_id]*len(annotations[0])
            tc += [category_id]*len(annotations[2])
        #if len( p ) == 0:
            # todo
            #pr = [blank_id]*len( tc )
            #tr = tc
        #elif len( t ) == 0:
            # todo
            #pr = pc
            #tr = [blank_id]*len( pc )
        #else:
            #m = iou_matrix( p, t ) > args.iou_threshold
    
    # todo
    
    result['images'] = truth['images']
    result['licenses'] = [{
        'id': 1,
        'url': 'http://creativecommons.org/licenses/by-nc-sa/2.0/',
        'name': 'Attribution-NonCommercial-ShareAlike License'
    }]
    result['info'] = {
        'contributor': 'Abyss Solutions',
        'total_time': '00h00m00s',
        'year': 2019,
        'date_created': datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f'),
        'description': 'This is a dataset configured by Abyss Solutions.',
        'version': '1.0',
        'url': 'http://www.abysssolutions.com.au',
    }
    json.dump(result, sys.stdout, indent=4)
    
def confusion_matrix(args):
    predicted = json.loads(sys.stdin.read())
    f = open(args.truth)
    truth = json.loads(f.read())
    category_ids = []
    for c in truth['categories']: category_ids.append(c['id'])
    images = load_annotations( predicted, truth, category_ids, score = args.score_threshold, bounding_boxes = args.bounding_boxes )
    iou_matrix = metrics.bbox_iou_matrix if args.bounding_boxes else None
    predicted_labels = [] # quick and dirty, watch performance
    truth_labels = [] # quick and dirty, watch performance
    blank_id = np.max( category_ids ) + 1
    for image_id, image in images.items():
        p = []
        t = []
        pc = []
        tc = []
        for category_id, annotations in image.items():
            p += annotations[0]
            t += annotations[2]
            pc += [category_id]*len(annotations[0])
            tc += [category_id]*len(annotations[2])
        if len( p ) == 0:
            pr = [blank_id]*len( tc )
            tr = tc
        elif len( t ) == 0:
            pr = pc
            tr = [blank_id]*len( pc )
        else:
            pr, tr = metrics.bbox_to_sklearn_pred_true( iou_matrix( p, t ), pc, tc, iou_threshold = args.iou_threshold, blank_id = blank_id )
        predicted_labels = np.append( predicted_labels, pr )
        truth_labels = np.append( truth_labels, tr )
    from sklearn import metrics as skm
    m = skm.confusion_matrix( truth_labels, predicted_labels, [blank_id] + category_ids )
    if args.normalize: m = m.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    if args.plot:
        classes = ['BG']
        for c in truth['categories']: classes.append(c['name'])
        plot_confusion_matrix( m, classes, args.normalize )
    np.savetxt( sys.stdout, m, delimiter=',', fmt='%d' )

def tfpn(args):
    predicted = json.loads(sys.stdin.read())
    f = open(args.truth)
    truth = json.loads(f.read())
    category_ids = []
    for c in truth['categories']: category_ids.append(c['id'])
    images = load_annotations( predicted, truth, category_ids, score = args.score_threshold, bounding_boxes = args.bounding_boxes )
    iou_matrix = metrics.bbox_iou_matrix if args.bounding_boxes else None
    match = eval( 'metrics.' + args.match ) # quick and dirty
    result = {}
    result['annotations'] = []
    def bbox_to_coco( box ): return [ box[1], box[0], box[3] - box[1], box[2] - box[0] ]
    def bbox_to_segmentation( box ): return [ box[1], box[0], box[1], box[0] + box[2], box[1] + box[3], box[0] + box[2], box[1] + box[3], box[0] ]
    for image_id, image in images.items():
        for category_id, annotations in image.items():
            category_id_offset = 0 if args.flat_categories else ( category_id - 1 ) * 4
            tp = [[], []]
            fp = tn = fn = []
            if len( annotations[0] ) == 0:
                fn = [ *range( len( annotations[2] ) ) ]
            else:
                if len( annotations[2] ) == 0:
                    fp = [ *range( len( annotations[0] ) ) ]
                else:
                    ious = iou_matrix( annotations[0], annotations[2] ) # todo: if too slow, remove; used only to calculate score as iou
                    tp, fp, tn, fn = metrics.tp_fp_tn_fn( annotations[0], annotations[2], args.iou_threshold, match )
            for k in range( len( tp[0] ) ):
                bbox = bbox_to_coco(annotations[0][tp[0][k]])
                result['annotations'].append( { "image_id": image_id
                                                , "bbox": bbox
                                                , "area": bbox[2] * bbox[3]
                                                , "iscrowd": 0
                                                , "id": len(result['annotations']) # annotations[1][tp[0][k]]
                                                , "score": ious[tp[0][k],tp[1][k]]
                                                , "category_id": 1 + category_id_offset
                                                , 'segmentation': bbox_to_segmentation( bbox ) } )
            for j in fp:
                bbox = bbox_to_coco(annotations[0][j])
                result['annotations'].append( { "image_id": image_id
                                              , "bbox": bbox
                                              , "area": bbox[2] * bbox[3]
                                              , "iscrowd": 0
                                              , "id": len(result['annotations']) # annotations[1][j]
                                              , "category_id": 2 + category_id_offset
                                              , 'segmentation': bbox_to_segmentation( bbox ) } )
            for j in tn:
                bbox = bbox_to_coco(annotations[2][j])
                result['annotations'].append( { "image_id": image_id
                                              , "bbox": bbox
                                              , "area": bbox[2] * bbox[3]
                                              , "iscrowd": 0
                                              , "id": len(result['annotations']) # annotations[3][j]
                                              , "category_id": 3 + category_id_offset
                                              , 'segmentation': bbox_to_segmentation( bbox ) } )
            for j in fn:
                bbox = bbox_to_coco(annotations[2][j])
                result['annotations'].append( { "image_id": image_id
                                              , "bbox": bbox
                                              , "area": bbox[2] * bbox[3]
                                              , "iscrowd": 0
                                              , "id": len(result['annotations']) # annotations[3][j]
                                              , "category_id": 4 + category_id_offset
                                              , 'segmentation': bbox_to_segmentation( bbox ) } )
    result['categories'] = []
    if args.flat_categories:
        result['categories'] += [ { 'name': 'TP', 'supercategory': '', 'id': 1 }
                                , { 'name': 'FP', 'supercategory': '', 'id': 2 }
                                , { 'name': 'TN', 'supercategory': '', 'id': 3 }
                                , { 'name': 'FN', 'supercategory': '', 'id': 4 } ]
    else:
        for c in truth['categories']:
            category_id_offset = ( c['id'] - 1 ) * 4
            result['categories'] += [ { 'name': c['name'] + ',TP', 'supercategory': c['name'], 'id': category_id_offset + 1 }
                                    , { 'name': c['name'] + ',FP', 'supercategory': c['name'], 'id': category_id_offset + 2 }
                                    , { 'name': c['name'] + ',TN', 'supercategory': c['name'], 'id': category_id_offset + 3 }
                                    , { 'name': c['name'] + ',FN', 'supercategory': c['name'], 'id': category_id_offset + 4 } ]
    result['images'] = truth['images']
    result['licenses'] = [{
        'id': 1,
        'url': 'http://creativecommons.org/licenses/by-nc-sa/2.0/',
        'name': 'Attribution-NonCommercial-ShareAlike License'
    }]
    result['info'] = {
        'contributor': 'Abyss Solutions',
        'total_time': '00h00m00s',
        'year': 2019,
        'date_created': datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f'),
        'description': 'This is a dataset configured by Abyss Solutions.',
        'version': '1.0',
        'url': 'http://www.abysssolutions.com.au',
    }
    json.dump(result, sys.stdout, indent=4)

def get_args():
    parser = argparse.ArgumentParser(description=description, formatter_class=argparse.RawDescriptionHelpFormatter)
    subparsers = parser.add_subparsers(title="operations", help="available operations")
    
    tfpn_parser = subparsers.add_parser('tfpn', description=tfpn_description, help="take predictions.json, truth.json, output to stdout coco annotations labeled as TP, FP, TN", formatter_class=argparse.RawDescriptionHelpFormatter)
    tfpn_parser.set_defaults( func=tfpn )
    tfpn_parser.add_argument( '--bounding-boxes', '--bbox', '-b', action='store_true', help="match bounding boxes" )
    tfpn_parser.add_argument( '--flat-categories', '--flat', action='store_true', help="output just four categories: TP, FP, TN, and FN" )
    tfpn_parser.add_argument( '--iou-threshold', default=0.5, type=float, help="iou threshold; default: %(default)s" )
    tfpn_parser.add_argument( '--match', default='one_to_one', type=str, help="how to match, 'one_to_one' or 'one_to_many'; default: %(default)s" )
    tfpn_parser.add_argument( '--score-threshold', default=0.5, type=float, help="score threshold: default: %(default)s" )
    tfpn_parser.add_argument( '--truth', '-t', type=str, help="ground truth coco.json file" )
    
    confusion_matrix_parser = subparsers.add_parser('confusion-matrix', description=confusion_matrix_description, help="take predictions.json, truth.json, output to stdout coco annotations labeled as TP, FP, TN", formatter_class=argparse.RawDescriptionHelpFormatter)
    confusion_matrix_parser.set_defaults( func=confusion_matrix )
    confusion_matrix_parser.add_argument( '--bounding-boxes', '--bbox', '-b', action='store_true', help="match bounding boxes" )
    confusion_matrix_parser.add_argument( '--iou-threshold', default=0.5, type=float, help="iou threshold; default: %(default)s" )
    confusion_matrix_parser.add_argument( '--normalize', action='store_true', help="normalize confusion matrix" )
    confusion_matrix_parser.add_argument( '--plot', action='store_true', help="plot confusion matrix, convenience option" )
    confusion_matrix_parser.add_argument( '--score-threshold', default=0.5, type=float, help="score threshold: default: %(default)s" )
    confusion_matrix_parser.add_argument( '--truth', '-t', type=str, help="ground truth coco.json file" )
    
    confusions_parser = subparsers.add_parser('confusions', description=confusions_description, help="take predictions.json, truth.json, output to stdout coco annotations labeled as confusions among categories with iou as score", formatter_class=argparse.RawDescriptionHelpFormatter)
    confusions_parser.set_defaults( func=confusions )
    confusions_parser.add_argument( '--bounding-boxes', '--bbox', '-b', action='store_true', help="match bounding boxes" )
    confusions_parser.add_argument( '--iou-threshold', default=0.5, type=float, help="iou threshold; default: %(default)s" )
    confusions_parser.add_argument( '--score-threshold', default=0.5, type=float, help="score threshold: default: %(default)s" )
    confusions_parser.add_argument( '--truth', '-t', type=str, help="ground truth coco.json file" )
    
    return parser.parse_args()

def main():
    args = get_args()
    args.func(args)
    
if __name__ == '__main__':
    main()
