#!/usr/bin/env python3
import argparse
import copy
import itertools
import json
import logging
import sys
import time
from datetime import datetime
from typing import Tuple

import numpy as np

from abyss_deep_learning import metrics
from abyss_deep_learning.utils import bbox_to_segmentation

np.set_printoptions(threshold=np.inf)

description = """

calculate metrics on predictions vs labels

run coco-metrics <operation> --help for operation option
"""

tfpn_description = """

take predictions.json, truth.json, output to stdout coco annotations labeled as TP, FP, TN

usage: cat predictions.json | coco-metrics confusion-matrix --truth truth.json --bounding-boxes > confusion_matrix.csv

limitations:
    - categories in predictions.json and truth.json should be the same, no checks performed
    - output annotation ids do NOT match the prediction or ground truth ids, since there is
      no way to make them unique across output; todo: work out required semantics (e.g.
      optionally output one category, e.g. TP, only)
"""

confusion_matrix_description = """

take predictions.json, truth.json, output to stdout confusion matrix as csv

usage: cat predictions.json | coco-metrics confusion-matrix --truth truth.json -b > confusion-matrix.csv

"""

confusion_description = """

calculate confusion between first and second inputs
 output to stdout coco annotations labeled as confusions among categories with iou as score

usage: cat first.json | coco-metrics confusion --second second.json -b > confusion.json
  e.g. cat predictions.json | coco-metrics confusion --second truth.json -b > confusion.json

"""


def load_annotations(first, second, category_ids, args):
    images = {}
    for i, annotation in enumerate(first['annotations']):
        if args.score_threshold is not None and 'score' in annotation and annotation['score'] < args.score_threshold:
            continue
        image_id = annotation['image_id']
        category_id = annotation['category_id']
        if category_id not in category_ids:
            print('coco-metrics: expected category id in', category_ids, '; got:', category_id, '; discarded',
                  file=sys.stderr)
            continue
        if image_id not in images:
            images[image_id] = {}
        if category_id not in images[image_id]:
            images[image_id][category_id] = ([], [], [], [])
        images[image_id][category_id][1].append(i)  # quick and dirty
        if args.bounding_boxes:
            bbox = annotation['bbox']
            images[image_id][category_id][0].append([bbox[1], bbox[0], bbox[1] + bbox[3], bbox[0] + bbox[2]])
        else:
            segmentation = annotation['segmentation']
            if len(segmentation) == 0 or len(segmentation[0]) == 0:
                continue
            if len(segmentation) > 1:
                print('coco-metrics: on annotation id ' + str(
                    annotation['id']) + ': segmentation has more than one polygon; not implemented')
                sys.exit(1)
            images[image_id][category_id][0].append(np.reshape(segmentation[0], (len(segmentation[0]) // 2, 2)))

    for i, annotation in enumerate(second['annotations']):
        # if args.score_threshold is not None and 'score' in annotation and annotation['score'] < args.score_threshold:
        #    continue
        image_id = annotation['image_id']
        if image_id not in images:
            images[image_id] = {}
        category_id = annotation['category_id']
        if category_id not in images[image_id]:
            images[image_id][category_id] = ([], [], [], [])
        images[image_id][category_id][3].append(i)  # quick and dirty
        if args.bounding_boxes:
            bbox = annotation['bbox']
            images[image_id][category_id][2].append([bbox[1], bbox[0], bbox[1] + bbox[3], bbox[0] + bbox[2]])
        else:
            segmentation = annotation['segmentation']
            if len(segmentation) == 0 or len(segmentation[0]) == 0:
                continue
            if len(segmentation) > 1:
                print('coco-metrics: on annotation id ' + str(
                    annotation['id']) + ': segmentation has more than one polygon; not implemented')
                sys.exit(1)
            images[image_id][category_id][2].append(np.reshape(segmentation[0], (len(segmentation[0]) // 2, 2)))
    return images


def plot_confusion_matrix(m, classes, normalized=False, save_fig=False):
    import matplotlib.pyplot as plt
    plt.imshow(m, interpolation='nearest', cmap=plt.cm.Blues)
    plt.title('confusion matrix')
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)
    fmt = '.2f' if normalized else 'd'
    thresh = m.max() / 2.
    for i, j in itertools.product(range(m.shape[0]), range(m.shape[1])):
        plt.text(j, i, format(m[i, j], fmt), horizontalalignment="center",
                 color="white" if m[i, j] > thresh else "black")
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()
    if save_fig:
        plt.savefig('confusion_matrix_normalized.png' if normalized else 'confusion_matrix.png')
    else:
        plt.show()


def confusion(args):
    if args.first is not None:
        with open(args.first, 'r') as f:
            first = json.load(f)
    else:
        first = json.load(sys.stdin)
    with open(args.second) as f:
        second = json.load(f)
    category_ids = []
    for c in second['categories']:
        if c['name'] == 'BG':
            logging.error(
                "confusion: found 'BG' in categories; cannot handle it for now: todo (for now just remove 'BG' from categories in '" + args.second + "')")
            sys.exit(1)
        category_ids.append(c['id'])
    background_id = max(category_ids) + 1
    images = load_annotations(first, second, category_ids, args)
    # for k, v in images.items(): print("--> k:", k, v, file = sys.stderr );
    if args.bounding_boxes:
        iou_matrix = metrics.bbox_iou_matrix
    else:
        iou_matrix = metrics.poly_iou_matrix
    result = {}
    result['categories'] = []
    confusion_category_ids = {}
    i = 0
    for first_cat in [{'name': 'BG', 'supercategory': '', 'id': background_id}] + first['categories']:
        for second_cat in [{'name': 'BG', 'supercategory': '', 'id': background_id}] + second['categories']:
            result['categories'].append(
                {'id': i, 'name': first_cat['name'] + ',' + second_cat['name'], 'supercategory': first_cat['name']})
            confusion_category_ids[(second_cat['id'], first_cat['id'])] = i
            # print( '--> A:', 'i:', i, '(', (t['id'], p['id']), ')', '->', confusion_category_ids[(t['id'], p['id'])], file = sys.stderr )
            i += 1
    # print( '--> B: fp: categories:', confusion_category_ids, file = sys.stderr )
    result['annotations'] = []
    for image_id, image in images.items():
        first_boxes = []
        first_indexes = []
        second_boxes = []
        second_indexes = []
        for category_id, annotations in image.items():  # quick and dirty
            first_boxes += annotations[0]
            first_indexes += annotations[1]
            second_boxes += annotations[2]
            second_indexes += annotations[3]
        first_only = []
        second_only = []
        if len(second_indexes) == 0:
            first_only = first_indexes
        elif len(first_indexes) == 0:
            second_only = second_indexes
        else:
            ious = iou_matrix(first_boxes, second_boxes)
            m = (ious > args.iou_threshold) * 1
            indices = np.nonzero(m)
            for i in range(len(indices[0])):
                a = copy.deepcopy(first['annotations'][first_indexes[indices[0][i]]])
                a['id'] = len(result['annotations'])
                if args.iou_as_score:
                    a['score'] = ious[indices[0][i], indices[1][i]]
                a['category_id'] = confusion_category_ids[
                    (second['annotations'][second_indexes[indices[1][i]]]['category_id'], a['category_id'])]
                if not 'segmentation' in a or a['segmentation'] is None:
                    a['segmentation'] = bbox_to_segmentation(a['bbox'])
                result['annotations'].append(a)
            first_only = np.array(first_indexes)[np.nonzero(np.max(m, axis=1) == 0)[0]]
            second_only = np.array(second_indexes)[np.nonzero(np.max(m, axis=0) == 0)[0]]
        for i in first_only:
            a = copy.deepcopy(first['annotations'][i])
            a['id'] = len(result['annotations'])
            if args.iou_as_score:
                a['score'] = 0
            a['category_id'] = confusion_category_ids[(background_id, a['category_id'])]
            if not 'segmentation' in a or a['segmentation'] is None:
                a['segmentation'] = bbox_to_segmentation(a['bbox'])
            result['annotations'].append(a)
        for i in second_only:
            a = copy.deepcopy(second['annotations'][i])
            a['id'] = len(result['annotations'])
            if args.iou_as_score:
                a['score'] = 0
            a['category_id'] = confusion_category_ids[(a['category_id'], background_id)]
            if not 'segmentation' in a or a['segmentation'] is None:
                a['segmentation'] = bbox_to_segmentation(a['bbox'])
            result['annotations'].append(a)
    # print("--> len(second['images']):", len(second['images']), file = sys.stderr)
    result['images'] = copy.deepcopy(second['images'])
    result['info'] = {
        'contributor': 'Abyss Solutions',
        'total_time': '00h00m00s',
        'year': datetime.now().strftime('%Y'),
        'date_created': datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f'),
        'description': 'This is a dataset configured by Abyss Solutions.',
        'version': '1.0',
        'url': 'http://www.abysssolutions.com.au',
    }
    json.dump(result, sys.stdout, indent=4)


def confusion_matrix(args):
    import sklearn.metrics
    if args.first is not None:
        with open(args.first, 'r') as f:
            predicted = json.load(f)
    else:
        predicted = json.load(sys.stdin)
    with open(args.second) as f:
        truths = json.load(f)
    category_ids = []
    categories = ['BG']
    for category in truths['categories']:
        if category['name'] == 'BG':
            logging.error(
                "confusion: found 'BG' in categories; cannot handle it for now: todo "
                "(for now just remove 'BG' from categories in '{}'".format(args.second))
            return 1

        category_ids.append(category['id'])
        if args.csv_output_per_image and args.header:
            categories.append(category['name'])

    if args.csv_output_per_image and args.header:
        from itertools import product
        print("path,id,{}".format(",".join(["/".join(pair) for pair in product(categories, categories)])))
    tic = time.perf_counter()
    images = load_annotations(predicted, truths, category_ids, args)
    logging.info("confusion-matrix: load annotations took {}h {}m {}s".format(*pretty_time(time.perf_counter() - tic)))
    predicted_labels = []  # quick and dirty, watch performance
    truth_labels = []  # quick and dirty, watch performance
    background_id = np.max(category_ids) + 1
    category_ids = [background_id] + category_ids
    if args.normalize:
        fmt = '%.2f'
        dtype = np.float32
    else:
        fmt = '%d'
        dtype = np.uint32
    matrix = np.zeros((len(category_ids), len(category_ids)), dtype=dtype)
    if not args.pixels:
        if args.bounding_boxes:
            iou_matrix = metrics.bbox_iou_matrix
        elif args.polygons:
            iou_matrix = metrics.poly_iou_matrix
        for image_id, image in images.items():
            prediction_annotations = []
            truth_annotations = []
            prediction_categories = []
            truth_categories = []

            for category_id, annotations in image.items():
                prediction_annotations += annotations[0]
                truth_annotations += annotations[2]
                prediction_categories += [category_id] * len(annotations[0])
                truth_categories += [category_id] * len(annotations[2])

            if len(prediction_categories) == 0:
                prediction_results = [background_id] * len(truth_categories)
                truth_results = truth_categories
            elif len(truth_categories) == 0:
                prediction_results = prediction_categories
                truth_results = [background_id] * len(prediction_categories)
            else:
                tic = time.perf_counter()
                iou = iou_matrix(prediction_annotations, truth_annotations)
                logging.info("confusion-matrix: time taken for iou_matrix {}: {}h {}m {}s".
                             format(image_id, *pretty_time(time.perf_counter() - tic)))
                tic = time.perf_counter()
                truth_results, prediction_results = metrics.ious_to_sklearn_pred_true(
                    iou,
                    truth_categories,
                    prediction_categories,
                    iou_threshold=args.iou_threshold,
                    blank_id=background_id,
                )
                logging.info("confusion-matrix: time taken for ious_to_sklearn_pred_true {}: {}h {}m {}s".
                             format(image_id, *pretty_time(time.perf_counter() - tic)))

            predicted_labels = np.append(predicted_labels, prediction_results)
            truth_labels = np.append(truth_labels, truth_results)
            if args.csv_output_per_image:
                matrix = sklearn.metrics.confusion_matrix(truth_labels, predicted_labels, category_ids).astype(
                    matrix.dtype)
                if args.normalize:
                    matrix = normalize_matrix(matrix)
                if args.suppress_background:
                    matrix[0, 0] = 0
                np.savetxt(sys.stdout, matrix, delimiter=',', fmt=fmt)
                predicted_labels = []
                truth_labels = []
        if not args.csv_output_per_image:
            matrix = sklearn.metrics.confusion_matrix(truth_labels, predicted_labels, category_ids).astype(matrix.dtype)
    else:
        from skimage.draw import polygon
        from abyss_deep_learning.utils import imread
        image_id_2_shape = {}
        image_id_2_path = {}
        for image in truths['images']:
            shape = (image.get('height'), image.get('width'))
            if None in shape:
                shape = imread(image['path']).shape[:2]
            image_id_2_shape[image['id']] = shape
            image_id_2_path[image['id']] = image['path']
        for image_id, image in images.items():
            if image_id not in image_id_2_shape:
                continue
            shape = image_id_2_shape[image_id]
            image_array_truth = np.ones(shape, dtype=np.uint8) * background_id
            image_array_pred = np.ones(shape, dtype=np.uint8) * background_id
            for category_id, annotations in image.items():
                prediction_annotations = annotations[0]
                for annotation in prediction_annotations:
                    column = annotation[:, 0]
                    row = annotation[:, 1]
                    rr, cc = polygon(row, column, shape)
                    image_array_pred[rr, cc] = category_id
                truth_annotations = annotations[2]
                for annotation in truth_annotations:
                    column = annotation[:, 0]
                    row = annotation[:, 1]
                    rr, cc = polygon(row, column, shape)
                    image_array_truth[rr, cc] = category_id

            image_matrix = sklearn.metrics.confusion_matrix(image_array_truth.ravel(), image_array_pred.ravel(),
                                                            category_ids).astype(matrix.dtype)
            if args.csv_output_per_image:
                if args.normalize:
                    image_matrix = normalize_matrix(image_matrix)
                if args.suppress_background:
                    image_matrix[0, 0] = 0
                print(image_id_2_path[image_id] + ',' + str(image_id), end=',')
                np.savetxt(sys.stdout, np.expand_dims(image_matrix.ravel(), axis=0), delimiter=',', fmt=fmt)
            matrix += image_matrix

    # if args.normalize: m = m.astype('float') / cm.sum(axis=1)[:, np.newaxis]  # Not working???
    if args.normalize:  # normalize per class
        matrix = normalize_matrix(matrix)

    if args.suppress_background:
        matrix[0, 0] = 0
    if args.plot or args.save_figure:
        classes = ['BG']
        for category in truths['categories']:
            classes.append(category['name'])
        plot_confusion_matrix(matrix, classes, normalized=args.normalize, save_fig=args.save_figure)

    if not args.csv_output_per_image:
        np.savetxt(sys.stdout, matrix, delimiter=',', fmt=fmt)


def normalize_matrix(matrix):
    matrix = matrix.astype(np.float32)
    matrix = np.divide(matrix.T, np.sum(matrix, axis=1)).T  # Normalise across rows
    return np.nan_to_num(matrix)


def tfpn(args: argparse.Namespace):
    predicted = json.loads(sys.stdin.read())
    with open(args.truth) as f:
        truth = json.load(f)

    category_ids = [category['id'] for category in truth.get('categories', [])]
    images = load_annotations(predicted, truth, category_ids, args)

    iou_matrix = None
    if args.bounding_boxes:
        iou_matrix = metrics.bbox_iou_matrix
    elif args.polygons:
        iou_matrix = metrics.poly_iou_matrix

    if iou_matrix is None:
        logging.error("please pass valid option to match on either --polyogons or --bbox")

    match = eval('metrics.' + args.match)  # quick and dirty
    result = dict(annotations=[])
    for image_id, image in images.items():
        for category_id, annotations in image.items():
            category_id_offset = 0 if args.flat_categories else (category_id - 1) * 4
            tp = [[], []]
            fp = []
            tn = []
            fn = []
            if len(annotations[0]) == 0:
                fn = list(range(len(annotations[2])))
            else:
                if len(annotations[2]) == 0:
                    fp = list(range(len(annotations[0])))
                else:
                    # todo: if too slow, remove; used only to calculate score as iou
                    ious = iou_matrix(annotations[0], annotations[2])
                    tp, fp, tn, fn = metrics.tp_fp_tn_fn(
                        annotations[0], annotations[2],
                        args.iou_threshold,
                        match,
                        iou_matrix,
                    )

            for i in range(len(tp[0])):
                a = copy.deepcopy(predicted['annotations'][annotations[1][tp[0][i]]])
                a['id'] = len(result['annotations'])
                a['score'] = ious[tp[0][i], tp[1][i]]
                a['category_id'] = 1 + category_id_offset
                if 'segmentation' not in a or a['segmentation'] is None:
                    a['segmentation'] = bbox_to_segmentation(a['bbox'])
                result['annotations'].append(a)
            for i in fp:
                a = copy.deepcopy(predicted['annotations'][annotations[1][i]])
                a['id'] = len(result['annotations'])
                a['score'] = 0
                a['category_id'] = 2 + category_id_offset
                if 'segmentation' not in a or a['segmentation'] is None:
                    a['segmentation'] = bbox_to_segmentation(a['bbox'])
                result['annotations'].append(a)
            for i in tn:
                a = copy.deepcopy(truth['annotations'][annotations[3][i]])
                a['id'] = len(result['annotations'])
                a['score'] = 0
                a['category_id'] = 3 + category_id_offset
                if 'segmentation' not in a or a['segmentation'] is None:
                    a['segmentation'] = bbox_to_segmentation(a['bbox'])
                result['annotations'].append(a)
            for i in fn:
                a = copy.deepcopy(truth['annotations'][annotations[3][i]])
                a['id'] = len(result['annotations'])
                a['score'] = 0
                a['category_id'] = 4 + category_id_offset
                if 'segmentation' not in a or a['segmentation'] is None:
                    a['segmentation'] = bbox_to_segmentation(a['bbox'])
                result['annotations'].append(a)
    result['categories'] = []
    if args.flat_categories:
        result['categories'] += [
            dict(name='TP', supercategory='', id=1),
            dict(name='FP', supercategory='', id=2),
            dict(name='TN', supercategory='', id=3),
            dict(name='FN', supercategory='', id=4),
        ]
    else:
        for cat in truth['categories']:
            category_id_offset = (cat['id'] - 1) * 4
            result['categories'] += [
                dict(name=cat['name'] + ',TP', supercategory=cat['name'], id=category_id_offset + 1),
                dict(name=cat['name'] + ',FP', supercategory=cat['name'], id=category_id_offset + 2),
                dict(name=cat['name'] + ',TN', supercategory=cat['name'], id=category_id_offset + 3),
                dict(name=cat['name'] + ',FN', supercategory=cat['name'], id=category_id_offset + 4),
            ]
    result['images'] = truth['images']
    result['licenses'] = [
        dict(
            id=1,
            url='http://creativecommons.org/licenses/by-nc-sa/2.0/',
            name='Attribution-NonCommercial-ShareAlike License',
        ),
    ]
    result['info'] = dict(
        contributor='Abyss Solutions',
        total_time='00h00m00s',
        year=str(datetime.now().year),
        date_created=str(datetime.now()),
        description='This is a dataset configured by Abyss Solutions.',
        version='1.0',
        url='http://www.abysssolutions.com.au',
    )
    json.dump(result, sys.stdout, indent=4)


def get_args() -> argparse.Namespace:
    verbose = argparse.ArgumentParser(add_help=False)
    verbose.add_argument(
        '-v', '--verbose',
        action='store_const',
        const=logging.INFO,
        dest='loglevel',
        help="verbose output to stderr",
    )

    debug = argparse.ArgumentParser(add_help=False)
    debug.add_argument(
        '-d', '--debug',
        action='store_const',
        const=logging.DEBUG,
        dest='loglevel',
        help="debug output to stderr"
    )

    annotation_types = argparse.ArgumentParser(add_help=False)
    annotation_group = annotation_types.add_mutually_exclusive_group(required=True)
    annotation_group.add_argument(
        '--polygons', '--poly', '-p',
        action='store_true',
        help='match on polygons',
    )
    annotation_group.add_argument(
        '--bounding-boxes', '--bbox', '-b',
        action='store_true',
        help='match on bounding boxes',
    )

    files = argparse.ArgumentParser(add_help=False)
    files.add_argument(
        '--first', '--prediction', '--pred',
        help="predictions coco.json (default STDIN)",
    )
    files.add_argument(
        '--second', '--truth', '-t',
        required=True,
        help='ground truth coco.json file',
    )

    thresholds = argparse.ArgumentParser(add_help=False)
    thresholds.add_argument(
        '--iou-threshold',
        default=0.5,
        type=float,
        help='default: %(default)s',
    )
    thresholds.add_argument(
        '--score-threshold',
        default=0.5,
        type=float,
        help='default: %(default)s',
    )

    parser = argparse.ArgumentParser(
        description=description,
        formatter_class=argparse.RawDescriptionHelpFormatter,
        parents=[verbose, debug],
    )
    subparsers = parser.add_subparsers()

    tfpn_parser = subparsers.add_parser(
        'tfpn',
        description=tfpn_description,
        help="take predictions.json, truth.json, output to stdout coco annotations labeled as TP, FP, TN",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        parents=[verbose, annotation_types, files, thresholds],
    )
    tfpn_parser.set_defaults(func=tfpn)

    tfpn_parser.add_argument(
        '--flat-categories', '--flat',
        action='store_true',
        help="output just four categories: TP, FP, TN, and FN",
    )
    tfpn_parser.add_argument(
        '--match',
        default='one_to_one',
        help="how to match, 'one_to_one' or 'one_to_many'; default: %(default)s",
    )

    confusion_parser = subparsers.add_parser(
        'confusion',
        description=confusion_description,
        help="take predictions.json, truth.json, output to stdout coco "
             "annotations labeled as confusion among categories with iou as score",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        parents=[verbose, debug, annotation_types, files, thresholds],
    )
    confusion_parser.set_defaults(func=confusion)

    confusion_parser.add_argument('--iou-as-score', action='store_true',
                                  help="set score to iou value; default: keep score from original annotation")

    annotation_group.add_argument(
        '--pixels', '--pixel',
        action='store_true',
        help="match predictions and ground truth by a per pixel basis",
    )
    confusion_matrix_parser = subparsers.add_parser(
        'confusion-matrix',
        description=confusion_matrix_description,
        help="take predictions.json, truth.json, output to stdout coco annotations labeled as TP, FP, TN",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        parents=[verbose, debug, annotation_types, files, thresholds],
    )
    confusion_matrix_parser.set_defaults(func=confusion_matrix)

    confusion_matrix_parser.add_argument('--normalize', action='store_true', help="normalize confusion matrix")
    confusion_matrix_parser.add_argument('--plot', action='store_true',
                                         help="plot confusion matrix, convenience option")
    confusion_matrix_parser.add_argument('--save-figure', action='store_true',
                                         help="save plotted confusion matrix, convenience option")
    confusion_matrix_parser.add_argument('--suppress-background', action='store_true',
                                         help="Set BG,BG predictions to 0 in output confusion")
    confusion_matrix_parser.add_argument('--csv-output-per-image', action='store_true',
                                         help="output to stdout the confusion values on a per image basis")
    confusion_matrix_parser.add_argument('--header', '--head', action='store_true',
                                         help="output csv header for each column")

    return parser.parse_args()


def main(args: argparse.Namespace):
    logging.basicConfig(
        format='%(filename)s: %(asctime)s.%(msecs)d: %(levelname)s: %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S',
        level=args.loglevel,
    )
    return args.func(args)


def pretty_time(seconds: float) -> Tuple[int, int, float]:
    minutes, seconds = divmod(seconds, 60)
    hours, minutes = divmod(minutes, 60)
    return int(hours), int(minutes), seconds


if __name__ == '__main__':
    sys.exit(main(get_args()))
