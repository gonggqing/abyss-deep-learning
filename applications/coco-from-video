#!/usr/bin/python3

import argparse
import glob
import json
import os
import re
import sys
from inspect import currentframe
from pathlib import Path

import cv2
from pycocotools.coco import COCO

description = """

Read JSON file(s) defined on command line or a list of JSON files on stdin,
output COCO JSON files in respective subdirectories in the current directory,
optionally, split videos into PNG frames

examples
    coco-from-video coco.cloud.001.json --dir /mnt/ssd1/processed/industry/what/not --cloud
    find . -name my-coco.json | coco-from-video --dir /mnt/ssd1/processed/industry/what/not --cloud

"""


# TODO:
#   ! output video details in info/dataset section
#   ! output bare minimum (see confluence) of fields in images section
#   ? pad frame numbers in filenames? 00001234.png vs 1234.png?
#   ? paths to images absolute by default
#   ? add --relative option?
#   ? add --absolute option?

# TODO: must be able to handle .avi video format?


def main():
    args = get_args()

    if not args.video_dir:
        print("No video directory specified to search for video/s")
        sys.exit(1)

    for json_file in args.json_files:
        coco = COCO(json_file)
        if len(get_unique_list_of_videos()) > 1:
            raise EnvironmentError("JSON has more than one video")
        video_basename = os.path.basename(get_unique_list_of_videos(coco)[0])

        # Open the video
        video_path = recursive_search(video_basename, args.video_dir)

        if not video_path:
            print("Unable to locate {}".format(video_basename))
            continue

        video_name, video_ext = os.path.splitext(video_basename)
        # Make a new folder for the video, and frames

        cap = cv2.VideoCapture(video_path)

        output_data_set_dir = None

        if args.make_images:
            # Create image folder
            output_data_set_dir = os.path.join(args.output_dir, video_name, 'images')
            if not os.path.exists(output_data_set_dir):
                os.makedirs(output_data_set_dir)

        # Retrieve first image id number in dataset to offset existing images by that amount
        img_id_offset = int(coco.dataset['images'][0]['id'])
        # Iterate through existing images in coco dataset
        for im_idx, im in enumerate(coco.dataset['images']):
            # Original image path
            img_path = im['path']

            if ':' not in img_path:
                raise EnvironmentError("COCO path variable contains no ':', cannot derive frame number")

            # Retrieve image frame number
            img_frame_number = int(img_path.split(":")[-1])

            # Set the frame in the video
            cap.set(cv2.CAP_PROP_POS_FRAMES, img_frame_number)

            # Read the Image
            ret, img = cap.read()

            # Default is video path and frame number
            new_path = video_path + ":" + str(img_frame_number)

            # If image folder exists, extract image in png format
            if ret and output_data_set_dir:
                # New Path
                new_path = os.path.join(output_data_set_dir, str(img_frame_number).zfill(8) + ".png")

                # Write the image to the new path
                cv2.imwrite(new_path, img)
            elif not ret:
                debug_print("Failed to get frame {}".format(img_frame_number))

            # New Coco Dataset
            coco.dataset['images'][im_idx] = {
                'id': int(im['id'] - img_id_offset),
                'height': int(im['height']),
                'width': int(im['width']),
                'original_uri': video_path + "?begin=" + str(img_frame_number) + "&end=" + str(img_frame_number),
                'path': new_path,
            }

        # if 'dataset' not in coco.dataset['info']: # Unsure if necessary if statement
        coco.dataset['info']['dataset'] = {
            "license": 1,
            "video": {
                "height": int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)),
                "width": int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),
                "frame_total": int(cap.get(cv2.CAP_PROP_FRAME_COUNT)),
                "frame_rate": cap.get(cv2.CAP_PROP_FPS)
            },
            "date_captured": "",
            "uri": video_path,
        }

        if args.keep_segmentation:
            return NotImplementedError("Not yet implemented")

        annotations = []
        if 'categories' in coco.dataset:
            # Converting instances to captions
            categories = coco.dataset.pop('categories')
            caption_map_r = {cat['id']: cat['name'] for cat in categories}
            # Retrieve first annotation id to offset existing annotations by that amount
            ann_id_offset = int(coco.dataset['annotations'][0]['id'])
            for ann in coco.dataset['annotations']:
                new_ann = {
                    'caption': translate_caption("", caption_map_r[ann['category_id']]),
                    'id': int(ann['id'] - ann_id_offset),
                    'image_id': int(ann['image_id'] - img_id_offset)
                }
                annotations.append(new_ann)
        else:
            # Converting old CSV captions to new captions
            for ann in coco.dataset['annotations']:
                for caption in ann['caption'].split(','):
                    new_ann = {
                        'caption': translate_caption("", caption),
                        'id': len(annotations),
                        'image_id': int(ann['image_id'] - img_id_offset)
                    }
                    annotations.append(new_ann)
        coco.dataset['annotations'] = annotations

        remove_redundant_fields_in_coco(coco)

        # Retrieve sub-directories if they exist
        path = Path(video_path)
        rel_dir = path.relative_to(args.video_dir)
        sub_dir, video_basename = os.path.split(rel_dir)

        # Create output data set folder that contains the coco.json file
        output_data_set_dir = os.path.join(args.output_dir, sub_dir, video_name)
        if not os.path.exists(output_data_set_dir):
            os.makedirs(output_data_set_dir)

        # Write the new data set
        with open(os.path.join(output_data_set_dir, 'coco.json'), 'w') as outfile:
            json.dump(coco.dataset, outfile, sort_keys=True, indent=4)

    sys.exit(0)


def debug_print(arg):
    """Debugging tool to print line number followed by normal print statement arguments

    """
    frame_info = currentframe()
    print('Line', frame_info.f_back.f_lineno, ':', arg)


def get_unique_list_of_videos(coco):
    """Gets a unique list of all videos in the dataset

    Args:
        coco (COCO): A coco dataset

    Returns:
        list: List of strings of all videos in the dataset

    """
    video_list = []
    img_ids = coco.getImgIds(imgIds=[])  # Load all images
    for img_id in img_ids:
        video_path = coco.loadImgs(img_id)[0]['path']
        # debug_print("video path is {}".format(video_path))
        video_name = os.path.basename(video_path).split(':')[0]
        video_name = re.sub(r' \([0-9]+\)', '', video_name)
        # debug_print(video_name)
        video_list.append(video_name)
    return list(set(video_list))


def get_video_path(coco):
    """Get the video path

    Args:
        coco (COCO): Get the video path

    Returns:
        str: The video path

    """
    imgIds = coco.getImgIds(imgIds=[])  # Load all images
    id = imgIds[0]
    video_path = coco.loadImgs(id)[0]['path'].split(':')[0]
    debug_print(video_path)
    return video_path


# def get_video_at_default_path(coco, default_path):
#     """Gets the video at the provided video default path. Full path is default_path/basename.
#
#     Args:
#         coco (COCO): A coco dataset
#         default_path (str): The default path of all the videos
#
#     Returns:
#         str: Video Path
#
#     """
#     return os.path.join(default_path, get_unique_list_of_videos(coco)[0])

def recursive_search(video_basename, root_dir):
    for file_name in glob.iglob(os.path.join(root_dir, "**", video_basename), recursive=True):
        return file_name


def get_args():
    parser = argparse.ArgumentParser(description=description, formatter_class=argparse.RawDescriptionHelpFormatter)
    parser.add_argument("json_files", nargs='*', type=str, help="Path to JSON file")
    parser.add_argument("--video-dir", "--dir", type=str,
                        help="Path under which all the videos referenced in JSON file(s) expected to be found")
    parser.add_argument("--keep-segmentation", "--seg", action='store_true',
                        help="Keep segmentation regions, otherwise assume to be classification task (and discard "
                             "regions)")
    # TODO: Check to see what differs between cloud factory format and other formats
    parser.add_argument("--cloud-factory", "--cloud", action="store_true",
                        help="Input JSON files are in (deprecated) cloud factory format flavour")
    parser.add_argument("--make-images", "--images", action="store_true",
                        help="Extract images from video and save as PNG")
    parser.add_argument("--output-dir", type=str,
                        help="Output directory for database", default=os.getcwd())
    return parser.parse_args()


# TODO: check with seva & jackson on good way to do this
def translate_caption(input_format, label):
    return label


def remove_redundant_fields_in_coco(coco: COCO):
    remove_redundant_fields_in_data_set(coco.dataset)
    remove_redundant_fields_in_images(coco.dataset['images'])
    remove_redundant_fields_in_info(coco.dataset['info'])


def remove_redundant_fields_in_info(info: dict):
    fields_of_interest = ["url",
                          "year",
                          "contributor",
                          "total_time",
                          "description",
                          "version",
                          "date_created",
                          "dataset",
                          ]
    for field in list(info.keys()):
        if field not in fields_of_interest:
            del info[field]


def remove_redundant_fields_in_images(images: list):
    fields_of_interest = ["height", "width", "id", "path", "original_uri"]
    for img in images:
        for field in list(img):
            if field not in fields_of_interest:
                del img[field]


def remove_redundant_fields_in_data_set(data_set: dict):
    fields_of_interest = ['info', 'images', 'annotations']
    for field in list(data_set.keys()):
        if field not in fields_of_interest:
            del data_set[field]


if __name__ == "__main__":
    main()
