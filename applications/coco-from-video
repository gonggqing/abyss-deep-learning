#!/usr/bin/env python3

import argparse
import glob
import json
import os
import re
import sys
from inspect import currentframe
from pathlib import Path

import cv2
from pycocotools.coco import COCO

DESCRIPTION = """

Read JSON file(s) defined on command line or a list of JSON files on stdin,
output COCO JSON files in respective subdirectories in the current directory,
optionally, split videos into PNG frames

examples
    coco-from-video coco.cloud.001.json --dir /mnt/ssd1/processed/industry/what/not --cloud
    find . -name my-coco.json | coco-from-video --dir /mnt/ssd1/processed/industry/what/not --cloud

"""


# TODO:
#   ! output video details in info/dataset section
#   ! output bare minimum (see confluence) of fields in images section
#   ? pad frame numbers in filenames? 00001234.png vs 1234.png?
#   ? paths to images absolute by default
#   ? add --relative option?
#   ? add --absolute option?

# TODO: must be able to handle .avi video format?


def main():
    args = get_args()

    if not args.video_dir:
        error_print("No video directory specified to search for video/s")
        sys.exit(1)

    # Add stdin inputs to list of json files
    # TODO: Check edge case usage of isatty()
    if not sys.stdin.isatty():
        for line in sys.stdin:
            args.json_files.append(line.strip())

    for json_file in args.json_files:
        print("Creating COCO object for {}".format(os.path.basename(json_file)))
        coco = COCO(json_file)
        if len(get_unique_list_of_videos(coco)) != 1:
            error_print("JSON file does not have a unique video reference")
            continue

        video_basename = os.path.basename(get_unique_list_of_videos(coco)[0])
        video_name, video_ext = os.path.splitext(video_basename)

        # Open the video
        video_path = recursive_search(video_basename, args.video_dir)

        if not video_path:
            error_print("Unable to locate {}".format(video_basename))
            continue

        video_capture = cv2.VideoCapture(video_path)

        # Retrieve sub-directories if they exist between root video directory and video file path
        path = Path(video_path)
        rel_dir = path.relative_to(args.video_dir)
        sub_dir, video_basename = os.path.split(rel_dir)

        # Create output data set folder that contains the coco.json file
        output_data_set_dir = os.path.join(args.output_dir, sub_dir, video_name)
        os.makedirs(output_data_set_dir, exist_ok=True)

        img_dir = None

        if args.make_images:
            # Create image folder
            img_dir = os.path.join(output_data_set_dir, 'images')
            os.makedirs(img_dir, exist_ok=True)

        # Retrieve first image id number in data set to ensure image ids begin from 0
        img_id_offset = int(coco.dataset['images'][0]['id'])
        # Iterate through existing images in coco dataset

        unreadable_imgs = set()
        images = []
        for im_idx, im in enumerate(coco.dataset['images']):
            # Original image path
            img_path = im['path']

            if ':' not in img_path:
                error_print("COCO path variable contains no ':', cannot derive frame number")

            # Retrieve image frame number
            img_frame_number = int(img_path.split(":")[-1])

            # Set the frame in the video
            video_capture.set(cv2.CAP_PROP_POS_FRAMES, img_frame_number)

            # Read the Image
            ret, img = video_capture.read()

            if not ret:
                unreadable_imgs.add(im_idx)
                continue

            # Default is video path and frame number
            img_path = video_path + ":" + str(img_frame_number)

            # If make images specified, extract image in png format and write to image directory
            if args.make_images:
                # Pad image file name to contain 8 integers
                img_file_name = str(img_frame_number).zfill(8) + ".png"
                img_path = os.path.join(img_dir, img_file_name)

                # Write the image to the new path
                if os.path.exists(img_path):
                    error_print("Image {} exists".format(img_path))
                else:
                    cv2.imwrite(img_path, img)
                    print('Converting image {}'.format(im_idx), end='\r')

                if args.relative_path:
                    img_path = os.path.join(os.path.basename(img_dir), img_file_name)

            # New Coco Dataset
            images.append({
                'id': int(im['id'] - img_id_offset) if id in im else None,
                'height': int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT)),
                'width': int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH)),
                'original_uri': video_path + "?begin=" + str(img_frame_number) + "&end=" + str(img_frame_number),
                'path': img_path,
            })

        old_info = coco.dataset['info'] if 'info' in coco.dataset else None
        info = {}
        if old_info is not None:
            info = {
                'version': old_info['version'] if 'version' in old_info else None,
                'contributor': old_info['contributor'] if 'contributor' in old_info else None,
                'description': old_info['description'] if 'description' in old_info else None,
                'url': old_info['url'] if 'url' in old_info else None,
                'total_time': old_info['total_time'] if 'total_time' in old_info else None,
                'date_created': old_info['date_created'] if 'date_created' in old_info else None,
            }

        info['dataset'] = {
            "license": 1,
            "video": {
                "height": int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT)),
                "width": int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH)),
                "frame_total": int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT)),
                "frame_rate": video_capture.get(cv2.CAP_PROP_FPS),
            },
            "date_captured": "",
            "uri": video_path,
        }

        if args.keep_segmentation:
            return NotImplementedError("Not yet implemented")

        annotations = []
        if 'categories' in coco.dataset:
            # Converting instances to captions
            categories = coco.dataset.pop('categories')
            caption_map_r = {cat['id']: cat['name'] for cat in categories}
            # Retrieve first annotation id to offset existing annotations by that amount to ensure ann ids begin from 0
            ann_id_offset = int(coco.dataset['annotations'][0]['id'])
            for ann in coco.dataset['annotations']:
                # video_capture.read() did not read a proper image so skip annotation
                if ann['id'] in unreadable_imgs:
                    continue
                new_ann = {
                    'id': int(ann['id'] - ann_id_offset),
                    'image_id': int(ann['image_id'] - img_id_offset),
                    'caption': translate_caption("", caption_map_r[ann['category_id']]),
                }
                annotations.append(new_ann)
        else:
            # Converting old CSV captions to new captions
            for ann in coco.dataset['annotations']:
                # video_capture.read() did not read a proper image so skip annotation
                if ann['id'] in unreadable_imgs:
                    continue
                for caption in ann['caption'].split(','):
                    new_ann = {
                        'id': len(annotations),
                        'image_id': int(ann['image_id'] - img_id_offset),
                        'caption': translate_caption("", caption),
                    }
                    annotations.append(new_ann)

        dataset = {
            'info': info,
            'images': images,
            'annotations': annotations,
        }
        remove_redundant_fields(dataset)

        # Write the new data set
        with open(os.path.join(output_data_set_dir, 'coco.json'), 'w') as outfile:
            json.dump(dataset, outfile, indent=4)
    sys.exit(0)


def error_print(*arg, **kwargs):
    """Debugging tool to print line number followed by normal print statement arguments

    """
    frame_info = currentframe()
    print(os.path.splitext(os.path.basename(sys.argv[0]))[0], 'Line', frame_info.f_back.f_lineno, ':', *arg,
          file=sys.stderr, **kwargs)


def get_unique_list_of_videos(coco):
    """Gets a unique list of all videos in the dataset

    Args:
        coco (COCO): A coco dataset

    Returns:
        list: List of strings of all videos in the dataset

    """
    video_list = []
    img_ids = coco.getImgIds(imgIds=[])  # Load all images
    for img_id in img_ids:
        video_path = coco.loadImgs(img_id)[0]['path']
        # debug_print("video path is {}".format(video_path))
        video_name = os.path.basename(video_path).split(':')[0]
        video_name = re.sub(r' \([0-9]+\)', '', video_name)
        # debug_print(video_name)
        video_list.append(video_name)
    return list(set(video_list))


def recursive_search(video_basename: str, root_dir: str) -> str:
    """

    Args:
        video_basename: Base video file name
        root_dir: Root directory to search recursively in sub directories for video file

    Returns: Full file path to specified video base name

    """
    for file_path in glob.iglob(os.path.join(root_dir, "**", video_basename), recursive=True):
        return file_path


def get_args():
    parser = argparse.ArgumentParser(description=DESCRIPTION, formatter_class=argparse.RawDescriptionHelpFormatter)
    parser.add_argument("json_files", nargs='+', type=str, help="Path to JSON file")
    parser.add_argument("--video-dir", "--video", "-v", type=str,
                        help="Path under which all the videos referenced in JSON file(s) expected to be found")
    parser.add_argument("--output-dir", "--output", "-o", type=str,
                        help="Output directory path for database", default=os.getcwd())
    parser.add_argument("--keep-segmentation", "--seg", "-s", action='store_true',
                        help="Keep segmentation regions, otherwise assume to be classification task (and discard "
                             "regions)")
    # TODO: Check to see what differs between cloud factory format and other formats
    parser.add_argument("--cloud-factory", "--cloud", "-c", action="store_true",
                        help="Input JSON files are in (deprecated) cloud factory format flavour")
    parser.add_argument("--make-images", "--image", "-i", action="store_true",
                        help="Extract images from video and save as PNG")
    parser.add_argument("--relative-path", "--relative", "-r", action="store_true",
                        help="Image paths will be relative to the "
                             "coco.json file in output data set "
                             "directory")
    return parser.parse_args()


# TODO: check with seva & jackson on good way to do this
def translate_caption(input_format, label):
    return label


def remove_redundant_fields(dataset: dict):
    """
    Filter old formatted COCO object to new format

    Args:
        dataset: dictionary that should contain
            keys:
                info
                images
                annotations

    """
    remove_redundant_fields(dataset, ['info', 'images', 'annotations'])
    for img in dataset['images']:
        remove_redundant_fields(img, ["height", "width", "id", "path", "original_uri"])
    remove_redundant_fields(dataset['info'], ["url",
                                              "year",
                                              "contributor",
                                              "total_time",
                                              "description",
                                              "version",
                                              "date_created",
                                              "dataset",
                                              ])


def remove_redundant_fields(dictionary: dict, fields: list):
    """

    Args:
        dictionary: Standard dictionary inside a COCO object
        fields: Fields of interest to keep

    """
    for field in list(dictionary):
        if field not in fields:
            del dictionary[field]


if __name__ == "__main__":
    main()
