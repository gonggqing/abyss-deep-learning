#!/usr/bin/env python3
from __future__ import print_function

import argparse
import json
import os
import sys
from operator import itemgetter
from pathlib import Path

import cv2
from abyss_deep_learning.datasets.coco import CocoDataset

DESCRIPTION = \
"""
Read coco.json file(s) defined on command line or piped in from another coco utility.

Output merged coco json contents in stdout.

examples:
    coco-merge labelled/abc/coco.json labelled/def/coco.json
    find . -name coco.json | coco-merge
"""


def main():
    args = get_args()

    json_string = ""
    if not sys.stdin.isatty():
        for line in sys.stdin:
            json_string += line.strip()

    if json_string:
        args.json_files.append(json_string)

    images = []  # Merged images
    annotations = []  # Merged annotations
    img_count = {}  # Count occurrences of an image across all json files to be merged
    old_id_2_path = {}  # Mapping of old image id to the unique image path, many to one relationship
    path_2_new_id = {}  # Mapping of unique image path to new id, one to one relationship

    for json_file in args.json_files:
        old_data_set = CocoDataset(json_file).coco.dataset

        # Retrieve sub-directories if they exist between root video directory and video file path
        path = Path(os.path.join(os.getcwd(), json_file))
        rel_dir = path.relative_to(os.getcwd())
        sub_dir, file_basename = os.path.split(rel_dir)

        for image in old_data_set['images']:
            if 'path' not in image:
                continue

            img_path = os.path.join(os.getcwd(), sub_dir, image['path'])
            if img_path in img_count:
                img_count[img_path] += 1
            else:
                img_count[img_path] = 1

            if img_count[img_path] == 1:
                new_id = len(images)
                if not ('height' in image or 'weight' in image):
                    img = cv2.imread(img_path)

                images.append({
                    'id': new_id,
                    'height': image['height'] if 'height' in image else img.shape[0],
                    'width': image['width'] if 'width' in image else img.shape[1],
                    'original_uri': image['original_uri'] if 'original_uri' in image else None,
                    'path': img_path,
                })
                path_2_new_id[img_path] = new_id

            # Keep note of any image ids even duplicates to map the annotations to the correct new id
            old_id = image['id']
            old_id_2_path[old_id] = img_path

        for ann in old_data_set['annotations']:
            old_id = ann['image_id']
            img_path = old_id_2_path[old_id]
            new_id = path_2_new_id[img_path]
            annotations.append({
                'image_id': new_id,
                'caption': ann['caption']
            })

    # Remove duplicate annotations
    annotations = [dict(item) for item in set(tuple(ann.items()) for ann in annotations)]

    # Sort annotations by image id
    annotations.sort(key=itemgetter('image_id'))

    # Assign annotation ids
    tmp_annotations = annotations.copy()
    annotations.clear()

    for idx, ann in enumerate(tmp_annotations):
        ann = {
            'id': idx,
            'image_id': ann['image_id'],
            'caption': ann['caption'],
        }
        annotations.append(ann)

    merged_data_set = {
        'images': images,
        'annotations': annotations,
    }

    json.dump(merged_data_set, sys.stdout, indent=4)


def get_args():
    """Get args from the command line args"""
    parser = argparse.ArgumentParser(description=DESCRIPTION, formatter_class=argparse.RawDescriptionHelpFormatter)
    parser.add_argument('json_files', nargs='*', type=str, help="JSON files to merge together into one json")
    return parser.parse_args()


if __name__ == '__main__':
    main()
