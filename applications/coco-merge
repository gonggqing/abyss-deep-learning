#!/usr/bin/env python3
import logging

__author__ = 'Kent Hu, and Jamie McColl'
__maintainer__ = 'Kent Hu'
import argparse
import json
import os
import sys
from operator import itemgetter

DESCRIPTION = """
Read coco.json file(s) defined on command line and 
merge together 'images', 'annotations', 'categories' and 'licenses' array.

Entries in the 'images' array with duplicate file names are removed.
Entries in the 'annotations' array with duplicate keys, except for id, are removed 

Images are removed on a per image basis on the file name
i.e. if dataset has two images that have file name 0001.jpeg and 0001.png, one will be discarded
     if dataset has two images that have file name 0001.jpeg and 0002.jpeg, none will be discarded
     
Annotations are removed on a per annotation basis on the annotation meta data excluding the unique identifier 'id'
i.e. if dataset has two annotations -> 
    first_ann = {                           |   second_ann = {    
                    "id": 1,                |                   "id": 2,
                    "category_id": 1,       |                   "category_id": 1, 
                    "bbox": [0,0,0,0],      |                   "bbox": [0,0,0,0],
                }                           |                }
The second annotation will be removed as there exists duplicate category id and bbox 

If --handle-duplicate-ids is specified, then all image, annotation, and category ids will be reindexed from 0

Use case is when coco-1.json contains image-1.png with id 0 and coco-2.json contains image-2.png with id 0
then image-1.png will be given id 0 and image-2.png will be given id 1 as the 'id' key should be unique 

Output merged coco json contents in stdout.

examples:
    coco-merge labelled/abc/coco.json labelled/def/coco.json
"""


def main(args):
    logging.basicConfig(format='%(filename)s: %(asctime)s.%(msecs)d: %(levelname)s: %(lineno)d: %(message)s',
                        datefmt='%Y-%m-%d %H:%M:%S',
                        level=args.loglevel)
    info = []
    try:
        with open(args.file) as f:
            try:
                info = json.load(f).get('info')
            except json.decoder.JSONDecodeError:
                logging.warning("Could not interpret {} as a JSON".format(args.file))
    except FileNotFoundError:
        logging.warning("Could not find JSON file {}".format(args.file))
    except TypeError:
        try:
            with open(args.json_files[0]) as f:
                try:
                    info = json.load(f).get('info')
                except json.decoder.JSONDecodeError:
                    logging.warning("Could not interpret {} as a JSON".format(args.file))
        except IndexError:
            logging.warning("At least one JSON file has to be given")
        except FileNotFoundError:
            logging.warning("Could not find JSON file {}".format(args.file))
        except TypeError:
            logging.warning("Expected <path/to/file>, got {}".format(args.json_files[0]))

    images = []  # Merged images
    annotations = []  # Merged annotations
    categories = []  # Merged categories
    licenses = []  # Merged licenses
    old_id_2_path = {}  # Mapping of old image id to the unique image path, many to one relationship
    path_2_new_id = {}  # Mapping of unique image path to new id, one to one relationship
    category_old_id_2_name_and_super = {}
    name_and_super_2_new_id = {}  # Mapping of old categories to new id
    seen_categories = set()  # Filter out duplicate categories/supercategories
    seen_images = set()  # Filter out duplicate images

    for json_file in args.json_files:
        logging.info("Loading {}".format(json_file))
        try:
            with open(json_file) as f:
                try:
                    dataset = json.load(f)
                except json.decoder.JSONDecodeError:
                    logging.warning("Could not interpret {} as a JSON".format(json_file))
                    continue
        except FileNotFoundError:
            logging.warning("Could not find JSON file {}".format(json_file))
            continue
        except TypeError:
            logging.warning("Expected <path/to/file>, got {}".format(repr(json_file)))
            continue

        # Add new categories
        for category in dataset.get('categories', []):
            if not isinstance(category, dict):
                continue
            if args.handle_duplicate_ids:
                # Check to see if category name and supercategory exists in merged categories
                name = category.get('name', '')
                supercategory = category.get('supercategory', '')
                if name == '' and supercategory == '':
                    continue
                name_and_super = (name, supercategory)
                if name_and_super not in seen_categories:
                    new_id = category['id'] if args.keep_category_ids else len(categories)
                    category_old_id_2_name_and_super[category['id']] = name_and_super
                    categories.append({
                        'id': new_id,
                        'name': name,
                        'supercategory': supercategory,
                    })
                    name_and_super_2_new_id[name_and_super] = new_id
                    seen_categories.add(name_and_super)
                elif category['id'] not in category_old_id_2_name_and_super.keys():
                    category_old_id_2_name_and_super[category['id']] = name_and_super
            else:
                categories.append(category)
                try:
                    id_ = int(category['id'])
                except KeyError:
                    logging.error("Expected 'id' key, found only {}".format(category.keys()))
                except ValueError:
                    logging.error("Could not interpret {} as an integer".format(category['id']))
                else:
                    if id_ in seen_categories:
                        logging.warning("Found duplicate category ids for id {}".format(id_))
                    else:
                        seen_categories.add(id_)

        for image in dataset.get('images', []):
            if not isinstance(image, dict):
                logging.warning("Expected type dict, got type {}".format(type(image)))
                continue

            if args.handle_duplicate_ids:
                img_file_name, img_ext = os.path.splitext(os.path.basename(image.get("path", image['file_name'])))

                old_id = image['id']
                old_id_2_path[old_id] = img_file_name

                if img_file_name not in seen_images:
                    seen_images.add(img_file_name)
                    new_id = len(images)
                    path_2_new_id[img_file_name] = new_id
                    image['id'] = new_id
                    images.append(image)
            else:
                images.append(image)
                try:
                    id_ = int(image['id'])
                except KeyError:
                    logging.error("Expected 'id' key, found only {}".format(image.keys()))
                except ValueError:
                    logging.error("Could not interpret {} as an integer".format(image['id']))
                else:
                    if id_ in seen_images:
                        logging.warning("Found duplicate image ids for id {}".format(id_))
                    else:
                        seen_images.add(id_)

        for ann in dataset.get('annotations', []):
            if not isinstance(ann, dict):
                logging.warning("Expected type dict, got type {}".format(type(ann)))
                continue

            if args.handle_duplicate_ids:
                old_id = ann['image_id']
                img_file_name = old_id_2_path[old_id]
                new_id = path_2_new_id[img_file_name]
                ann['image_id'] = new_id
                ann['category_id'] = name_and_super_2_new_id[category_old_id_2_name_and_super[ann['category_id']]]
                del ann['id']
            annotations.append(ann)

        for license_ in dataset.get('licenses', []):
            if not isinstance(license_, dict):
                logging.warning("Expected type dict, got type {}".format(type(license_)))
                continue

            license_['id'] = len(licenses)
            licenses.append(license_)

    # Remove duplicate annotations
    annotations = [dict(item) for item in set(to_tuple([list(i) for i in sorted(ann.items())]) for ann in annotations)]
    licenses = [dict(item) for item in set(to_tuple([list(i) for i in sorted(license_.items())]) for license_ in
                                           licenses)]

    # Sort annotations by image id
    annotations.sort(key=itemgetter('image_id'))
    # Sort images by id
    images.sort(key=itemgetter('id'))

    for id_, ann in enumerate(annotations):
        ann['id'] = id_

    merged_data_set = {
        'images': images,
        'annotations': annotations,
        'categories': categories,
        'info': info,
        'licenses': licenses,
    }

    json.dump(merged_data_set, sys.stdout, indent=4)


def get_args():
    parser = argparse.ArgumentParser(description=DESCRIPTION, formatter_class=argparse.RawDescriptionHelpFormatter)
    parser.add_argument('json_files', nargs='*', type=str, help="JSON files to merge together into one json")
    parser.add_argument('-f', '--file', nargs=1, type=str,
                        help="COCO json file to copy info to new dataset. Default is the first json file")
    parser.add_argument('--keep-category-ids', '--keep-categories', '-k', action='store_true',
                        help="Keep categories as in input files. "
                             "i.e. expect that all files to merge have the same set of categories")
    parser.add_argument('--handle-duplicate-ids', action='store_true',
                        help="Handle duplicate image, annotation, category and license ids by reassigning them from 0")
    logging_group = parser.add_mutually_exclusive_group()
    logging_group.add_argument('-v', '--verbose', action='store_const', const=logging.INFO, dest='loglevel',
                               help="Verbose output to stderr")
    logging_group.add_argument('-d', '--debug', action='store_const', const=logging.DEBUG, dest='loglevek',
                               help="Debug output to stderr")
    return parser.parse_args()


def to_tuple(list_: list):
    return tuple(to_tuple(i) if isinstance(i, list) else i for i in list_)


if __name__ == '__main__':
    sys.exit(main(get_args()))
