#!/usr/bin/env python3
import argparse
import copy
import json
import logging
import sys
from contextlib import redirect_stdout
from io import TextIOWrapper
from typing import Union

from pycocotools.coco import COCO

__author__ = 'Kent Hu, and Jamie McColl'
__maintainer__ = 'Kent Hu'

DESCRIPTION = """
Read coco.json file(s) defined on command line and
merge together 'images', 'annotations', 'categories' and 'licenses' array.

TODO: LICENSE

Entries in the 'images' array with duplicate path are removed.
Entries in the 'annotations' array with duplicate fields, except for id, are removed

Images are removed on a per image basis on the path
i.e. if dataset has two images that have file name /home/user/images/0001.jpeg and /home/user/images/0001.png, none will be discarded
     if dataset has two images that have file name /home/user/images/0001.jpeg and /home/user/images/0002.jpeg, none will be discarded
     if dataset has two images that have file name /home/user/images/0001.jpeg and /home/user/tmp/0001.jpeg, none will be discarded

     if dataset has two images that have file name /home/user/images/0001.jpeg and /home/user/images/0001.jpeg, one will be discarded

Annotations are removed on a per annotation basis on the annotation meta data excluding the unique identifier 'id'
i.e. if dataset has two annotations ->
    first_ann = {                           |   second_ann = {
                    "id": 1,                |                   "id": 2,
                    "category_id": 1,       |                   "category_id": 1,
                    "bbox": [0,0,0,0],      |                   "bbox": [0,0,0,0],
                }                           |                }
                
The second annotation will be removed as it has duplicate category id and bbox with the first annotation

Output merged coco json contents in stdout.

examples:
    coco-merge labelled/abc/coco.json labelled/def/coco.json
"""


def main(args):
    logging.basicConfig(
        format='%(filename)s: %(asctime)s.%(msecs)d: %(levelname)s: %(lineno)d: %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S',
        level=args.loglevel,
    )

    # TODO:
    # Clean this block up
    info = []
    try:
        with open(args.file) as f:
            try:
                info = json.load(f).get('info')
            except json.decoder.JSONDecodeError:
                logging.warning("Could not interpret {} as a JSON".format(args.file))
    except FileNotFoundError:
        logging.warning("Could not find JSON file {}".format(args.file))
    except TypeError:
        try:
            with open(args.files[0]) as f:
                try:
                    info = json.load(f).get('info')
                except json.decoder.JSONDecodeError:
                    logging.warning("Could not interpret {} as a JSON".format(args.file))
        except IndexError:
            logging.critical("At least one JSON file has to be given")
            return 1
        except FileNotFoundError:
            logging.warning("Could not find JSON file {}".format(args.file))
        except TypeError:
            logging.warning("Expected <path/to/file>, got {}".format(args.json_files[0]))

    # TODO:
    # Licenses are ignored for now

    coco_list = []
    num_imgs = 0
    num_anns = 0
    num_cats = 0
    for file in args.files:
        curr_coco = MyCOCO(file)
        coco_list.append(curr_coco)
        num_imgs += len(curr_coco.images)
        num_anns += len(curr_coco.annotations)
        num_cats += len(curr_coco.categories)
    img_id_range = set(range(num_imgs))
    ann_id_range = set(range(num_anns))
    cat_id_range = set(range(num_cats))

    seen_img_maps = {}
    seen_cat_maps = {}
    seen_img_vals = set()
    seen_ann_vals = set()
    seen_cat_vals = set()

    # Handle duplicate values in whole list of COCO JSONS
    for curr_coco in coco_list:
        # Get current relevant values for images, annotations and categories
        curr_img_map, curr_img_vals = get_header_vals(curr_coco.images, field='path')
        curr_cat_map, curr_cat_vals = get_header_vals(curr_coco.categories)

        # Find those with duplicates
        dupe_img_vals = curr_img_vals & seen_img_vals
        dupe_cat_vals = curr_cat_vals & seen_cat_vals

        # Get relevant ids
        dupe_img_val_ids = [key for key, value in curr_img_map.items() if value in dupe_img_vals]
        dupe_cat_val_ids = [key for key, value in curr_cat_map.items() if value in dupe_cat_vals]

        if dupe_img_val_ids and dupe_img_vals:
            pass
            # logging.info("Found image entries {} with duplicate paths {}".format(dupe_img_val_ids, dupe_img_vals))

        if dupe_cat_val_ids and dupe_cat_vals:
            pass
            # logging.info("Found category entries {} with duplicate names/supercategories {}".format(dupe_cat_val_ids, dupe_cat_vals))

        # Update annotation entries of those with duplicate image or categories values
        dupe_img_val_entries = curr_coco.loadAnns(
            curr_coco.getAnnIds(imgIds=dupe_img_val_ids)) if dupe_img_val_ids else []
        dupe_cat_val_entries = curr_coco.loadAnns(
            curr_coco.getAnnIds(catIds=dupe_cat_val_ids)) if dupe_cat_val_ids else []

        for ann in dupe_img_val_entries:
            ann['image_id'] = seen_img_maps[curr_img_map[ann['image_id']]]

        for ann in dupe_cat_val_entries:
            ann['category_id'] = seen_cat_maps[curr_cat_map[ann['category_id']]]

        # Drop duplicate annotations
        curr_ann_map, curr_ann_vals = get_header_vals(curr_coco.annotations)
        dupe_ann_vals = curr_ann_vals & seen_ann_vals
        dupe_ann_val_ids = [key for key, value in curr_ann_map.items() if value in dupe_ann_vals]
        curr_coco.annotations = [ann for ann in curr_coco.annotations if ann['id'] not in dupe_ann_val_ids]

        # Drop duplicate images
        curr_coco.images = [img for img in curr_coco.images if img['id'] not in dupe_img_val_ids]

        # Drop duplicate categories
        curr_coco.categories = [cat for cat in curr_coco.categories if cat['id'] not in dupe_cat_val_ids]

        # Update sets of vals
        seen_img_vals |= curr_img_vals
        seen_ann_vals |= curr_ann_vals
        seen_cat_vals |= curr_cat_vals

        logging.debug(type(curr_img_map))
        # Update seen img map
        for key, value in curr_img_map.items():
            if value not in seen_img_maps.keys():
                seen_img_maps[value] = key

        # Update seen category map
        for key, value in curr_cat_map.items():
            if value not in seen_cat_maps.keys():
                seen_cat_maps[value] = key

        # Reindex updated annotations, good or bad idea? leave and see what happens
        curr_coco.createIndex()

    coco_out = coco_list.pop(0)
    img_id_range -= set(coco_out.getImgIds())
    ann_id_range -= set(coco_out.getAnnIds())
    cat_id_range -= set(coco_out.getCatIds())

    coco_out.info = info
    dupes = []
    for curr_coco in coco_list:
        """ Get entries that are duplicate ids as well as non duplicate ids """
        curr_img_ids = set(curr_coco.getImgIds())
        out_img_ids = set(coco_out.getImgIds())
        imgs_to_extend = curr_coco.loadImgs(curr_img_ids - out_img_ids)  # Img id doesn't exist in out coco
        duplicate_id_imgs = curr_coco.loadImgs(curr_img_ids & out_img_ids)
        coco_out.images.extend(imgs_to_extend)
        img_id_range -= curr_img_ids

        curr_ann_ids = set(curr_coco.getAnnIds())
        out_ann_ids = set(coco_out.getAnnIds())
        anns_to_extend = curr_coco.loadAnns(curr_ann_ids - out_ann_ids)  # Ann id doesn't exist in out coco
        duplicate_id_anns = curr_coco.loadAnns(curr_ann_ids & out_ann_ids)
        coco_out.annotations.extend(anns_to_extend)
        ann_id_range -= curr_ann_ids

        curr_cat_ids = set(curr_coco.getCatIds())
        out_cat_ids = set(coco_out.getCatIds())
        cats_to_extend = curr_coco.loadCats(curr_cat_ids - out_cat_ids)  # Cat id doesn't exist in out coco
        duplicate_id_cats = curr_coco.loadCats(curr_cat_ids & out_cat_ids)
        coco_out.categories.extend(cats_to_extend)
        cat_id_range -= curr_cat_ids

        # Create duplicates JSON and run another loop to clean duplicate ids by taking from leftover ids in id ranges
        dupes.append(dict(
            images=duplicate_id_imgs,
            annotations=duplicate_id_anns,
            categories=duplicate_id_cats,
        ))

        # Reindex output dataset
        coco_out.createIndex()

    for dupe in dupes:
        dupe_coco = MyCOCO()
        dupe_coco.dataset = dupe
        dupe_coco.createIndex()

        old_img_id_2_new_img_id = {}
        old_cat_id_2_new_cat_id = {}

        # Reassign image id and save mapping to update annotation image_id
        for img in dupe_coco.images:
            old_img_id_2_new_img_id[img['id']] = img_id_range.pop()
            img['id'] = old_img_id_2_new_img_id[img['id']]
            logging.warning("Assigning new image id {} to {}".format(img['id'], img['path']))

        coco_out.images.extend(dupe_coco.images)

        # Reassign category id and save mapping to update annotation category id
        for cat in dupe_coco.categories:
            old_cat_id_2_new_cat_id[cat['id']] = cat_id_range.pop()
            cat['id'] = old_cat_id_2_new_cat_id[cat['id']]
            logging.warning("Assigning new category id {} to {}".format(cat['id'], cat['name']))

        coco_out.categories.extend(dupe_coco.categories)

        # Reassign annotation id
        for ann in dupe_coco.annotations:
            old_id = ann['id']
            ann['id'] = ann_id_range.pop()

            if ann['image_id'] in old_img_id_2_new_img_id.keys():
                ann['image_id'] = old_img_id_2_new_img_id[ann['image_id']]

            if ann['category_id'] in old_cat_id_2_new_cat_id.keys():
                ann['category_id'] = old_cat_id_2_new_cat_id[ann['category_id']]

            logging.warning("Assigning new annotation id {} to old annotation id {}".format(ann['id'], old_id))

        coco_out.annotations.extend(dupe_coco.annotations)

    json.dump(coco_out.dataset, sys.stdout, indent=4)
    return 0


def get_header_vals(header: list, field: str = None):
    uniq_vals = [copy.deepcopy(i) for i in header]
    map_ = {}
    entries = set()
    if field is None:
        for entry in uniq_vals:
            id_ = entry.pop('id')
            entry = to_tuple([list(i) for i in sorted(entry.items())])
            map_[id_] = entry
            entries.add(entry)
    else:
        for entry in uniq_vals:
            id_ = entry.pop('id')
            entry = to_tuple([list(i) for i in sorted(((field, entry[field]),))])
            map_[id_] = entry
            entries.add(entry)
    return map_, entries


def to_tuple(list_: list):
    return tuple(to_tuple(i) if isinstance(i, list) else i for i in list_)


class MyCOCO(COCO):
    """ Create COCO object by reading from file path or from stdin """

    class Verbose:
        @staticmethod
        def write(line: str):
            line = line.strip()
            if line:
                logging.info(line)

    def __init__(self, buffer: Union[str, TextIOWrapper] = None):
        if isinstance(buffer, str) or buffer is None:
            with redirect_stdout(MyCOCO.Verbose):
                super().__init__(annotation_file=buffer)
        elif isinstance(buffer, TextIOWrapper):
            json_string = buffer.read().strip()
            if json_string:
                self.dataset = json.loads(json_string)
                self.createIndex()
            else:
                logging.error("Expecting input from stdin: received empty characters {}".format(repr(json_string)))
                sys.exit(1)
        else:
            logging.error("Unknown data type {}, exiting".format(type(buffer)))
            sys.exit(1)

    @property
    def info(self):
        return self.dataset.get('info', {})

    @property
    def annotations(self):
        return self.dataset.get('annotations', [])

    @property
    def images(self):
        return self.dataset.get('images', [])

    @property
    def categories(self):
        return self.dataset.get('categories', [])

    @info.setter
    def info(self, info):
        self.dataset['info'] = info

    @annotations.setter
    def annotations(self, annotations):
        self.dataset['annotations'] = annotations

    @images.setter
    def images(self, images):
        self.dataset['images'] = images

    @categories.setter
    def categories(self, categories):
        self.dataset['categories'] = categories

    def createIndex(self):
        with redirect_stdout(MyCOCO.Verbose):
            super().createIndex()


def get_args():
    parser = argparse.ArgumentParser(description=DESCRIPTION, formatter_class=argparse.RawDescriptionHelpFormatter)
    parser.add_argument(
        'files',
        nargs='*',
        help="JSON files to merge together into one json",
    )
    parser.add_argument(
        '-f', '--file',
        nargs=1,
        help="COCO json file to copy info to new dataset. Default is the first json file",
    )
    logging_group = parser.add_mutually_exclusive_group()
    logging_group.add_argument(
        '-v', '--verbose',
        action='store_const',
        const=logging.INFO,
        dest='loglevel',
        help="Verbose output to stderr",
    )
    logging_group.add_argument(
        '-d', '--debug',
        action='store_const',
        const=logging.DEBUG,
        dest='loglevel',
        help="Debug output to stderr",
    )
    return parser.parse_args()


if __name__ == '__main__':
    sys.exit(main(get_args()))
