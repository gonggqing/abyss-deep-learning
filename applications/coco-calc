#!/usr/bin/env python3
import argparse
import copy
import itertools
import json
import logging
import os
import sys
from ast import literal_eval
from operator import itemgetter
from typing import List, Tuple, Union

import numpy as np
import pandas as pd
from PIL import Image
from pycocotools.coco import COCO

from abyss_deep_learning.metrics import poly_intersection_area
from abyss_deep_learning.utils import polygon_area, imwrite, imread, do_overlap, polygon_x, polygon_y, MyCOCO

__author__ = 'Kent Hu, Vsevolod Vlaskine'

_ASSIGN_NEW_IMAGE_IDS_DESCRIPTION = """
Read in COCO file from stdin and reassigned the image ids of incoming COCO file based on the image ids from
the command line COCO. The new ids are matched by the file_name of either COCO files

Input COCO file from stdin is output to stdout.

If no COCO file is passed in, the input COCO file can have new image ids assigned based on the existing ordering or
the input COCO file can be sorted based on a field in 'images' and then have new ids assigned based on the ordering

examples:
    cat input.json | coco-calc assign-new-image-id coco coco.json --key file_name # Match by key and assign new ids to input COCO from command line COCO 
    cat input.json | coco-calc assign-new-image-id sort --key file_name # Sort the images by a key then assign new ids, if no key, assign image ids based on existing ordering
"""

_DESCRIPTION = """
Read COCO json file piped in from stdin or a list of COCO json files and apply image manipulation and 
processing. Output new COCO json file with relevant changes to the data and if output images is specified, 
output an image directory in the current working directory with all modified images.

RLE not currently supported for segmentation resizing
tile action is currently dummy action for demonstration purposes

examples:
    remap
        cat pf.json | coco-calc remap "{1:1,2:1,3:1,4:1}" --categories '{ "id": 1, "name": "PF" }'
        cat pf.json | coco-calc remap "{'PF-G':'PF','PF-L':'PF','PF-M':'PF','PF-H':'PF'}"

    resize
        cat coco.json | coco-calc resize --factor 0.3
        cat coco.json | coco-calc --verbose resize --size 480,640

    tile
        cat coco.json | coco-calc --apply-to-images tile --tiles 4,5
"""

_COVERAGE_DESCRIPTION = """
Generate coverage file for a given JSON to be used for fabric maintenance. Coverage files are essentially for a given polygon
how much of it is covered by another category class for all category classes in either the other given JSON or in the
intersections.csv file generated from coco-calc coverage. Polygons are matched together on the same image id they are in.

examples:
    cat first.json | coco-calc coverage --second second.json > first.coverage.csv # Generate coverage file for first JSON
    cat second.json | coco-calc coverage --first first.json > second.coverage.csv # Generate coverage file for second JSON
    coco-calc coverage --first first.json --second second.json > first.coverage.csv # Generate coverage file for first JSON
    
    cat first.json | coco-calc coverage --intersections intersections.csv # Generate coverage file for first JSON using intersections.csv file produced from coco-calc intersections
    
    # intersections.csv is not symmetrical so be aware of the different usage with --first/--second and --intersections
    coco-calc coverage --first first.json --intersections intersections.csv # Generate coverage file for first JSON using intersections.csv file produced from coco-calc intersections
    coco-calc coverage --second second.json --intersections intersections.csv # Generate coverage file for second JSON using intersections.csv file produced from coco-calc intersections
    
    coco-calc coverage --output-fields # Output CSV header fields
"""

_CROP_DESCRIPTION = """
Crop images and labels; currently supports only cropping images

examples:
    cat crop.json | applications/coco-calc -v crop --size 513,513 --point --apply-to-images --fit
    
"""

_INTERSECT_DESCRIPTION = """
Output intersecting polygons from stdin JSON with command line JSON in format of 
<image-id>,<first-polygon-id>,<second-polygon-id>,<first-polygon-area>,<second-polygon-area>,<intersection-area>
"""

_NMS_DESCRIPTION = """
Non Max Suppression
Remove non max overlapping bounding boxes
"""

_REMAP_DESCRIPTION = """
Remaps categories entry in a COCO json file based on its own existing
categories or a --target-categories can be given to remap existing categories
to new ones

examples
    Remap all categories in pf.json to a single category defined on the command line
    cat pf.json | coco-calc remap "{1:0, 2:0, 3:0, 4:0}" \
            --target-categories "[{'id': 0, 'name': 'PF', 'supercategory': ''}]" | sponge pf.json

    Remap category 1 to 0 and category 3 to 4 using the input COCO json as a source of categories to take from
    cat pf.json | coco-calc remap "{1:0, 3:4}" | sponge pf.json
"""

_REPATH_DESCRIPTION = """
Read coco file from stdin and re-path all images.

Output coco json contents in stdout.

examples:
    cat coco.json | coco-calc repath images
    cat coco.json | coco-calc repath /mnt/rhino/processed/industry-data/anadarko/images
"""

_RESIZE_DESCRIPTION = """
Resize images in a given coco json file. Image dimensions can be specified by [width],[height] or by a 
scaling factor to enlarge or reduce. Specify -o/--output-images with an output image directory which 
will write the new directory, if not created, in the current directory with the new images.

examples
    cat coco.json | coco-calc resize --factor 0.3
    cat coco.json | coco-calc --verbose resize --size 480,640
"""


def main(args: argparse.Namespace) -> int:
    # Setup logging of default warnings and verbose print
    logging.basicConfig(
        level=args.loglevel,
        datefmt='%Y-%m-%d %H:%M:%S',
        format='%(filename)s: %(asctime)s.%(msecs)d: %(levelname)s: line no %(lineno)s: %(message)s',
    )

    try:
        if args.output_fields:
            print(args.output_fields, file=sys.stdout)
            return 0
    except AttributeError:
        pass

    try:
        coco, status = args.func(args)
    except AttributeError:
        get_args(['--help'])
    else:
        if coco is None:
            logging.warning("No output coco, not necessarily an error")
        else:
            json.dump(coco.dataset, sys.stdout, indent=args.indent)
        logging.shutdown()
        return status


def crop(args: argparse.Namespace) -> Tuple[Union[COCO, None], int]:
    coco = MyCOCO(sys.stdin)

    if not args.point:
        logging.error(
            "crop: Operation currently not supported if crop points are not annotated as 'points', "
            "please use abyss-annotation-tool to label crop points"
        )
        return None, 1
    crop_width, crop_height = [int(i) for i in args.size.split(',')]
    if args.apply_to_images:
        os.makedirs(args.image_output_dir, exist_ok=True)
    imgs_out = []
    anns_out = []
    for img_entry in coco.dataset['images']:
        if os.path.isfile(img_entry['path']):
            out_path = img_entry['path']
        else:
            logging.error("crop: Image at {} not found".format(img_entry['path']))
            sys.exit(1)
        basename = os.path.basename(out_path)
        filename, _ = os.path.splitext(basename)
        stubname = os.path.join(args.image_output_dir, filename)
        anns = coco.loadAnns(coco.getAnnIds(img_entry['id']))
        if not any(ann_entry.get('annotation_type') == 'point' for ann_entry in anns):
            logging.info("crop: No cropping points for image {}".format(img_entry.get('id', -1)))
            continue
        img = imread(img_entry['path'])
        for ann_entry in anns:
            # Find crops from points defined in 'annotations'
            if ann_entry.get('annotation_type') != 'point':
                continue  # TODO: point selection quick and dirty for now; implement better usage semantics

            offset_x = int(ann_entry['bbox'][0]) - int(crop_width / 2)
            offset_y = int(ann_entry['bbox'][1]) - int(crop_height / 2)
            max_offset_x = img_entry['width'] - crop_width
            max_offset_y = img_entry['height'] - crop_height
            if args.fit:
                # Align image to border if crop goes out of image width or height
                offset_x = min(max(offset_x, 0), max_offset_x)
                offset_y = min(max(offset_y, 0), max_offset_y)
            elif offset_x < 0 or offset_y < 0 or offset_x >= max_offset_x or offset_y >= max_offset_y:
                logging.warning(
                    "crop: Discarded annotation with crop beyond image height and width "
                    "(or please use --fit to align the crop to image height and width): "
                    "annotation id: {} image: {}".format(ann_entry['id'], img_entry['path'])
                )
                continue
            cropped = img[offset_y: offset_y + crop_height, offset_x: offset_x + crop_width]
            out_path = stubname + '.' + str(ann_entry['id']) + '.png'
            cropped_entry = {
                'file_name': os.path.basename(out_path),
                'path': out_path,
                'id': len(imgs_out),
                'height': crop_height,
                'width': crop_width,
            }
            imgs_out.append(cropped_entry)
            for ann_entry in anns:
                if ann_entry.get('annotation_type') == 'point':
                    continue  # TODO: better semantics
                cropped_ann = copy.deepcopy(ann_entry)
                x, y, width, height = ann_entry['bbox']
                x -= offset_x
                y -= offset_y
                if not do_overlap((x, y, x + width, y + height), (0, 0, crop_width, crop_height)):
                    continue

                cropped_bbox = crop_bbox(ann_entry, crop_height, crop_width, (x, y, width, height), args.keep_partial)
                cropped_ann['bbox'] = cropped_bbox
                cropped_poly = None
                if isinstance(ann_entry['segmentation'], list):
                    cropped_poly = crop_poly(ann_entry, crop_height, crop_width, offset_x, offset_y, args.keep_partial)
                    cropped_ann['segmentation'] = [cropped_poly]
                elif isinstance(ann_entry['segmentation'], dict):
                    logging.warning('crop: TODO RLE annotations, currently discards these annotations')
                    continue
                cropped_ann['id'] = len(anns_out)
                cropped_ann['image_id'] = cropped_entry['id']
                if cropped_bbox is None or cropped_poly is None:
                    continue
                cropped_ann['area'] = polygon_area(np.array(cropped_poly[::2]), np.array(cropped_poly[1::2]))
                anns_out.append(cropped_ann)
            if args.apply_to_images:
                imwrite(cropped, out_path)
    coco.dataset['images'] = imgs_out
    coco.dataset['annotations'] = anns_out
    return coco, 0


def crop_bbox(ann_entry, crop_height, crop_width, bbox, keep_partial):
    x, y, width, height = bbox
    if not keep_partial and (x < 0 or y < 0 or x + width >= crop_width or y + height >= crop_height):
        logging.warning("Skipping bbox {}, specify --keep-partial to keep partial cropped bbox".format(ann_entry['id']))
        return None
    if x < 0:
        width += x
        x = 0
    if y < 0:
        height += y
        y = 0
    width = min(crop_width - x, width)
    height = min(crop_height - y, height)
    return x, y, width, height


def crop_poly(ann_entry, crop_height, crop_width, offset_x, offset_y, keep_partial):
    cropped_polygon = []
    polygon = ann_entry.get('segmentation', [[]])[0]
    for x, y in grouper(polygon, 2):
        x -= offset_x
        y -= offset_y
        if not keep_partial and (x < 0 or y < 0 or x >= crop_width or y >= crop_height):
            logging.warning(
                """Skipping segmentation {}, 
                specify --keep-partial to keep partial cropped segmentation""".format(ann_entry.get('id', -1))
            )
            return None
        x = min(max(0, x), crop_width)
        y = min(max(0, y), crop_height)
        cropped_polygon += [x, y]
    return cropped_polygon


def remap(args: argparse.Namespace) -> Tuple[Union[COCO, None], int]:
    coco = MyCOCO(sys.stdin)

    # literal_eval allows usage of either single quote (') or double quote(") to interpret a json string
    if os.path.isfile(args.mapping):
        with open(args.mapping) as f:
            mapping = {category['name']: category['id'] for category in json.load(f)['categories']}
    else:
        mapping = literal_eval(args.mapping)

    source_categories = coco.loadCats(ids=coco.getCatIds())
    if not source_categories:
        return coco, 1
    for category in source_categories:
        if not (category['id'] in mapping or category['name'] in mapping):
            mapping[category['name']] = category['name']

    output_categories = copy.deepcopy(coco.loadCats(ids=coco.getCatIds()))
    for category in output_categories:
        if category['name'] in mapping.keys():
            category_id = mapping[category['name']]
            if isinstance(category_id, int):
                category['id'] = category_id

    target_categories = source_categories
    if args.target_categories:
        args.target_categories = literal_eval(args.target_categories)
        if args.keep_source_categories:
            source_category_ids = set([category['id'] for category in output_categories])
            target_category_ids = set([category['id'] for category in args.target_categories])
            for idx in target_category_ids:
                if idx in source_category_ids:
                    logging.error("Duplicate id {} found between source and target categories".format(idx))
                    logging.error("Taken ids are {}".format(", ".join((str(i) for i in source_category_ids))))
                    sys.exit(1)
        target_categories = args.target_categories

    new_name_2_id = {category['name']: category['id'] for category in target_categories}
    source_category_names = [category['name'] for category in source_categories]
    target_category_names = [category['name'] for category in target_categories]
    annotations = []
    if len(coco.dataset['annotations']) > 0:
        added_annotation_ids = set()

        for old, new in mapping.items():
            if isinstance(old, str) and old not in source_category_names:
                logging.error("Cannot find source key {} in category names {}".format(old, source_category_names))
                sys.exit(1)

            if isinstance(new, str) and new not in target_category_names:
                logging.error("cannot find target key {} in category names {}".format(new, target_category_names))
                sys.exit(1)

            elif isinstance(old, int) and isinstance(new, int):
                for ann in coco.loadAnns(ids=coco.getAnnIds(catIds=old)):
                    ann_copy = copy.deepcopy(ann)
                    ann_copy['category_id'] = new
                    annotations.append(ann_copy)
                    added_annotation_ids.add(ann_copy['id'])
                category = coco.loadCats(ids=coco.getCatIds(catIds=old)).pop()
                category['id'] = new

            elif isinstance(old, int) and isinstance(new, str):
                if new in new_name_2_id.keys():
                    for ann in coco.loadAnns(ids=coco.getAnnIds(catIds=old)):
                        ann_copy = copy.deepcopy(ann)
                        ann_copy['category_id'] = new_name_2_id[new]
                        annotations.append(ann_copy)
                        added_annotation_ids.add(ann_copy['id'])

            elif isinstance(old, str) and isinstance(new, int):
                for ann in coco.loadAnns(ids=coco.getAnnIds(catIds=coco.getCatIds(catNms=old))):
                    ann_copy = copy.deepcopy(ann)
                    ann_copy['category_id'] = new
                    annotations.append(ann_copy)
                    added_annotation_ids.add(ann_copy['id'])
                category = coco.loadCats(ids=coco.getCatIds(catNms=old)).pop()
                category['id'] = new

            elif isinstance(old, str) and isinstance(new, str):
                if new in new_name_2_id.keys():
                    for ann in coco.loadAnns(ids=coco.getAnnIds(catIds=coco.getCatIds(catNms=old))):
                        ann_copy = copy.deepcopy(ann)
                        ann_copy['category_id'] = new_name_2_id[new]
                        annotations.append(ann_copy)
                        added_annotation_ids.add(ann_copy['id'])

        to_add_annotation_ids = set(coco.getAnnIds()) - added_annotation_ids
        annotations.extend(coco.loadAnns(ids=to_add_annotation_ids))

    coco.dataset['annotations'] = annotations

    categories = target_categories
    if args.keep_source_categories:
        categories.extend(source_categories)

    coco.dataset['categories'] = sorted(drop_duplicates(categories), key=itemgetter('id'))
    category_ids = coco.getCatIds()
    if len(set(category_ids)) != len(category_ids):
        logging.warning('There are duplicate category ids in the coco dataset.')

    return coco, 0


def resize(args: argparse.Namespace) -> Tuple[Union[COCO, None], int]:
    coco = MyCOCO(sys.stdin)

    if args.apply_to_images:
        os.makedirs(os.path.join(os.getcwd(), args.image_output_dir), exist_ok=True)

    for img_entry in coco.dataset['images']:
        img_height = img_entry.get('height')
        img_width = img_entry.get('width')

        if img_height is None or img_width is None:
            logging.warning("Cannot find 'height' or 'width' fields in COCO JSON. Looking for image")
            try:
                img_width, img_height = Image.open(img_entry.get('path')).size
            except AttributeError:
                logging.warning("Cannot find image {}, skipping".format(img_entry.get('id')))

        if args.factor is None:
            dims = args.size.split(',')
            h_scale = float(dims[0]) / img_width
            v_scale = float(dims[1]) / img_height
        else:
            h_scale = v_scale = args.factor

        img_entry['height'] = round(img_height * v_scale, None)
        img_entry['width'] = round(img_width * h_scale, None)

        if args.apply_to_images:
            if os.path.isfile(img_entry['path']):
                new_path = os.path.join(os.getcwd(), args.image_output_dir, os.path.basename(img_entry['path']))
                resized_img = imread(img_entry['path'], size=(img_entry['width'], img_entry['height']))
                img_entry['path'] = new_path
                logging.info("Saving image to {}".format(new_path))
                imwrite(resized_img, new_path)
            else:
                logging.warning("Cannot find path to image [{}]".format(img_entry['path']))

        # Rescaling bbox and segmentation values
        # format is [x, y, width, height]
        # Rescaled area value i.e. new_width * new_height
        for ann in coco.loadAnns(coco.getAnnIds(imgIds=img_entry['id'])):
            if 'bbox' in ann:
                x = round(ann['bbox'][0] * h_scale, args.digits)
                y = round(ann['bbox'][1] * v_scale, args.digits)
                width = round(ann['bbox'][2] * h_scale, args.digits)
                height = round(ann['bbox'][3] * v_scale, args.digits)
                ann['bbox'] = [x, y, width, height]
                ann['area'] = width * height

            if 'segmentation' not in ann:
                logging.warning(
                    "Annotation with id {} does not have 'segmentation'. Skipping annotation".format(ann['id'])
                )
                continue
            tmp = ann['segmentation']
            if type(tmp) is list:
                segment = tmp[0]
                new_segment = [
                    (round(x * h_scale, args.digits), round(y * v_scale, args.digits))
                    for x, y in
                    grouper(segment, 2)
                ]
                # Flatten the list of tuples
                new_segment = list(itertools.chain(*new_segment))
                ann['segmentation'] = [new_segment]
                ann['area'] = polygon_area(polygon_x(new_segment), polygon_y(new_segment))
            elif type(tmp) is dict:
                logging.warning("RLE is not yet supported")
            else:
                logging.warning("Unknown data type [{}]".format(type(tmp)))

    return coco, 0


def repath(args: argparse.Namespace) -> Tuple[Union[COCO, None], int]:
    coco = MyCOCO(sys.stdin)

    for img_entry in coco.dataset['images']:
        try:
            img_entry['path'] = os.path.join(args.path, os.path.basename(img_entry['path']))
        except KeyError:
            logging.warning("Unable to find 'path' key for image {}".format(img_entry))
            try:
                img_entry['path'] = os.path.join(args.path, os.path.basename(img_entry['file_name']))
            except KeyError:
                logging.warning("Unable to find 'file_name' key for image {}".format(img_entry))
    return coco, 0


def drop_duplicates(list_: list) -> list:
    def to_tuple(list_entry: list):
        return tuple(to_tuple(i) if isinstance(i, list) else i for i in list_entry)

    return [dict(i) for i in set(to_tuple([list(i) for i in sorted(i.items())]) for i in list_)]


def grouper(iterable, size: int, fillvalue=None):
    args = [iter(iterable)] * size
    return itertools.zip_longest(*args, fillvalue=fillvalue)


def assign_new_image_ids(args: argparse.Namespace) -> Tuple[Union[COCO, None], int]:
    """Assigning new image id methods"""
    coco = MyCOCO(sys.stdin)
    if args.assign_method == 'sort':
        return assign_from_sorted(coco, args)
    elif args.assign_method == 'coco':
        return assign_from_coco(coco, args)
    logging.warning("Output coco is the same as input coco")
    return coco, 1


#  Felzenszwalb et al.
def non_max_suppression_impl(boxes, overlap_thresh):
    # if there are no boxes, return an empty list
    if len(boxes) == 0:
        return []

    # initialize the list of picked indexes
    pick = []

    # grab the coordinates of the bounding boxes
    x1 = boxes[:, 0]
    y1 = boxes[:, 1]
    x2 = boxes[:, 2]
    y2 = boxes[:, 3]

    # compute the area of the bounding boxes and sort the bounding
    # boxes by the bottom-right y-coordinate of the bounding box
    area = (x2 - x1 + 1) * (y2 - y1 + 1)
    idxs = np.argsort(y2)

    # keep looping while some indexes still remain in the indexes
    # list
    while len(idxs) > 0:
        # grab the last index in the indexes list, add the index
        # value to the list of picked indexes, then initialize
        # the suppression list (i.e. indexes that will be deleted)
        # using the last index
        last = len(idxs) - 1
        i = idxs[last]
        pick.append(i)
        suppress = [last]

        # loop over all indexes in the indexes list
        for pos in range(0, last):
            # grab the current index
            j = idxs[pos]

            # find the largest (x, y) coordinates for the start of
            # the bounding box and the smallest (x, y) coordinates
            # for the end of the bounding box
            xx1 = max(x1[i], x1[j])
            yy1 = max(y1[i], y1[j])
            xx2 = min(x2[i], x2[j])
            yy2 = min(y2[i], y2[j])

            # compute the width and height of the bounding box
            w = max(0, xx2 - xx1 + 1)
            h = max(0, yy2 - yy1 + 1)

            # compute the ratio of overlap between the computed
            # bounding box and the bounding box in the area list
            overlap = float(w * h) / area[j]

            # if there is sufficient overlap, suppress the
            # current bounding box
            if overlap > overlap_thresh:
                suppress.append(pos)

        # delete all indexes from the index list that are in the
        # suppression list
        idxs = np.delete(idxs, suppress)

    # return only the bounding boxes that were picked
    return boxes[pick], pick


def groupbyUnsorted(input, key=lambda x: x):
    yielded = set()
    keys = [key(element) for element in input]
    for i, wantedKey in enumerate(keys):
        if wantedKey not in yielded:
            yield (wantedKey,
                   (input[j] for j in range(i, len(input)) if keys[j] == wantedKey))
        yielded.add(wantedKey)


def non_max_suppression(args: argparse.Namespace) -> Tuple[Union[COCO, None], int]:
    try:
        coco = MyCOCO(sys.stdin)
        ann = coco.dataset.get('annotations')

        keep_ann = []

        # perform nms per image per category
        group_on = lambda x: str(x['image_id']) + '-' + str(x['category_id'])

        # if it ever makes sense to ignore category
        # group_on = lambda x: x['image_id']

        for key, group in groupbyUnsorted(ann, group_on):
            g = list(group)
            boxes = np.empty((len(g), 4))
            for i, image_annotation in enumerate(g):
                # "bbox" : [x,y,width,height]
                bbox = image_annotation['bbox']
                boxes[i, :] = [bbox[0], bbox[1],
                               bbox[0] + bbox[2],
                               bbox[1] + bbox[3]]

            boxes, keep_i = non_max_suppression_impl(boxes, args.overlap_threshold)

            for i in keep_i:
                keep_ann.append(g[i])

        coco.dataset['annotations'] = keep_ann

        return coco, 0
    except Exception as e:
        print("Error!", e)
        print("Error!", sys.exc_info()[0], "occured.")
        raise


def assign_from_coco(stdin_coco: COCO, args: argparse.Namespace) -> Tuple[Union[COCO, None], int]:
    """

    Args:
        stdin_coco: COCO file read from stdin
        args: arguments passed from argparse

    Returns: COCO file with image ids that have been reassigned based on the image ids from a command line COCO file

    """
    old_id_2_key = {img_entry['id']: img_entry[args.key] for img_entry in stdin_coco.dataset['images']}
    with open(args.coco) as f:
        cl_coco = json.load(f)
    key_2_new_id = {img_entry[args.key]: img_entry['id'] for img_entry in cl_coco['images']}
    all_ids = set(range(len(stdin_coco.dataset['images']) + len(cl_coco['images'])))
    new_ids = {new_id for new_id in key_2_new_id.values()}
    available_ids = all_ids - new_ids
    old_id_2_new_id = {}
    if not args.keep_mismatch:
        kept_img_ids = set()
        kept_ann_ids = set()
    for old_id, key in old_id_2_key.items():
        if key not in key_2_new_id.keys():
            old_id_2_new_id[old_id] = available_ids.pop()
    for i, image_entry in enumerate(stdin_coco.dataset['images']):
        try:
            image_entry['id'] = key_2_new_id[old_id_2_key[image_entry['id']]]
            if not args.keep_mismatch:
                kept_img_ids.add(id(image_entry))
        except KeyError:
            if args.keep_mismatch:
                # current image entry key does not exist in the command line COCO file
                logging.warning(
                    "Image entry with id {} and key: value -> {}: {} cannot be found on command line COCO file".format(
                        image_entry['id'], args.key, image_entry[args.key]))
                logging.warning(
                    "ID {} will be assigned to image entry with id {}".format(old_id_2_new_id[image_entry['id']],
                                                                              image_entry['id']))
                image_entry['id'] = old_id_2_new_id[image_entry['id']]
            else:
                continue

    for i, ann_entry in enumerate(stdin_coco.dataset['annotations']):
        try:
            ann_entry['image_id'] = key_2_new_id[old_id_2_key[ann_entry['image_id']]]
            if not args.keep_mismatch:
                kept_ann_ids.add(id(ann_entry))
        except KeyError:
            if args.keep_mismatch:
                logging.warning("This annotation has an image that could not be matched")
                ann_entry['image_id'] = old_id_2_new_id[ann_entry['image_id']]
            else:
                continue

    if not args.keep_mismatch:
        stdin_coco.dataset['images'] = [img for img in stdin_coco.dataset['images'] if id(img) in kept_img_ids]
        stdin_coco.dataset['annotations'] = [ann for ann in stdin_coco.dataset['annotations'] if
                                             id(ann) in kept_ann_ids]
    return stdin_coco, 0


def assign_from_sorted(coco: COCO, args: argparse.Namespace) -> Tuple[Union[COCO, None], int]:
    """

    Args:
        coco: COCO file read from stdin
        args: arguments pass from argparse

    Returns: COCO file with image ids that have been reassigned based on the sorted order of the images from args.key

    """
    old_id_2_path = {image['id']: image['path'] for image in coco.dataset['images']}
    from operator import itemgetter
    coco.dataset['images'].sort(key=itemgetter(args.key))
    for i, image_entry in enumerate(coco.dataset['images']):
        image_entry['id'] = i
    path_2_new_id = {image_entry['path']: image_entry['id'] for image_entry in coco.dataset['images']}
    for annotation in coco.dataset.get('annotations'):
        annotation['image_id'] = path_2_new_id[old_id_2_path[annotation['image_id']]]
    return coco, 0


def intersections(args: argparse.Namespace) -> Tuple[Union[COCO, None], int]:
    first_coco = None
    second_coco = None

    if args.first is not None and args.second is not None:
        first_coco = MyCOCO(args.first)
        second_coco = MyCOCO(args.second)

    elif args.first is not None:
        first_coco = MyCOCO(args.first)
        second_coco = MyCOCO(sys.stdin)

    elif args.second is not None:
        second_coco = MyCOCO(args.second)
        first_coco = MyCOCO(sys.stdin)

    assert first_coco is not None and second_coco is not None, "could not load --first or --second COCO JSON's properly"

    for first_img_entry in first_coco.dataset['images']:
        try:
            second_img_entry = second_coco.loadImgs(ids=first_img_entry['id']).pop()
        except KeyError as e:
            logging.warning(
                "Cannot find matching image {} from stdin in JSON file {},"
                "try matching image ids using coco-calc assign-new-image-ids".format(first_img_entry['id'], args.second)
            )
            raise e
        first_ann_entries, first_polys = get_img_polys(first_coco, first_img_entry['id'])
        second_ann_entries, second_polys = get_img_polys(second_coco, second_img_entry['id'])
        first_areas, second_areas, intersection_areas = poly_intersection_area(first_polys, second_polys)
        if intersection_areas.size == 0:
            logging.warning("Skipping image {} as no annotations exist".format(first_img_entry['id']))
            continue

        first_areas = np.array(first_areas, dtype=np.uint)  # cast to numpy array
        second_areas = np.array(second_areas, dtype=np.uint)  # cast to numpy array
        if args.output_non_intersecting == 'first':
            first_idxs = np.where(~intersection_areas.any(axis=1))[0]
            for i in first_idxs.tolist():
                print(",".join(str(j) for j in (
                    first_img_entry['path'],
                    first_img_entry['id'],
                    first_ann_entries[i]['id'],
                    first_ann_entries[i]['category_id'],
                    first_areas[i],
                )))
        elif args.output_non_intersecting == 'second':
            second_idxs = np.where(~intersection_areas.any(axis=0))[0]
            for i in second_idxs.tolist():
                print(",".join(str(j) for j in (
                    first_img_entry['path'],
                    first_img_entry['id'],
                    second_ann_entries[i]['id'],
                    second_ann_entries[i]['category_id'],
                    second_areas[i],
                )))
        else:
            first_idxs, second_idxs = np.where(intersection_areas != 0)
            for i, j in zip(first_idxs.tolist(), second_idxs.tolist()):
                if first_areas[i] <= 0:
                    logging.warning("annotation {} from {} may be malformed polygon as it has area {}".format(
                        first_ann_entries[i]['id'], args.first, first_areas[i]
                    ))

                if second_areas[j] <= 0:
                    logging.warning("annotation {} from {} may be malformed polygon as it has area {}".format(
                        second_ann_entries[j]['id'], args.second, second_areas[j]
                    ))

                print(",".join(str(k) for k in (
                    first_img_entry['path'],
                    first_img_entry['id'],
                    first_ann_entries[i]['id'],
                    first_ann_entries[i]['category_id'],
                    first_areas[i],
                    second_ann_entries[j]['id'],
                    second_ann_entries[j]['category_id'],
                    second_areas[j],
                    intersection_areas[i, j],
                )))
    return None, 0


def coverage(args: argparse.Namespace) -> Tuple[Union[COCO, None], int]:
    if args.intersections is None:
        first_coco = None
        second_coco = None

        if args.first is not None and args.second is None:
            first_coco = MyCOCO(args.first)
            second_coco = MyCOCO(sys.stdin)

        elif args.first is None and args.second is not None:
            first_coco = MyCOCO(sys.stdin)
            second_coco = MyCOCO(args.second)

        elif args.first is not None and args.second is not None:
            first_coco = MyCOCO(args.first)
            second_coco = MyCOCO(args.second)

        assert first_coco is not None and second_coco is not None, "could not load first or second COCO file"

        for first_img_entry in first_coco.images:
            try:
                second_img_entry = second_coco.loadImgs(ids=first_img_entry['id']).pop()
            except KeyError as e:
                logging.warning(
                    "cannot find matching image {} from stdin in JSON file {},"
                    "try matching image ids using coco-calc assign-new-image-ids".format(first_img_entry['id'],
                                                                                         args.second)
                )
                raise e
            first_ann_entries, first_polys = get_img_polys(first_coco, first_img_entry['id'])
            second_ann_entries, second_polys = get_img_polys(second_coco, second_img_entry['id'])
            first_areas, _, intersection_areas = poly_intersection_area(first_polys, second_polys)
            if intersection_areas.size == 0:
                logging.warning("skipping image {} as no annotations exist".format(first_img_entry['id']))
                continue

            first_areas = np.array(first_areas, dtype=np.uint)
            for i in range(len(first_ann_entries)):
                first_area = first_areas[i]
                if first_area <= 0:
                    logging.warning("annotation {} from {} may be malformed polygon as it has area {}".format(
                        first_ann_entries[i]['id'], args.first, first_area
                    ))
                    logging.warning("setting annotation {} to area from {} with area of {}".format(
                        first_ann_entries[i]['id'], args.first, first_ann_entries[i].get('area')
                    ))
                    first_area = first_ann_entries[i].get('area', 0)

                if first_area <= 0:
                    logging.warning("annotation {} from {} has incorrect area value of {}, setting to -1".format(
                        first_ann_entries[i]['id'], args.first, first_area
                    ))
                    first_area = -1

                category_sums = {id_: 0 for id_ in second_coco.getCatIds()}
                for j in range(len(second_ann_entries)):
                    category_sums[second_ann_entries[j]['category_id']] += intersection_areas[i][j]

                for category, area in category_sums.items():
                    print(",".join(str(k) for k in (
                        first_img_entry['path'],
                        first_img_entry['id'],
                        first_ann_entries[i]['id'],
                        first_ann_entries[i]['category_id'],
                        first_area,
                        category,
                        area,
                        float(area / first_area)
                    )))
    else:
        # TODO: Check with Seva if category ids should be inferred from input JSON or from input CSV
        logging.warning("inferring coverage category ids from {}".format(args.intersections))
        df = pd.read_csv(args.intersections, names=[
            'path',
            'image_id',
            'first_ann_id',
            'first_cat_id',
            'first_ann_area',
            'second_ann_id',
            'second_cat_id',
            'second_ann_area',
            'intersection_area',
        ])

        # TODO: Refactor if body to function
        coco = None
        if args.second is not None:
            coco = MyCOCO(args.second)

            for img_entry in coco.images:
                for ann in coco.loadAnns(coco.getAnnIds(imgIds=img_entry['id'])):
                    category_sums = {id_: 0 for id_ in df['first_cat_id'].unique().tolist()}
                    second_area = 0
                    for intersection in df[df.second_ann_id == ann['id']].itertuples(index=False):
                        second_area = intersection.second_ann_area
                        category_sums[intersection.first_cat_id] += intersection.intersection_area

                    if second_area <= 0:
                        logging.warning(
                            "annotation {} from {} may be malformed polygon or missing from {} as it has area {}".format(
                                ann['id'], args.second, args.intersections, second_area
                            ))
                        logging.warning('consider recalculating annotation area')
                        logging.warning("setting annotation {} to area from {} with area of {}".format(
                            ann['id'], args.second, ann.get('area')
                        ))
                        second_area = ann.get('area', 0)

                    if second_area <= 0:
                        logging.warning("annotation {} from {} has incorrect area value of {}, setting to -1".format(
                            ann['id'], args.second, second_area
                        ))
                        second_area = -1

                    for category, area in category_sums.items():
                        print(",".join(str(i) for i in (
                            img_entry['path'],
                            img_entry['id'],
                            ann['id'],
                            ann['category_id'],
                            second_area,
                            category,
                            area,
                            float(area / second_area),
                        )))

        else:
            if args.first is None and args.second is None:
                coco = MyCOCO(sys.stdin)
                logging.warning("assuming coco from stdin to be --first")
            elif args.first is not None:
                coco = MyCOCO(args.first)

            if coco is None:
                logging.error("coco JSON was not loaded properly")
                return None, 1

            for img_entry in coco.images:
                for ann in coco.loadAnns(coco.getAnnIds(imgIds=img_entry['id'])):
                    category_sums = {id_: 0 for id_ in df['second_cat_id'].unique().tolist()}
                    first_area = 0
                    for intersection in df[df.first_ann_id == ann['id']].itertuples(index=False):
                        first_area = intersection.first_ann_area
                        category_sums[intersection.second_cat_id] += intersection.intersection_area

                    if first_area <= 0:
                        logging.warning(
                            "annotation {} from {} may be malformed polygon or missing from {} as it has area {}".format(
                                ann['id'], args.first, args.intersections, first_area
                            ))
                        logging.warning('consider recalculating annotation area')
                        logging.warning("setting annotation {} to area from {} with area of {}".format(
                            ann['id'], args.first, ann.get('area')
                        ))
                        first_area = ann.get('area', 0)

                    if first_area <= 0:
                        logging.warning("annotation {} from {} has incorrect area value of {}, setting to -1".format(
                            ann['id'], args.first, first_area
                        ))
                        first_area = -1

                    for category, area in category_sums.items():
                        print(",".join(str(i) for i in (
                            img_entry['path'],
                            img_entry['id'],
                            ann['id'],
                            ann['category_id'],
                            first_area,
                            category,
                            area,
                            float(area / first_area),
                        )))

    return None, 0


def get_img_polys(coco: COCO, img_id: int) -> Tuple[dict, np.array]:
    ann_entries = coco.loadAnns(coco.getAnnIds(imgIds=img_id))
    polygons = [
        np.reshape(np.array(ann['segmentation'][0]), (len(ann['segmentation'][0]) // 2, 2)) for
        ann in ann_entries
    ]
    return ann_entries, polygons


def get_args(flags: List[str] = None) -> argparse.Namespace:
    verbose = MyParser(add_help=False)
    verbose.add_argument(
        '--verbose',
        action='store_const',
        const=logging.INFO,
        dest='loglevel',
        help="verbose output to stderr",
    )
    debug = MyParser(add_help=False)
    debug.add_argument(
        '--debug',
        action='store_const',
        const=logging.DEBUG,
        dest='loglevel',
        help='debug output to stderr',
    )
    min_ = MyParser(add_help=False)
    min_.add_argument(
        '--minified',
        action='store_const',
        const=None,
        default=4,
        dest='indent',
        help="disable JSON pretty print",
    )
    apply_to_images = MyParser(add_help=False)
    apply_to_images.add_argument(
        '--apply-to-images',
        action='store_true',
        help='apply operation to images, output to directory specified by --image-output-dir',
    )
    image_output_dir = MyParser(add_help=False)
    image_output_dir.add_argument(
        '-o', '--image-output-dir',
        default='images',
        help='image output directory. Default is %(default)s',
    )
    parents = [verbose, debug, min_]
    image_operations = [apply_to_images, image_output_dir]

    parser = MyParser(
        description=_DESCRIPTION,
        formatter_class=argparse.RawDescriptionHelpFormatter,
        parents=parents,
    )
    subparsers = parser.add_subparsers()

    parser_crop = subparsers.add_parser(
        'crop',
        description=_CROP_DESCRIPTION,
        formatter_class=argparse.RawDescriptionHelpFormatter,
        parents=parents + image_operations,
        help="crop images and annotations. Annotations can be partially cropped if --keep-partial is specified",
    )

    parser_crop.add_argument(
        '--fit',
        action='store_true',
        help="if centre of cropped area is too close to image dimension, shift cropped area to fit inside image",
    )
    parser_crop.add_argument(
        '--keep-partial',
        action='store_true',
        help="if crop window cuts the annotation, only partially crop it, otherwise discard",
    )
    parser_crop.add_argument(
        '--point',
        action='store_true',
        help="centre of crop area is identified by annotation_type 'point'",
    )
    parser_crop.add_argument(
        '-s', '--size',
        help="<width>,<height> crop size"
    )

    parser_resize = subparsers.add_parser(
        'resize',
        description=_RESIZE_DESCRIPTION,
        formatter_class=argparse.RawDescriptionHelpFormatter,
        parents=parents + image_operations,
        help="resize all images and their respective annotations by a scaling factor or by specified width and height",
    )

    parser_resize.add_argument(
        '-d', '--digits',
        type=int,
        help="number of decimal digits in bbox and segmentation coordinates; default: %(default)s",
    )
    parser_resize_mx = parser_resize.add_mutually_exclusive_group(required=True)
    parser_resize_mx.add_argument(
        '-f', '--factor',
        type=float,
        help="scale images by a percentage",
    )
    parser_resize_mx.add_argument(
        '-s', '--size',
        help="<width>,<height> size",
    )

    parser_remap = subparsers.add_parser(
        'remap',
        description=_REMAP_DESCRIPTION,
        formatter_class=argparse.RawDescriptionHelpFormatter,
        parents=parents,
        help="remap categories and/or category id in COCO JSON",
    )
    parser_remap.add_argument(
        'mapping',
        help="dict style string for remapping of either category id/category name to category id/category name",
    )
    parser_remap.add_argument(
        '--keep-source-categories',
        action='store_true',
        help="if target categories is specified, append target to input categories",
    )
    parser_remap.add_argument(
        '--target-categories',
        help="Target categories to be used for re-mapping. "
             "Same format as default COCO categories list. Default categories is taken from input COCO JSON file",
    )

    parser_repath = subparsers.add_parser(
        'repath',
        description=_REPATH_DESCRIPTION,
        formatter_class=argparse.RawDescriptionHelpFormatter,
        parents=parents,
        help="Re-path every image in COCO JSON.",
    )
    parser_repath.add_argument(
        'path',
        help="New path to prepend to each image in data set",
    )

    parser_coverage = subparsers.add_parser(
        'coverage',
        description=_COVERAGE_DESCRIPTION,
        formatter_class=argparse.RawDescriptionHelpFormatter,
        parents=parents,
        help="Output coverage of polygon annotations in one JSON file against polygon annotations in another JSON file"
    )
    parser_coverage.add_argument(
        '--first', '--covered',
        help="First JSON file, match polygons against second JSON file and calculate coverage, or use precomputed "
             "intersections file from coco-calc intersections to calculate coverage.",
    )
    parser_coverage.add_argument(
        '--intersections',
        help="Intersections CSV produced from coco-calc intersections to be used rather than computing from two JSON files",
    )
    parser_coverage.add_argument(
        '--output-fields',
        action='store_const',
        const="image/path,"
              "image/id,"
              "id,"
              "category/id,"
              "area,"
              "coverage/category/id,"
              "coverage/area,"
              "coverage/ratio",
        help="Print output CSV header fields to stdout and exit",
    )
    parser_coverage.add_argument(
        '--second', '--covering',
        help="Second JSON file, polygons to be matched against from first JSON file to produced coverage for first, or "
             "use precomputed intersections file from coco-calc intersections to calculate coverage",
    )

    parser_intersections = subparsers.add_parser(
        'intersections',
        description=_INTERSECT_DESCRIPTION,
        formatter_class=argparse.RawDescriptionHelpFormatter,
        parents=parents,
        help='Output intersecting polygons along with their polygon areas and intersection area in CSV format',
    )
    parser_intersections.add_argument(
        '--first', '--covered',
        help="First JSON, TODO: Add better help"
    )
    parser_intersections.add_argument(
        '--output-fields',
        action='store_const',
        const="image/path,image/id,"
              "first/id,first/category_id,first/area,"
              "second/id,second/category_id,second/area,"
              "intersection/area",
        help="Print output CSV fields to stdout and exit",
    )
    parser_intersections.add_argument(
        '--output-non-intersecting',
        choices=['first', 'second'],
        help="Output non-intersecting polygons from either first or second JSON",
    )
    parser_intersections.add_argument(
        '--second', '--covering',
        help="JSON file to match intersecting polygons with",
    )

    parser_non_max_suppression = subparsers.add_parser(
        'nms',
        description=_NMS_DESCRIPTION,
        formatter_class=argparse.RawDescriptionHelpFormatter,
        parents=parents,
        help='Non Max Suppression - Remove overlapping bounding boxes',
    )
    parser_non_max_suppression.add_argument(
        '--overlap-threshold',
        default=0.5,
        help="Remove where overlap > overlap threshold default: %(default)s.",
    )

    parser_new_img_ids = subparsers.add_parser(
        'assign-new-image-ids',
        description=_ASSIGN_NEW_IMAGE_IDS_DESCRIPTION,
        formatter_class=argparse.RawDescriptionHelpFormatter,
        parents=parents,
        help="Assign new ids to COCO file from stdin based on ids from COCO file passed as command line argument.",
    )
    parser_new_img_ids.add_argument(
        '--keep-mismatch',
        action='store_true',
        help="Keep image if the key to match by does not exist from the COCO file passed from command line, "
             "otherwise it is dropped from the output along with its annotations",
    )

    subparser_new_ids = parser_new_img_ids.add_subparsers(dest='assign_method')
    parser_sort = subparser_new_ids.add_parser(
        'sort',
        parents=parents,
        help="Sort images by a given key and assign new ids based on ordering",
    )
    parser_sort.add_argument(
        '-k', '--key',
        help="Key to sort by before assigning imade ids. "
             "If no key is given, assigned new image ids based on existing ordering",
    )

    parser_coco = subparser_new_ids.add_parser(
        'coco',
        parents=parents,
        help="Match images from stdin COCO with command line COCO by key and assign image ids from command line COCO.",
    )
    parser_coco.add_argument(
        'coco',
        help="COCO file to assign image ids from.",
    )
    parser_coco.add_argument(
        '-k', '--key',
        default='path',
        help="Key to match by. Default is %(default)s.",
    )

    parser_crop.set_defaults(func=crop)
    parser_coverage.set_defaults(func=coverage)
    parser_intersections.set_defaults(func=intersections)
    parser_new_img_ids.set_defaults(func=assign_new_image_ids)
    parser_non_max_suppression.set_defaults(func=non_max_suppression)
    parser_remap.set_defaults(func=remap)
    parser_repath.set_defaults(func=repath)
    parser_resize.set_defaults(func=resize)
    return parser.parse_args(flags)


class MyParser(argparse.ArgumentParser):
    def error(self, msg):
        self.print_help(sys.stderr)
        logging.critical(msg)
        sys.exit(1)


class Verbose:
    @staticmethod
    def write(line):
        line = line.strip()
        if line:
            logging.info(line)


if __name__ == '__main__':
    sys.exit(main(get_args()))
