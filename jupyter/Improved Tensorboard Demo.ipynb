{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "from keras.regularizers import l1_l2\n",
    "from keras.models import Model\n",
    "from keras.callbacks import TensorBoard, ReduceLROnPlateau\n",
    "from tensorflow.python import debug as tf_debug\n",
    "import keras.layers as layers\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "\n",
    "from abyss_deep_learning.keras.tensorboard import ImprovedTensorBoard, procuce_embeddings_tsv\n",
    "from abyss_deep_learning.keras.metrics import mpca_factory, auc_factory\n",
    "\n",
    "# Note the debugger doesn't work with Jupyter/Docker well. Change the address below to your computers name or localhost.\n",
    "K.set_session(\n",
    "    tf_debug.TensorBoardDebugWrapperSession(K.get_session(), \"herbicide_dl:7003\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Setup data #####\n",
    "# This will load MNIST, select an evenly sampled val_data, scale and shuffle the data\n",
    "val_data_samples_per_class = 10\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "val_data = (x_test, y_test)\n",
    "mask = np.hstack([\n",
    "    np.random.choice(np.where(y_test == l)[0], val_data_samples_per_class, replace=False)\n",
    "    for l in np.unique(val_data[1])])\n",
    "\n",
    "x_train = x_train[..., np.newaxis] / 127.5 - 1\n",
    "x_test = x_test[..., np.newaxis] / 127.5 - 1\n",
    "y_train_ = np.zeros((len(y_train), 10))\n",
    "y_train_[np.arange(len(y_train)), y_train] = 1\n",
    "y_train = y_train_\n",
    "y_test_ = np.zeros((len(y_test), 10))\n",
    "y_test_[np.arange(len(y_test)), y_test] = 1\n",
    "y_test = y_test_\n",
    "del y_train_, y_test_\n",
    "\n",
    "idx = np.arange(len(mask))\n",
    "np.random.shuffle(idx)\n",
    "val_data = (x_test[mask][idx], y_test[mask][idx])\n",
    "\n",
    "print(\"val_data has\", val_data[1].shape, \"shape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Model Construction Functions ###########\n",
    "\n",
    "def tensor_size(x):\n",
    "    return np.prod(x.shape.as_list()[1:])\n",
    "\n",
    "def make_backbone(x, num_layers, l1, l2):\n",
    "    for i in range(num_layers):\n",
    "        name = 'conv{:d}'.format(i)\n",
    "        x = layers.Conv2D(\n",
    "            64 * 2 ** i, 3,\n",
    "            strides=1, kernel_regularizer=l1_l2(0, l2 / tensor_size(x))\n",
    "        )(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.ActivityRegularization(l1 / tensor_size(x))(x)\n",
    "    x = layers.GlobalAveragePooling2D(name='features')(x)\n",
    "    return x\n",
    "\n",
    "def make_head(x):\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(10, activation='softmax', name='predictions')(x)\n",
    "    return x\n",
    "\n",
    "########### Scalar Summary Functions ##########\n",
    "# these are also defined in abyss_deep_learning.keras.tensorboard, but shown here for demo purposes.\n",
    "\n",
    "def kernel_sparsity(model, min_value=1e-6):\n",
    "    num = tf.zeros(1)\n",
    "    den = tf.zeros(1)\n",
    "    for weight in model.trainable_weights:\n",
    "        size = tf.cast(tf.size(weight), tf.float32)\n",
    "        zeros = size - tf.cast(tf.count_nonzero(tf.greater(weight, min_value)), tf.float32)\n",
    "        num += zeros\n",
    "        den += size\n",
    "    return num / den\n",
    "\n",
    "def avg_update_ratio(model, weight):\n",
    "    grads = model.optimizer.get_gradients(model.total_loss, [weight])[0]\n",
    "    return tf.norm(grads) * model.optimizer.lr / tf.norm(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Setup vars #########\n",
    "log_dir = '/tmp/test' # For tensorboard output\n",
    "batch_size = 32\n",
    "shape = (28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Construct the model #######3\n",
    "K.clear_session()\n",
    "inputs = layers.Input(shape=shape)\n",
    "model = Model(\n",
    "    inputs,\n",
    "    make_head(\n",
    "        make_backbone(inputs, num_layers=3, l1=1e-2, l2=1e-2)))\n",
    "model.compile(\n",
    "    'nadam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[\n",
    "        'categorical_accuracy',\n",
    "        mpca_factory(num_classes=y_test.shape[1]),\n",
    "        auc_factory('PR'),\n",
    "        auc_factory('ROC'),\n",
    "    ])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Instantiate and save the callbacks list ######3\n",
    "predictions_kernel = model.layers[-1].trainable_weights[0] # Used in a scalar callback\n",
    "\n",
    "callbacks = [\n",
    "    ImprovedTensorBoard(\n",
    "        log_dir=log_dir,\n",
    "        histogram_freq=5, batch_size=batch_size,\n",
    "        scalars={\n",
    "            'learning_rate': model.optimizer.lr,\n",
    "            'feature_sparsity': kernel_sparsity(model),\n",
    "            'prediction_UW_ratio': avg_update_ratio(model, predictions_kernel)\n",
    "        },\n",
    "        groups={'performance': {\n",
    "            'loss': ['loss', 'val_loss'],\n",
    "            'accuracy': [r'.*accuracy.*'],\n",
    "            'Mean Per-Class Average Accuracy': [r'.*mpca.*'],\n",
    "            'Mean Avg Precision': [r'.*PR.*'],\n",
    "            'ROC AUC': [r'.*ROC.*']\n",
    "        }},\n",
    "        pr_curve=True,\n",
    "        num_classes=val_data[1].shape[1],\n",
    "        write_graph=True,\n",
    "        write_grads=True,\n",
    "        write_images=False,\n",
    "        embeddings_freq=10,\n",
    "        embeddings_layer_names=['predictions', 'features'],\n",
    "        embeddings_metadata=(log_dir + \"/data_labels.tsv\"),\n",
    "        embeddings_data=val_data[0],\n",
    "#         val_size=len(x_test), \n",
    "#         img_path='/tmp/test/mnist_10k_sprite.png', img_size=(28, 28)\n",
    "    ),\n",
    "    ReduceLROnPlateau(patience=3, factor=0.5, min_delta=0.0, verbose=1)\n",
    "]\n",
    "\n",
    "y = val_data[1].argmax(axis=1)\n",
    "procuce_embeddings_tsv(\n",
    "    log_dir + \"/data_labels.tsv\",\n",
    "    headers=['label', 'text'],\n",
    "    labels=np.array([y + 1, y]).T.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Train the model #######\n",
    "epochs = [11]\n",
    "lrs = [5e-3]\n",
    "epoch = 0\n",
    "print(\"val_data\", val_data[1].shape)\n",
    "for num_epochs, lr in zip(epochs, lrs):\n",
    "    K.set_value(model.optimizer.lr, lr)\n",
    "    model.fit(\n",
    "        x=x_train[::10], y=y_train[::10],\n",
    "        batch_size=batch_size,\n",
    "        validation_data=val_data,\n",
    "        epochs=(epoch + num_epochs),\n",
    "        callbacks=callbacks,\n",
    "        initial_epoch = epoch\n",
    "    )\n",
    "    epoch += num_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now go check out tensorboard!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check training is working\n",
    "Calculate update / weight ratio, should be ~ 1e-3 if training well.\n",
    "Note the predictions/kernel value is shown as a custom scalar in tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_update_ratios(model, batch_data):\n",
    "    names, weights, values = zip(*[(weight.name, weight, K.get_value(weight))\n",
    "        for layer in model.layers if hasattr(layer, 'weights')\n",
    "        for weight in layer.weights])\n",
    "    lr = K.eval(model.optimizer.lr)\n",
    "    grads = model.optimizer.get_gradients(model.total_loss, weights)\n",
    "    tensors = (model.inputs + model.targets + model.sample_weights)\n",
    "    feed_dict = dict(zip(tensors, batch_data))\n",
    "\n",
    "    grads, values_2 = K.get_session().run([grads, weights], feed_dict=feed_dict)\n",
    "    stats = []\n",
    "    for name, weight, value, value2, grad in zip(names, weights, values, values_2, grads):\n",
    "        update_ratio = np.linalg.norm(grad) * lr / np.linalg.norm(value)\n",
    "        stats.append(update_ratio)\n",
    "        print(\"{:30s}{:.2e}\".format(name, update_ratio))\n",
    "    return np.array(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Update to weight ratios:\")\n",
    "batch_val = [x_test[:batch_size, ...], y_test[:batch_size], np.ones(batch_size)]\n",
    "stats = model_update_ratios(model, batch_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
