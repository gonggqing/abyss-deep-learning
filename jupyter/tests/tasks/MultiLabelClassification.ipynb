{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "\n",
    "from keras.backend import clear_session\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from abyss_deep_learning.datasets.coco import ImageClassificationDataset\n",
    "from abyss_deep_learning.datasets.translators import CocoCaptionTranslator\n",
    "from abyss_deep_learning.keras.classification import caption_map_gen, onehot_gen, hamming_loss\n",
    "from abyss_deep_learning.keras.utils import lambda_gen, batching_gen\n",
    "from abyss_deep_learning.keras.models import ImageClassifier\n",
    "from abyss_deep_learning.utils import balanced_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directory paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/mnt/ssd1/processed/industry-data/swc/train_1/cloudfactory/datasets/with-bg/notebook-ready/split-batch1/\"\n",
    "JSON_FILE = \"train-nb.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_DIR = \"/mnt/ssd1/processed/industry-data/project-max/ml/cloud-factory-data/with-bg/multi-label-datasets\n",
    "# /forwards/notebook-ready\"\n",
    "\n",
    "DATA_DIR = \"/home/users/khu/src/abyss/project-max/ml/cloud-factory-data/with-bg/multi-label-datasets/both/notebook-ready\"\n",
    "\n",
    "DIR_NAME = os.path.dirname(os.path.realpath('__file__'))\n",
    "IM_DIR = os.path.join(DIR_NAME, \"../../../../project-max/all-images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correct image paths in train json file \n",
    "# Count occurrences of classes for multi label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_DIR, \"train-nb.json\"), 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "for image in (data[\"images\"]):\n",
    "    image_name = image['path'].rpartition('/')\n",
    "    image['path'] = os.path.join(IM_DIR, image_name[2])\n",
    "    \n",
    "caption_count = {}\n",
    "    \n",
    "for ann in data[\"annotations\"]:\n",
    "    for cap in ann['caption'].split(','):\n",
    "        if cap not in caption_count:\n",
    "            caption_count[cap] = 0\n",
    "        caption_count[cap] += 1\n",
    "\n",
    "with open(os.path.join(DATA_DIR, \"train-nb.json\"), 'w') as json_file:\n",
    "    json.dump(data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count of all classes in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(caption_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = sum(caption_count.values())\n",
    "caption_pct = {}\n",
    "for key, value in caption_count.items():\n",
    "    caption_pct[key] = value / total * 100\n",
    "print(caption_pct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correct images paths in validation json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_DIR, \"val-nb.json\"), 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "for image in (data[\"images\"]):\n",
    "    image_name = image['path'].rpartition('/')\n",
    "    image['path'] = os.path.join(IM_DIR, image_name[2])\n",
    "\n",
    "with open(os.path.join(DATA_DIR, \"val-nb.json\"), 'w') as json_file:\n",
    "    json.dump(data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load annotations into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "train_ds = ImageClassificationDataset(\n",
    "    os.path.join(DATA_DIR, JSON_FILE),\n",
    "    image_dir=DATA_DIR,\n",
    "    cached=False,\n",
    "    translator=CocoCaptionTranslator(separator=\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced = list(balanced_set(train_ds.coco))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SeRJ', 'SO', 'StB', 'F', 'SJ', 'SeRM', 'StD', 'StJL', 'StiL', 'SeRT', 'SeE', 'StJR', 'SeRF', 'StF', 'STF', 'StCR', 'BG', 'SeRB'}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'class_weights'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b6f657386da2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_class_stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/abyss_deep_learning/datasets/coco.py\u001b[0m in \u001b[0;36mclass_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclass_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;34m'''Returns the class weights that will balance the backprop update over the class distribution.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprint_class_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'class_weights'"
     ]
    }
   ],
   "source": [
    "train_ds.print_class_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_ds = ImageClassificationDataset(\n",
    "    os.path.join(DATA_DIR, \"val-nb.json\"),\n",
    "    image_dir=DATA_DIR,\n",
    "    cached=False,\n",
    "    translator=CocoCaptionTranslator(separator=','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds._calc_class_stats()\n",
    "print(val_ds.captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caption map for one hot gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_map = {key: i for i, key in enumerate(caption_count)}\n",
    "print(caption_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reverse caption map to determine what class an integer represents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_map_r = {val: key for key, val in caption_map.items()}\n",
    "print(caption_map_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine class weights to penalize frequently occurring classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights =  1 / np.array([j**2 for i, j in sorted(caption_count.items(), key=lambda x: x[0])], dtype=np.float32)\n",
    "class_weights /= np.linalg.norm(class_weights)\n",
    "class_weights = dict(zip(sorted(caption_count.keys()), class_weights.tolist()))\n",
    "\n",
    "for key in caption_map:\n",
    "    class_weights[caption_map[key]] = class_weights.pop(key)\n",
    "\n",
    "print(\"class_weights:\")\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define pipeline for the generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(gen, caption_map):\n",
    "    \"\"\" Pipeline consists of lambda expression mapping x -> x, and y to become a set of comma separated captions\n",
    "        i.e.  {'ED,IP'} -> {'ED', 'IP'}\n",
    "        caption_map_gen converts caption labels to numeric integers\n",
    "        onehot_gen converts numeric integers to a vector of 1's and 0's where 1 is a given label\n",
    "    \"\"\"\n",
    "    return onehot_gen(\n",
    "        caption_map_gen(\n",
    "            lambda_gen(gen, lambda x, y: (x, set(y.pop().split(',')))),\n",
    "            caption_map\n",
    "        ),\n",
    "        len(caption_map)\n",
    "    )\n",
    "\n",
    "def create_new_model():\n",
    "    clear_session()\n",
    "    model = ImageClassifier(\n",
    "        backbone='xception', input_shape=(None, None, 3), classes=5,\n",
    "        init_lr=1e-5, init_weights='imagenet',\n",
    "        trainable=True, loss='binary_crossentropy', output_activation='sigmoid',\n",
    "        metrics=['accuracy', hamming_loss]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process 100 images 1 by 1 per epoch for 10 epochs\n",
    "\n",
    "# Cross validate on 100 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "#steps_per_epoch = math.floor(len(train_ds.coco.getImgIds()) / batch_size)\n",
    "steps_per_epoch = 100\n",
    "\n",
    "model = create_new_model()\n",
    "print(\"Break-even loss is\", -np.log(1 / model.classes))\n",
    "model.save_on_epoch_end()\n",
    "model.fit_generator(batching_gen(pipeline(train_ds.generator(data_ids=balanced,shuffle_ids=True), caption_map), batch_size=batch_size),\n",
    "                    validation_data=batching_gen(pipeline(val_ds.generator(endless=True), caption_map), batch_size=batch_size), \n",
    "                    validation_steps=100,\n",
    "                    epochs=10, \n",
    "                    use_multiprocessing=True,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.history.validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model_.load_weights('./logs/weights_epoch:01-val_loss:0.39.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_image, val_labels = val_ds.sample()\n",
    "\n",
    "print(\"Predictions (%)\")\n",
    "print(model.predict_proba(np.expand_dims(val_image, axis=0))[0])\n",
    "print(\"Actual\")\n",
    "print([caption_map[x] for x in val_labels])\n",
    "plt.imshow(val_ds.sample()[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
