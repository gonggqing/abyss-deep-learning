{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from abyss_deep_learning.utils import config_gpu\n",
    "config_gpu(gpu_ids=[0], allow_growth=True, log_device_placement=True)\n",
    "\n",
    "from abyss_deep_learning.datasets.simulated import shapes_gen\n",
    "from abyss_deep_learning.keras.classification import batching_gen, onehot_gen\n",
    "from abyss_deep_learning.keras.utils import gen_dump_data, lambda_gen\n",
    "from keras.backend import clear_session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_adaptor(gen, expand_dims=False):\n",
    "    from abyss_deep_learning.utils import instance_to_categorical\n",
    "\n",
    "    for image, name, instances, cats in gen:\n",
    "        row = (\n",
    "            (image.astype(np.float32) - 127.5) / 127.5,\n",
    "            instance_to_categorical(instances, cats, num_classes=4))\n",
    "        if expand_dims:\n",
    "            row  = tuple(np.expand_dims(element, 0) for element in row)\n",
    "        yield row\n",
    "        \n",
    "def unshift_image(image):\n",
    "    return (image * 127.5 + 127.5).astype(np.uint8)\n",
    "\n",
    "def example_image(model=None):\n",
    "    for image, targets in dataset_adaptor(shapes_gen(scale=10, max_shapes=5, nms=0.5, noise=10)):\n",
    "        print(np.min(image), np.max(image))\n",
    "        plt.figure()\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(unshift_image(image))\n",
    "        plt.subplot(1, 2, 2)\n",
    "        if model:\n",
    "            targets = model.predict_proba(image[np.newaxis, ...])[0]\n",
    "        print(np.unique(targets.argmax(-1)))\n",
    "        plt.imshow(targets.argmax(-1))\n",
    "        break\n",
    "example_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abyss_deep_learning.keras.models import FcnCrfSegmenter\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "batch_size = 1 # MUST BE 1 for FcnCrf\n",
    "\n",
    "def create_new_model():\n",
    "    '''Change init_lr if necessary'''\n",
    "    from keras.utils import get_file\n",
    "    from keras_applications.vgg16 import WEIGHTS_PATH_NO_TOP\n",
    "    model = None  # Clear any existing models\n",
    "    clear_session()\n",
    "    model = FcnCrfSegmenter(classes=4, crf_iterations=5, init_lr=5e-5)\n",
    "    weights_path = get_file(\n",
    "                'vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                WEIGHTS_PATH_NO_TOP,\n",
    "                cache_subdir='models',\n",
    "                file_hash='6d6bbae143d832006294945121d1f1fc')\n",
    "    model.set_weights(weights_path)\n",
    "    \n",
    "    callbacks = [\n",
    "        ReduceLROnPlateau(patience=5, factor=0.5, cooldown=5, verbose=1),\n",
    "        EarlyStopping(patience=15, verbose=1, restore_best_weights=True)\n",
    "    ]\n",
    "    return model, callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit: batch method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create the generators that we will pull the data from.\n",
    "gen_train = dataset_adaptor(shapes_gen(scale=10, max_shapes=3, nms=0.3, noise=10), expand_dims=False)\n",
    "gen_val = dataset_adaptor(shapes_gen(scale=10, max_shapes=3, nms=0.5, noise=15), expand_dims=False)\n",
    "# Dump data from the generators\n",
    "x_train, y_train = gen_dump_data(gen_train, 50)\n",
    "validation_data = gen_dump_data(gen_val, 10)\n",
    "model = None\n",
    "model, callbacks = create_new_model()\n",
    "print(\"Random output loss is\", -np.log(1 / model.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_trainable(True)\n",
    "model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=validation_data,\n",
    "    batch_size=batch_size, epochs=100,\n",
    "    callbacks=callbacks)\n",
    "example_image(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should you want to train only parts of the model\n",
    "model.set_trainable('crf')\n",
    "model.recompile()\n",
    "model.set_lr(5e-2) # CRF only\n",
    "model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=validation_data,\n",
    "    batch_size=batch_size, epochs=10,\n",
    "    callbacks=callbacks)\n",
    "example_image(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x_train, y_train, validation_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit: generator method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create the generators that we will pull the data from.\n",
    "# Requires expand_dims=True\n",
    "gen_train = dataset_adaptor(shapes_gen(scale=10, max_shapes=3, nms=0.3, noise=10), expand_dims=True)\n",
    "gen_val = dataset_adaptor(shapes_gen(scale=10, max_shapes=3, nms=0.5, noise=15), expand_dims=True)\n",
    "model = None\n",
    "model, callbacks = create_new_model()\n",
    "\n",
    "print(\"Random output loss is\", -np.log(1 / model.classes))\n",
    "model.fit_generator(\n",
    "    gen_train,\n",
    "    validation_data=gen_val,\n",
    "    steps_per_epoch=50, validation_steps=10,\n",
    "    epochs=100, use_multiprocessing=True,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit: dataset method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abyss_deep_learning.datasets.misc import CachedGenClassificationDataset\n",
    "# Make two datasets (image data, classification task) that dumps data from gen_train and gen_val\n",
    "# and makes it available via the standard abyss Dataset API calls.\n",
    "\n",
    "dataset_train = CachedGenClassificationDataset(gen_train, n_samples=50)\n",
    "dataset_val = CachedGenClassificationDataset(gen_val, n_samples=20)\n",
    "for image, target in dataset_train.generator():\n",
    "    print(image.shape, image.dtype, target.shape, target.dtype)\n",
    "    plt.figure()\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(unshift_image(image[0]))\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(target[0].argmax(-1))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "model, callbacks = create_new_model()\n",
    "print(\"Random output loss is\", -np.log(1 / model.classes))\n",
    "model.fit_dataset(\n",
    "    dataset_train, dataset_val=dataset_val,\n",
    "    steps_per_epoch=50, validation_steps=10,\n",
    "    epochs=100, use_multiprocessing=True,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob1 = model.predict_proba(image)\n",
    "model.save(\"/tmp/abcd\")\n",
    "model = FcnCrfSegmenter.load(\"/tmp/abcd\")\n",
    "prob2 = model.predict_proba(image)\n",
    "plt.figure()\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(prob1[0, ...].argmax(-1)*255)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(prob2[0, ...].argmax(-1)*255)\n",
    "\n",
    "# !rm \"/tmp/abcd\"\n",
    "print(\"Testing serialization: [{}]\".format(np.allclose(prob1, prob2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model_.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = model.model_.get_layer('upsample')\n",
    "layer.weights\n",
    "# plt.figure()\n",
    "\n",
    "# plt.imshow(layer.get_weights()[0]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
