{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from abyss_deep_learning.utils import config_gpu\n",
    "config_gpu(gpu_ids=[0], allow_growth=True, log_device_placement=True)\n",
    "\n",
    "from abyss_deep_learning.keras.classification import batching_gen, onehot_gen\n",
    "from abyss_deep_learning.keras.utils import gen_dump_data, lambda_gen\n",
    "from abyss_deep_learning.datasets.coco import ImageSemanticSegmentationDataset\n",
    "from abyss_deep_learning.datasets.translators import AnnotationTranslator\n",
    "from keras.backend import clear_session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnadarkoTrialTranslator(AnnotationTranslator):\n",
    "    def filter(self, annotation):\n",
    "        return 'category_id' in annotation and annotation['category_id'] == 1\n",
    "    def translate(self, annotation):\n",
    "        return annotation\n",
    "    \n",
    "dataset = ImageSemanticSegmentationDataset(\n",
    "    \"/data/abyss/anadarko/test-run/Test Anadarko/all.json\",\n",
    "    image_dir=\"/data/abyss/anadarko/test-run/Test Anadarko\",\n",
    "    translator=AnadarkoTrialTranslator(),\n",
    "    num_classes=2,\n",
    "    cached=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unshift_image(image):\n",
    "    return (image * 127.5 + 127.5).astype(np.uint8)\n",
    "\n",
    "def example_image(model=None):\n",
    "    image, targets = dataset.sample()\n",
    "    print(image.shape, image.dtype, np.min(image), np.max(image))\n",
    "    plt.figure()\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    if model:\n",
    "        targets = model.predict_proba(image[np.newaxis, ...])[0]\n",
    "    print(np.unique(targets.argmax(-1)))\n",
    "    plt.imshow(targets.argmax(-1))\n",
    "example_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abyss_deep_learning.keras.models import FcnCrfSegmenter\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from abyss_deep_learning.keras.utils import tiling_gen, batching_gen\n",
    "\n",
    "batch_size = 1 # MUST BE 1 for FcnCrf\n",
    "\n",
    "def create_new_model():\n",
    "    '''Change init_lr if necessary'''\n",
    "    from keras.utils import get_file\n",
    "    from keras_applications.vgg16 import WEIGHTS_PATH_NO_TOP\n",
    "    model = None  # Clear any existing models\n",
    "    clear_session()\n",
    "    model = FcnCrfSegmenter(\n",
    "        classes=dataset.num_classes, crf_iterations=5, init_lr=5e-5)\n",
    "    weights_path = get_file(\n",
    "                'vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                WEIGHTS_PATH_NO_TOP,\n",
    "                cache_subdir='models',\n",
    "                file_hash='6d6bbae143d832006294945121d1f1fc')\n",
    "    model.set_weights(weights_path)\n",
    "    \n",
    "    callbacks = [\n",
    "        ReduceLROnPlateau(patience=3, factor=0.5, cooldown=3, verbose=1),\n",
    "        EarlyStopping(patience=10, restore_best_weights=True, verbose=1)\n",
    "    ]\n",
    "    return model, callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit: generator method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, target in dataset.generator(shuffle_ids=True):\n",
    "    print(image.shape, image.dtype, target.shape, target.dtype)\n",
    "    plt.figure()\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(target.argmax(-1))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "model, callbacks = create_new_model()\n",
    "print(\"Random output loss is\", -np.log(1 / model.classes))\n",
    "model.fit_generator(\n",
    "    batching_gen(tiling_gen(dataset.generator(endless=True), (500, 500)), batch_size=1),\n",
    "    steps_per_epoch=50,\n",
    "    epochs=100, use_multiprocessing=False,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob1 = model.predict_proba(image[np.newaxis, :500, :500, :])\n",
    "model.save(\"/tmp/abcd\")\n",
    "model = FcnCrfSegmenter.load(\"/tmp/abcd\")\n",
    "prob2 = model.predict_proba(image[np.newaxis, :500, :500, :])\n",
    "plt.figure()\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(prob1[0, ...].argmax(-1)*255)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(prob2[0, ...].argmax(-1)*255)\n",
    "\n",
    "!rm \"/tmp/abcd\"\n",
    "print(\"Testing serialization: [{}]\".format(np.allclose(prob1, prob2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
