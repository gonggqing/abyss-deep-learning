{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "from imgaug import augmenters as iaa\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from pycocotools.coco import COCO\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "\n",
    "\n",
    "from abyss_deep_learning.keras.classification import ClassificationDataset, FromAnnDataset, caption_map_gen, onehot_gen, augmentation_gen\n",
    "from abyss_deep_learning.keras.utils import batching_gen, lambda_gen\n",
    "import abyss_deep_learning.abyss_dataset as dataset_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Steve's Config\n",
    "# ################ CONFIGURE THIS ########################\n",
    "# # num_classes assumed from caption_map entries\n",
    "# image_dims = (299, 299, 3) # Preset for InceptionV3\n",
    "# batch_size = 5\n",
    "# log_dir = \"/data/log/cnn/cso\"\n",
    "\n",
    "# # maps caption strings to class numbers (ensure minimal set of class numbers)\n",
    "# # eg use {0, 1, 2} not {4, 7, 8}\n",
    "\n",
    "# # Caption type can be either \"single\" or \"multi\".\n",
    "# # This sets up various parameters in the system.\n",
    "# # If conversion between single and multi is required this should be done explicitly and presented\n",
    "# # in a separate json file. The internal representation of all the labels is one-hot encoding.\n",
    "# caption_type = \"single\" \n",
    "# caption_map = {\n",
    "#     'f': 1,\n",
    "#     's': 0\n",
    "# }\n",
    "# coco_train = ClassificationDataset(caption_map, \"/data/abyss/projectmax/cso/dataset_train.json\")\n",
    "# coco_val = ClassificationDataset(caption_map, \"/data/abyss/projectmax/cso/dataset_val.json\")\n",
    "# coco_test = ClassificationDataset(caption_map, \"/data/abyss/projectmax/cso/dataset_test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jamie's Config\n",
    "############### CONFIGURE THIS ########################\n",
    "# num_classes assumed from caption_map entries\n",
    "image_dims = (299, 299, 3) # Preset for InceptionV3\n",
    "batch_size = 2 # Just supporting 1 right now\n",
    "log_dir = \"/mnt/ssd1/processed/industry-data/project-max/ml/front-facing/img-class-keras/no-bg/multi/all-classes/\"\n",
    "\n",
    "# maps caption strings to class numbers (ensure minimal set of class numbers)\n",
    "# eg use {0, 1, 2} not {4, 7, 8}\n",
    "\n",
    "coco_train = FromAnnDataset(\"/mnt/ssd1/processed/industry-data/project-max/ml/front-facing/annotations/training.json\")\n",
    "coco_val = FromAnnDataset(\"/mnt/ssd1/processed/industry-data/project-max/ml/front-facing/annotations/validation.json\")\n",
    "coco_test = FromAnnDataset(\"/mnt/ssd1/processed/industry-data/project-max/ml/front-facing/annotations/validation.json\")\n",
    "\n",
    "caption_type=\"multi\"\n",
    "\n",
    "caption_map_train = {cat['name']: cat['id']-1 for cat in coco_train.dataset['categories']}\n",
    "print(caption_map_train)\n",
    "caption_map_val = {cat['name']: cat['id']-1 for cat in coco_val.dataset['categories']}\n",
    "print(caption_map_val)\n",
    "\n",
    "caption_map = caption_map_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coco_train.dataset['categories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_map_r = {val: key for key, val in caption_map.items()}\n",
    "num_classes = len(caption_map)\n",
    "steps_per_epoch = len(coco_train.imgs) // batch_size\n",
    "steps_per_epoch_val = len(coco_val.imgs) // batch_size\n",
    "print(\"Number of classes:\", num_classes)\n",
    "print(\"Steps per epoch:\", steps_per_epoch)\n",
    "print(\"Steps per steps_per_epoch_val:\", steps_per_epoch_val)\n",
    "caption_map_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image, caption):\n",
    "    image = resize(image, image_dims, preserve_range=True)\n",
    "    return preprocess_input(image.astype(np.float32)), caption\n",
    "\n",
    "def postprocess(image):\n",
    "    return ((image + 1) * 127).astype(np.uint8)\n",
    "\n",
    "def pipeline(gen, aug_config=None, from_captions=True):\n",
    "    base_gen = caption_map_gen(gen, caption_map) if from_captions else gen\n",
    "    return (\n",
    "        augmentation_gen(\n",
    "            onehot_gen(\n",
    "                lambda_gen(\n",
    "                    base_gen\n",
    "                , func=preprocess)\n",
    "            , num_classes=num_classes)\n",
    "        , aug_config, enable=(aug_config is not None))\n",
    "    )\n",
    "        \n",
    "aug_config = {\n",
    "    'flip_lr_percentage': 0.5,\n",
    "    'flip_ud_percentage': 0.5,\n",
    "    'affine': {\n",
    "        \"order\": 1,\n",
    "        'scale': {\n",
    "            \"x\": (0.8, 1.2),\n",
    "            \"y\": (0.8, 1.2)\n",
    "        },\n",
    "        \"rotate\": (-10, 10),\n",
    "        \"shear\": (-5, 5),\n",
    "        \"mode\": 'constant'\n",
    "    },\n",
    "#     'color': {\n",
    "#         'probability': 1.00,\n",
    "#         'hue': (0, 0),\n",
    "#         'saturation': (0, 0),\n",
    "#         'value': (0, 0)\n",
    "#     }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Steve's\n",
    "train_gen = pipeline(\n",
    "    coco_train.generator(shuffle_ids=True),\n",
    "    aug_config=aug_config)\n",
    "val_gen = pipeline(coco_val.generator(shuffle_ids=True))\n",
    "test_gen = pipeline(coco_test.generator(shuffle_ids=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jamie's\n",
    "train_gen = pipeline(\n",
    "    coco_train.generator(shuffle_ids=True),\n",
    "    aug_config=aug_config,from_captions=False)\n",
    "val_gen = pipeline(coco_val.generator(shuffle_ids=True),from_captions=False)\n",
    "test_gen = pipeline(coco_test.generator(shuffle_ids=True),from_captions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k,l = next(train_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = coco_val\n",
    "# imgIds = [ann['image_id'] for ann in ds.loadAnns(ids=ds.getAnnIds())]\n",
    "# for image_id in set(imgIds):\n",
    "#     caps = [annotation['category_id']-5\n",
    "#             for annotation in ds.loadAnns(ds.getAnnIds([image_id]))\n",
    "#             ]\n",
    "#     print(image_id,set(caps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (train, val, test) in enumerate(zip(train_gen, val_gen, test_gen)):\n",
    "    print(train[0].shape, train[1])\n",
    "    print(val[0].shape, val[1])\n",
    "    print(test[0].shape, test[1])\n",
    "    plt.figure()\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(postprocess(train[0]))\n",
    "    plt.title(', '.join([caption_map_r[int(cap_id)] for cap_id in np.argwhere(train[1])]))\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(postprocess(val[0]))\n",
    "    plt.title(', '.join([caption_map_r[int(cap_id)] for cap_id in np.argwhere(val[1])]))\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(postprocess(test[0]))\n",
    "    plt.title(', '.join([caption_map_r[int(cap_id)] for cap_id in np.argwhere(test[1])]))\n",
    "    \n",
    "    if i >= 0:\n",
    "        break\n",
    "print(\"Left to right: ground truth samples from train, val test\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dump_data(gen, num_images):\n",
    "    data = [[],[]]\n",
    "    for i, (image, caption) in enumerate(gen):\n",
    "        data[0].append(image)\n",
    "        data[1].append(caption)\n",
    "        if i >= num_images:\n",
    "            break\n",
    "    data = (\n",
    "        np.concatenate([i[np.newaxis, ...] for i in data[0]], axis=0),\n",
    "        np.concatenate([i[np.newaxis, ...] for i in data[1]], axis=0)\n",
    "    )\n",
    "    return data\n",
    "\n",
    "def set_to_onehot(captions):\n",
    "    return np.array([1 if i in captions else 0 for i in range(num_classes)])\n",
    "\n",
    "def count_labels_multi(label_set_list):\n",
    "    data = []\n",
    "    for label in label_set_list:\n",
    "        data.append(set_to_onehot(label))\n",
    "    return Counter([int(j) for i in data for j in np.argwhere(i)])\n",
    "\n",
    "def num_image_wo_labels(cocodata):\n",
    "    image_with_labels = len(np.unique([ann['image_id'] for ann in cocodata.loadAnns(cocodata.getAnnIds())]))\n",
    "    total_images = len(cocodata.imgs)\n",
    "    return total_images - image_with_labels\n",
    "\n",
    "def category_list(cocodata):\n",
    "    imgIds = []\n",
    "    label_categories = []\n",
    "    for imgkey in cocodata.imgs:\n",
    "        img_id = cocodata.imgs[imgkey]['id']\n",
    "        categories = cocodata.load_categories(img_id)\n",
    "        imgIds.append(img_id)\n",
    "        label_categories.append(categories)\n",
    "    return imgIds, label_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustration of problem with num ids and imgs\n",
    "imgIds = [ann['image_id'] for ann in coco_train.loadAnns(coco_train.getAnnIds())]\n",
    "print(len(imgIds), len(np.unique(imgIds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_image_wo_labels(coco_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgIds_train, label_categories_train = category_list(coco_train)\n",
    "train_counts = count_labels_multi(label_categories_train)\n",
    "train_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights =  1 / np.array([j for i, j in sorted(train_counts.items(), key=lambda x: x[0])], dtype=np.float32)\n",
    "class_weights /= np.linalg.norm(class_weights)\n",
    "class_weights = dict(zip(sorted(train_counts.keys()), class_weights.tolist()))\n",
    "print(\"class_weights:\")\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = gen_dump_data(val_gen, coco_val.num_images())\n",
    "test_data = gen_dump_data(test_gen, coco_test.num_images())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fault_of_interest = 0\n",
    "val_data[1][:,fault_of_interest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_head(base_model, num_classes, caption_type, train_features=False, opt_params={}):\n",
    "    '''make sure base_model has include_top=False'''\n",
    "    from keras.layers import Dense, MaxPooling2D, Dropout, Flatten\n",
    "    from keras.models import Model\n",
    "    \n",
    "    if not opt_params:\n",
    "        opt_params = {\"optimizer\": \"Nadam\"}\n",
    "    opt_params['loss'] = \"categorical_crossentropy\" if caption_type == \"single\" else \"binary_crossentropy\"\n",
    "    activation = \"softmax\" if caption_type == \"single\" else \"sigmoid\"\n",
    "    \n",
    "    # add a global spatial average pooling layer\n",
    "    x = base_model.output\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    predictions = Dense(num_classes, activation=activation, name='class_logits')(x)\n",
    "\n",
    "\n",
    "    # this is the model we will train\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "#     model = multi_gpu_model(model, gpus=2)\n",
    "    # first: train only the top layers (which were randomly initialized)\n",
    "    # i.e. freeze all convolutional InceptionV3 layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = train_features\n",
    "    # compile the model (should be done *after* setting layers to non-trainable)\n",
    "    model.compile(**opt_params, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Note:\n",
    "###    When single-label training a 'softmax' activation and 'categorical_crossentropy' loss is used\n",
    "###    When multi-label training a 'sigmoid' activation and 'binary_crossentropy' loss is used\n",
    "\n",
    "K.clear_session()\n",
    "model = create_new_head(\n",
    "    InceptionV3(\n",
    "        include_top=False, weights='imagenet', input_shape=image_dims),\n",
    "    num_classes, caption_type, train_features=False,\n",
    "    opt_params={'optimizer': \"Nadam\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_directory(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load model weights if needed\n",
    "log_dir = \"/mnt/ssd1/processed/industry-data/project-max/ml/front-facing/img-class-keras/no-bg/multi/suchet-trial/\"\n",
    "# model.load_weights(os.path.join(log_dir, \"run-1-interrupted/models/best.067-0.4350.h5\"))\n",
    "# model.load_weights(\"/data/log/cnn/cso/models/best.036-0.1494.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_path = os.path.join(log_dir, \"models/best.{epoch:03d}-{val_loss:.4f}.h5\")\n",
    "setup_directory(os.path.dirname(best_path))\n",
    "\n",
    "callbacks=[\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=9, cooldown=6, verbose=1),\n",
    "        ModelCheckpoint(\n",
    "            best_path, monitor='val_loss', verbose=1,\n",
    "            save_best_only=True, save_weights_only=True, mode='auto', period=1),\n",
    "        TensorBoard(\n",
    "            log_dir=log_dir,\n",
    "            histogram_freq=0,\n",
    "            batch_size=batch_size,\n",
    "            write_graph=False,\n",
    "            write_grads=False,\n",
    "            write_images=False),\n",
    "#         EarlyStopping(\n",
    "#             monitor='val_loss', min_delta=0.0, patience=40, verbose=1, mode='auto')\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with initial LR\n",
    "learning_rate = 1e-5\n",
    "K.set_value(model.optimizer.lr, learning_rate)\n",
    "train_history = model.fit_generator(\n",
    "    batching_gen(train_gen, batch_size=batch_size),\n",
    "    validation_data=tuple(val_data),\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=steps_per_epoch_val,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks, \n",
    "    epochs=100,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_label_decision(y_true, y_pred, thresh=0.5):\n",
    "    return (y_true > thresh) == (y_pred > thresh)\n",
    "def single_label_decision(y_true, y_pred):\n",
    "    return np.argmax(y_true, axis=-1) == np.argmax(y_pred, axis=-1)\n",
    "\n",
    "decision_function = single_label_decision if caption_type == 'single' else multi_label_decision\n",
    "thresh = 0.5 # Used for multi-label decisions\n",
    "\n",
    "Y_true = test_data[1]\n",
    "Y_pred = model.predict(test_data[0])\n",
    "TP = decision_function(Y_true, Y_pred)\n",
    "print(\"Test accuracy for {:d} samples: {:.2f}\".format(len(test_data[0]), np.count_nonzero(TP) / TP.size))\n",
    "for i, (image, true_caption, pred_caption) in enumerate(zip(test_data[0], test_data[1], Y_pred)):\n",
    "    if i % 4 == 0:\n",
    "        if i > 0:\n",
    "            plt.tight_layout()\n",
    "        if i >= 4:\n",
    "            break\n",
    "        if i < len(test_data[0]):\n",
    "            plt.figure()\n",
    "    plt.subplot(2, 2, 1 + (i % 4))\n",
    "    plt.imshow(postprocess(image))\n",
    "    plt.title(\"T: {:s}; P: {:s}\".format(\n",
    "        ', '.join([caption_map_r[int(cap_id)] for cap_id in np.argwhere(true_caption > thresh)]),\n",
    "        ', '.join([caption_map_r[int(cap_id)] for cap_id in np.argwhere(pred_caption > thresh)])\n",
    "    ))\n",
    "test_metrics = model.evaluate(test_data[0], test_data[1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model (pretty important!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, name, class_map_r, prediction_type,\n",
    "               model_weights_path, model_def_path, model_info_path,\n",
    "               test_metrics=None, description=\"\"):\n",
    "    def merged(a, b):\n",
    "        merged = dict(a)\n",
    "        merged.update(b)\n",
    "        return merged\n",
    "        \n",
    "    # Encountered JSON TypeError when parsing np.float32\n",
    "    # Works for np.float64 or float\n",
    "    json_allowable_history = train_history.history\n",
    "    json_allowable_history['lr'] = [float(lr) for lr in train_history.history['lr']]\n",
    "    model_info = {\n",
    "        \"name\": name,\n",
    "        \"description\": description,\n",
    "        \"weights\": model_weights_path,\n",
    "        \"prediction_type\": caption_type,\n",
    "        \"model\": model_def_path,\n",
    "        \"classes\": class_map_r,\n",
    "        \"architecture\": {\n",
    "            \"backbone\": \"inceptionv3\",\n",
    "            \"logit_activation\": model.get_layer(\"class_logits\").activation.__name__,\n",
    "            \"input_shape\": image_dims\n",
    "        },\n",
    "        \"metrics\": {\n",
    "            \"loss_function\": train_history.model.loss,\n",
    "            \"train\": merged(\n",
    "                train_history.history,\n",
    "                {\n",
    "                    \"epoch\": train_history.epoch,\n",
    "                    \"params\": train_history.params\n",
    "                })\n",
    "        }\n",
    "    }\n",
    "    if test_metrics:\n",
    "        model_info['metrics']['test'] = test_metrics\n",
    "    \n",
    "    print(\"Writing model def to \" + model_def_path)\n",
    "    with open(model_def_path, \"w\") as file:\n",
    "        file.write(model.to_json())\n",
    "        \n",
    "    print(\"Writing model weights to \" + model_weights_path)\n",
    "    model.save_weights(model_weights_path)\n",
    "    \n",
    "    print(\"Writing model info to \" + model_info_path)\n",
    "    with open(model_info_path, \"w\") as file:\n",
    "        file.write(json.dumps(model_info))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = [r for r in train_history.history['lr']]\n",
    "# print(json.dumps(x[0].astype(float)))\n",
    "# print(json.dumps(x[0].astype(np.float64)))\n",
    "# x[0]\n",
    "# type(x[0])\n",
    "# type(1e-4)\n",
    "# json.dumps([1e-04,1e-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the relevant params below \n",
    "# log_dir = \"/mnt/ssd1/processed/industry-data/project-max/ml/front-facing/img-class-keras/no-bg/single/run-1-restarted/\"\n",
    "model_def_path = os.path.join(log_dir, \"model_def.json\")\n",
    "model_weights_path = os.path.join(log_dir, \"model_weights.h5\")\n",
    "model_info_path = os.path.join(log_dir, \"model.json\")\n",
    "save_model(\n",
    "    model, name=\"all-classes-no-bg\",\n",
    "    class_map_r=caption_map_r, prediction_type=caption_type,\n",
    "    model_weights_path=model_weights_path, model_def_path=model_def_path, model_info_path=model_info_path,\n",
    "    test_metrics=test_metrics,\n",
    "    description=\"All classes no background class\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
