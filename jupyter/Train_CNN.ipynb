{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings\n",
    "  * general\n",
    "    * batch size should be at least 1x(number of classes) in order to take advantage of MTL\n",
    "    * Always start training a model with dropout=ker_reg=act_reg=0, look at bias and variance then add until good fit\n",
    "    * No pooling gives very fast results but strong overfitting and large model size\n",
    "    * Don't worry about class weights unless heavily (20x or more) imbalanced\n",
    "    * head:0x** works best, any more layers underfits\n",
    "    * training with dropout makes val_loss very noisy and auto saving doesn't work well, enable regular saving\n",
    "    * Don't use both Dropout and Batch Norm together (if you do use very small dropout, https://arxiv.org/pdf/1801.05134.pdf)\n",
    "    \n",
    "  * multi-label output\n",
    "    * sigmoid output makes model very sensitive to learning_rate.\n",
    "      * I have found with VGG16 around 5e-5 is a good start\n",
    "      * Use eg setup_callbacks(hist=2, grads=True) to enable gradient outputs; check if class_logits_out is becoming spread between 0 and 1, check that gradients are not 0 (should be around 1e-3).\n",
    "  * single-label output\n",
    "    * Pretty stable with any architecture\n",
    "    \n",
    "\n",
    "## Bugs\n",
    "  * There may be a GPU memory leak somewhere... keras does not recycle models properly. I'll try to find this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import cycle\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda/lib64:' + os.environ['LD_LIBRARY_PATH']\n",
    "from abyss_deep_learning.utils import config_gpu\n",
    "\n",
    "\n",
    "from imgaug import augmenters as iaa\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.resnet50 import preprocess_input, ResNet50\n",
    "from keras.applications.vgg16 import VGG16\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, TensorBoard, EarlyStopping, LearningRateScheduler, Callback, TerminateOnNaN\n",
    "from keras.layers import (\n",
    "    Flatten, GlobalAveragePooling2D, GlobalMaxPooling2D, Input, Dense, Dropout, Activation)\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "import keras.layers as layers\n",
    "import keras.initializers\n",
    "from abyss_deep_learning.keras.activations import Hexpo\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.regularizers import l1_l2\n",
    "from keras.metrics import binary_accuracy, categorical_accuracy\n",
    "from pycocotools.coco import COCO\n",
    "from skimage.transform import resize\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "import keras.initializers\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "tf = K.tf\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from abyss_deep_learning.keras.classification import set_to_multihot\n",
    "\n",
    "from abyss_deep_learning.utils import balanced_set\n",
    "from abyss_deep_learning.keras.classification import (\n",
    "    ClassificationDataset, PRTensorBoard, Inference,\n",
    "    caption_map_gen, multihot_gen, augmentation_gen, skip_bg_gen, cached_gen)\n",
    "from abyss_deep_learning.keras.utils import (\n",
    "    batching_gen, lambda_gen, calc_class_weights, count_labels_multi, count_labels_single, gen_dump_data)\n",
    "import abyss_deep_learning.abyss_dataset as dataset_model\n",
    "\n",
    "config_gpu([1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from abyss_deep_learning.utils import instance_to_caption\n",
    "# for file in [\n",
    "#             os.path.join(database_dir, dataset_name, \"train.json\"),\n",
    "#             os.path.join(database_dir, dataset_name, \"val.json\"),\n",
    "#         ]:\n",
    "#     db = instance_to_caption(json.load(open(file,\"r\")))\n",
    "#     with open(file + \".2\",\"w\") as f:\n",
    "#         json.dump(db, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### CONFIGURE ALL VARIABLES IN THIS CELL ########################\n",
    "# num_classes assumed from caption_map entries\n",
    "image_dims = (480, 640, 3) # What to resize images to before CNN\n",
    "NN_DTYPE = np.float32 # Pretrained networks are in float32\n",
    "\n",
    "# Caption type can be either \"single\" or \"multi\". This sets up various other parameters in the system.\n",
    "caption_type = \"multi\" \n",
    "use_balanced_set = False\n",
    "use_cached = True\n",
    "aug_config = None # Uncomment bottom of cell to add augmentation\n",
    "use_class_weights = True\n",
    "\n",
    "ignore_sometimes = ['F', 'SJ', 'SO', 'IP', 'ED']\n",
    "ignore_captions = set(['JO', 'RI', 'U', 'X', 'C'] + ignore_sometimes)\n",
    "captions = set([\"C\", \"ED\", \"F\", \"IP\", \"JD\", \"SJ\", \"SO\", \"U\", \"X\"]) - ignore_captions\n",
    "\n",
    "\n",
    "# aug_config = {\n",
    "#     'flip_lr_percentage': 0.5,\n",
    "#     'flip_ud_percentage': 0.5,\n",
    "#     'affine': {\n",
    "#         \"order\": 1,\n",
    "#         'scale': {\n",
    "#             \"x\": (0.8, 1.2),\n",
    "#             \"y\": (0.8, 1.2)\n",
    "#         },\n",
    "#         \"rotate\": (-10, 10),\n",
    "#         \"shear\": (-5, 5),\n",
    "#         \"mode\": 'constant'\n",
    "#     },\n",
    "#     'color': {\n",
    "#         'probability': 0.9,\n",
    "#         'hue': (-25, 25),\n",
    "#         'saturation': (25, 25),\n",
    "#         'value': (-25, -25)\n",
    "#     }\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_map = {k: v for v, k in enumerate(sorted(captions))}\n",
    "print(caption_map)\n",
    "caption_map_r = {val: key for key, val in caption_map.items()}\n",
    "num_classes = len(caption_map)\n",
    "print(\"Number of classes:\", num_classes)\n",
    "\n",
    "# Import or define the right translator\n",
    "from abyss_deep_learning.datasets.translators import AbyssCaptionTranslator, CloudFactoryCaptionTranslator\n",
    "translator = AbyssCaptionTranslator()\n",
    "\n",
    "database_dir = \"/data/abyss/projectmax/feature-detection/large-fromCF\"\n",
    "dataset_name = \"alltogether/unique\"\n",
    "'/data/ab'\n",
    "dataset_files = {\n",
    "    'train': os.path.join(database_dir, \"{:s}/train.json\".format(dataset_name)),\n",
    "    'val': os.path.join(database_dir, \"{:s}/val.json\".format(dataset_name)),\n",
    "    'test': os.path.join(database_dir, \"{:s}/test.json\".format(dataset_name))\n",
    "}\n",
    "dataset_name = dataset_name.replace(\"/\", \"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup pre/post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONFIG THIS\n",
    "def augmentation_gen(gen, aug_config, enable=True):\n",
    "    '''\n",
    "    Data augmentation for classification task.\n",
    "    Target is untouched.\n",
    "    '''\n",
    "    if not enable:\n",
    "        while True:\n",
    "            yield from gen\n",
    "    aug_list = []\n",
    "    if 'flip_lr_percentage' in aug_config:\n",
    "        aug_list += [iaa.Fliplr(aug_config['flip_lr_percentage'])]\n",
    "    if 'flip_ud_percentage' in aug_config:\n",
    "        aug_list += [iaa.Flipud(aug_config['flip_ud_percentage'])]\n",
    "    if 'affine' in aug_config:\n",
    "        aug_list += [iaa.Affine(**aug_config['affine'])]\n",
    "    if 'color' in aug_config:\n",
    "        aug_list += [\n",
    "            iaa.Sometimes(\n",
    "                aug_config['color']['probability'],\n",
    "                iaa.Sequential([\n",
    "                    iaa.ChangeColorspace(from_colorspace=\"RGB\", to_colorspace=\"HSV\"),\n",
    "                    iaa.WithChannels(0, iaa.Add(aug_config['color']['hue'])),\n",
    "                    iaa.WithChannels(1, iaa.Add(aug_config['color']['saturation'])),\n",
    "                    iaa.WithChannels(2, iaa.Add(aug_config['color']['value'])),\n",
    "                    iaa.ChangeColorspace(from_colorspace=\"HSV\", to_colorspace=\"RGB\")\n",
    "        ]))]\n",
    "    if 'custom' in aug_config:\n",
    "        aug_list += aug_config['custom']\n",
    "    seq = iaa.Sequential(aug_list)\n",
    "    for image, target in gen:\n",
    "        yield seq.augment_image(image), target\n",
    "        \n",
    "def preprocess(image, caption):\n",
    "    image = resize(image, image_dims, preserve_range=True, mode='constant')\n",
    "    return preprocess_input(image.astype(NN_DTYPE), mode='tf'), caption\n",
    "\n",
    "def postprocess(image):\n",
    "    return ((image + 1) * 127.5).astype(np.uint8)\n",
    "     \n",
    "def pipeline(gen, num_classes, aug_config=None):\n",
    "    return (\n",
    "        lambda_gen(\n",
    "            augmentation_gen(\n",
    "                skip_bg_gen(\n",
    "                    multihot_gen(\n",
    "                        caption_map_gen(gen, caption_map)\n",
    "                    , num_classes=num_classes)\n",
    "                )\n",
    "            , aug_config, enable=(aug_config is not None))\n",
    "        , func=preprocess)\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "dataset = {\n",
    "    'names': list(dataset_files.keys()),\n",
    "    'classes': sorted(list(caption_map.values())),\n",
    "    'class_weights': {},\n",
    "    'ids' : {},\n",
    "    'gens': {},\n",
    "    'data': {},\n",
    "    'coco': {}\n",
    "}\n",
    "print(\"Combinations of labels present\")\n",
    "\n",
    "\n",
    "for name, path in dataset_files.items():\n",
    "    coco = ClassificationDataset(caption_map, translator, path)\n",
    "    ids = list(balanced_set(coco)) if use_balanced_set else [img['id'] for img in coco.imgs.values()]\n",
    "#     if name in ['test']:\n",
    "#         ids = ids[::4] #TODO: REMOVE AFTER TESTING\n",
    "#     if name in ['train']:\n",
    "#         ids = ids[::2] #TODO: REMOVE AFTER TESTING\n",
    "    gen = pipeline(\n",
    "        coco.generator(imgIds=ids, shuffle_ids=True),\n",
    "        num_classes, aug_config if name == 'train' else None)\n",
    "    print(\"{:s}: {:d} images\".format(name, len(ids)))\n",
    "    print(set([tuple(coco.load_caption(image['id'])) for image in coco.imgs.values() if image['id'] in ids]))\n",
    "    \n",
    "    print(\"{:s} set size {:d}\".format(name, len(ids)))\n",
    "    \n",
    "    dataset['coco'][name] = coco\n",
    "    dataset['ids'][name] = ids\n",
    "    dataset['gens'][name] = gen\n",
    "    if use_cached:\n",
    "        expected_size = 4 * len(ids) * np.product(image_dims)\n",
    "        print(\"Caching {:s} set will take {:.1f} GB\".format(name, expected_size / 1024 ** 3))\n",
    "        dataset['data'][name] = [\n",
    "            np.zeros((len(ids),) + image_dims, dtype=np.float32),\n",
    "            np.zeros((len(ids), num_classes), dtype=np.float32)]\n",
    "        \n",
    "#         dataset['data'][name] = gen_dump_data(gen, len(ids))\n",
    "        def procedure(a):\n",
    "            idx, img_id = a\n",
    "            image, caption = preprocess(coco.load_image(img_id), coco.load_caption(img_id))\n",
    "            caption = set_to_multihot({caption_map[i] for i in caption if i in captions}, num_classes)\n",
    "            return idx, img_id, image, caption\n",
    "        \n",
    "        with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "            for idx, img_id, image, caption in executor.map(procedure, enumerate(ids)):\n",
    "                dataset['data'][name][0][idx] = image\n",
    "                dataset['data'][name][1][idx] = np.array(caption)\n",
    "        print(\"Dataset {:s} has {:d} classes\".format(name, np.count_nonzero(dataset['data'][name][1].sum(axis=0))))\n",
    "    \n",
    "\n",
    "def data_source(name):\n",
    "    if dataset['data'] and name in dataset['data']:\n",
    "        return dataset['data'][name]\n",
    "    return dataset['gens'][name]\n",
    "\n",
    "def data_sample(name, size=None):\n",
    "    source = data_source(name)\n",
    "    if isinstance(source, list):\n",
    "        if dataset['data'][name]:\n",
    "            idx = np.random.choice(np.arange(dataset['data'][name][0].shape[0]), size=size)\n",
    "            images = dataset['data'][name][0][idx]\n",
    "            labels = dataset['data'][name][1][idx]\n",
    "#             if size == None:\n",
    "#                 images, labels = images[0], labels[0]\n",
    "            return images, labels\n",
    "    for image, label in source:\n",
    "        return image, label\n",
    "# print(np.unique(\n",
    "#     [i \n",
    "#      for image in coco_train.imgs.values()\n",
    "#     for i in coco_train.load_caption(image['id'])]\n",
    "# ))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit -n1 -r1\n",
    "image, target = data_sample('train', 10)\n",
    "if np.sum(target) == 0:\n",
    "    print(\"BG\")\n",
    "else:\n",
    "    print(image.shape)\n",
    "    print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "num_rows = 3\n",
    "print(\"Left to right: ground truth samples from \", end='')\n",
    "for j in range(num_rows):\n",
    "    for i, name in enumerate(dataset['names']):\n",
    "        plt.subplot(num_rows, 3, 3 * j + i + 1)\n",
    "    #     print(data[0].shape, data[1], (np.min(data[0]), np.max(data[0])))\n",
    "        image, label = data_sample(name, None)\n",
    "        print(image.dtype, image.shape)\n",
    "        plt.imshow(postprocess(image))\n",
    "        plt.title(', '.join([caption_map_r[int(cap_id)] for cap_id in np.argwhere(label)]))\n",
    "        print(name, end=', ')\n",
    "        plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell intentionally left blank due to display bug above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "def label_encoding(y, mode):\n",
    "    if mode == 'multihot':\n",
    "        return np.argwhere(y)[:, 1]\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "    #   return np.sum(y * (2 ** np.arange(y.shape[1])[::-1]), axis=1)\n",
    "\n",
    "def noop(args, **kwargs):\n",
    "    return args\n",
    "    \n",
    "if use_class_weights:\n",
    "    for name, coco in dataset['coco'].items():\n",
    "        print(\"{:s} {:s} class stats {:s}\".format('=' * 8, name, '=' * 8))\n",
    "        labels = [coco.load_caption(image['id'])\n",
    "                  for image in coco.imgs.values() if image['id'] in dataset['ids'][name]]\n",
    "        y = [caption_map[l]\n",
    "            for fields in labels for l in fields if l in captions]\n",
    "#         print(y)\n",
    "        count = np.array(list(dict(sorted(Counter(y).items(), key=lambda x: x[0])).values()))\n",
    "        spread = {i: float(v.round(2)) for i, v in enumerate(count / np.sum(count))}\n",
    "        class_weights = compute_class_weight('balanced', dataset['classes'], y)\n",
    "        class_weights = {i: float(np.round(v, 3)) for i, v in enumerate(class_weights)}\n",
    "        dataset['class_weights'][name] = class_weights\n",
    "        a = np.array(list(dataset['class_weights'][name].values()))\n",
    "        \n",
    "        print(\"class weights:\".format(name))\n",
    "        print(\" \", class_weights)\n",
    "        trivial = np.mean(a / np.max(a))\n",
    "        print(\"trivial result accuracy:\\n  {:.2f} or {:.2f}\".format(trivial, 1-trivial))\n",
    "        print(\"class cover fractions:\\n  \", spread )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_model_regularization(model, kernel_regularizer_l2, activity_regularizer_l1):\n",
    "#     # Add L2 Regularization\n",
    "#     # Skip gamma and beta weights of batch normalization layers.\n",
    "#     if kernel_regularizer_l2:\n",
    "#         reg_losses = [\n",
    "#             keras.regularizers.l2(kernel_regularizer_l2)(w) / tf.cast(tf.size(w), tf.float32)\n",
    "#             for w in model.trainable_weights\n",
    "#             if not any([l in w.name for l in ['gamma', 'beta']])]\n",
    "#         model.add_loss(tf.add_n(reg_losses, name='l2_reg_loss'))\n",
    "#     if activity_regularizer_l1:\n",
    "#         reg_losses = [\n",
    "#             keras.regularizers.l1(activity_regularizer_l1)(\n",
    "#                 layer.get_output_at(0)) / tf.cast(tf.size(layer.get_output_at(0)), tf.float32)\n",
    "#             for layer in model.layers\n",
    "#             if layer.trainable and not any([l in layer.name for l in ['class_logits', 'batch_norm']])]\n",
    "#         model.add_loss(tf.add_n(reg_losses, name='l1_reg_loss'))\n",
    "    for layer in model.layers: #Save \n",
    "        if not layer.trainable or 'batch_norm' in layer.name or 'logits' in layer.name:\n",
    "            continue\n",
    "        if hasattr(layer, 'kernel_regularizer') and kernel_regularizer_l2:\n",
    "            if 'kernel' in layer.weights[0].name:\n",
    "                size = np.product(layer.weights[0].shape.as_list())\n",
    "                if size:\n",
    "                    layer.kernel_regularizer = l1_l2(0, kernel_regularizer_l2 / size)\n",
    "        if hasattr(layer, 'activity_regularizer') and activity_regularizer_l1:\n",
    "            size = np.product(layer.get_output_shape_at(0)[1:])\n",
    "            if size:\n",
    "                layer.activity_regularizer = l1_l2(activity_regularizer_l1 / size, 0)\n",
    "            \n",
    "    # Suspect this is where GPU memory leak is coming from\n",
    "    model_config = model.get_config()\n",
    "    model_weights = model.get_weights()\n",
    "    model = None\n",
    "    K.clear_session()\n",
    "    model = Model.from_config(model_config)\n",
    "    model.set_weights(model_weights)\n",
    "    return model\n",
    "    \n",
    "def create_new_head(base_model, model_params, add_reg=False):\n",
    "    '''make sure base_model has include_top=False. If loss=None then it is determined.'''\n",
    "    if model_params.activation == None:\n",
    "        if model_params.caption_type == \"single\":\n",
    "            model_params.activation = \"softmax\" \n",
    "        else:\n",
    "            model_params.activation = \"sigmoid\"\n",
    "\n",
    "    x = base_model.output\n",
    "    if model_params.pool == 'avg':\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "    elif model_params.pool == 'max':\n",
    "        x = GlobalMaxPooling2D()(x)\n",
    "    else:\n",
    "        x = Flatten()(x)\n",
    "\n",
    "#     x = BatchNormalization()(x)\n",
    "    if model_params.dropout:\n",
    "        x = Dropout(model_params.dropout)(x)\n",
    "\n",
    "    for i in range(model_params.num_hidden_layers):\n",
    "        x = Dense(model_params.num_hidden_neurons)(x)\n",
    "#         x = BatchNormalization()(x)\n",
    "#         x = Activation('relu')(x)\n",
    "        x = PReLU()(x) # layer_depth=(4+i)\n",
    "        if model_params.dropout:\n",
    "            x = Dropout(model_params.dropout)(x)\n",
    "\n",
    "    predictions = Dense(\n",
    "        model_params.num_classes,\n",
    "        activation=model_params.activation,\n",
    "        kernel_initializer='uniform',\n",
    "#         bias_initializer=keras.initializers.he_uniform(),\n",
    "        name='class_logits')(x)\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = model_params.train_features\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    if add_reg:\n",
    "        model = add_model_regularization(\n",
    "            model, \n",
    "            model_params.kernel_regularizer_l2,\n",
    "            model_params.activity_regularizer_l1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection and training\n",
    "You may have to change these callbacks to suit the dataset and model\n",
    "Note that calculating gradients and histogram on large layered networks (resnet and inception) takes a long time (5 minutes per epoch calculated) so you may only want to do this infrequently or not at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_model_factory(depth, activity_reg_l1=0, kernel_reg_l2=0):\n",
    "    model_name = 'simple-{:d}'.format(depth)\n",
    "    def function(include_top=False, weights=None, input_shape=None):\n",
    "        def unit(x, filters, conv=3, stride=1, layer_depth=None):\n",
    "            layer_no = str(layer_depth if layer_depth is not None else '')\n",
    "            for i, s in enumerate([stride, stride, stride + 1]):\n",
    "                x = layers.Conv2D(\n",
    "                    filters, (conv, conv),\n",
    "                    strides=(s, s),\n",
    "                    padding='same',\n",
    "                    kernel_initializer='he_uniform',\n",
    "                    kernel_regularizer=l1_l2(0, kernel_reg_l2),\n",
    "                    name='layer{:s}_conv{:d}'.format(layer_no, i))(x)\n",
    "#                 x = layers.BatchNormalization(\n",
    "#                     name='layer{:s}_batch_norm{:d}'.format(layer_no, i))(x)\n",
    "                if activity_reg_l1:\n",
    "                    size = np.product(x.shape.as_list()[1:])\n",
    "                    reg = {\"activity_regularizer\": l1_l2(activity_reg_l1 / size, 0)}\n",
    "#                 act = PReLU(shared_axes=[1, 2])\n",
    "                act = Hexpo(layer_depth, shared_axes=[1, 2]) #layers.Activation('relu')\n",
    "                if activity_reg_l1:\n",
    "                    size = np.product(x.shape.as_list()[1:])\n",
    "                    act.activity_regularizer = l1_l2((activity_reg_l1 / size), 0)\n",
    "                x = act(x)\n",
    "            return x\n",
    "\n",
    "        img_input = layers.Input(shape=input_shape)\n",
    "        x = img_input\n",
    "        for i in range(depth):\n",
    "            x = unit(x, 2 ** (6 + i), conv=3, stride=1, layer_depth=(i))\n",
    "        return Model(img_input, x, name=model_name)\n",
    "    setattr(function, '__name__', model_name)\n",
    "    return function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify this class to change search parameters\n",
    "class Experiment(object):\n",
    "    def __init__(self, data_name, input_shape):\n",
    "        self.callbacks = None\n",
    "        self.history = dict()\n",
    "        self.data_name = data_name\n",
    "        self.num_classes = num_classes\n",
    "        self.caption_type = caption_type\n",
    "        self.model = None\n",
    "        self.model2 = None\n",
    "        self.input_shape = input_shape\n",
    "        self.batch_size = num_classes\n",
    "        self.id = str(np.random.randint(0, 9999))\n",
    "        if self.caption_type == 'single':\n",
    "            self.activation = 'softmax'\n",
    "            self.loss = 'categorical_crossentropy'\n",
    "        else:\n",
    "            self.activation = 'sigmoid'\n",
    "            self.loss = 'binary_crossentropy'\n",
    "            \n",
    "        self.feature_extractor = VGG16\n",
    "        self.model_name = self.feature_extractor.__name__\n",
    "        self.learning_rate = 5e-5\n",
    "        self.dropout = None\n",
    "        self.train_features = True\n",
    "        self.pool = 'avg'\n",
    "        self.num_hidden_layers = 0\n",
    "        self.num_hidden_neurons = 0\n",
    "        self.pretrained_weights = 'imagenet'\n",
    "        self.class_weights = None\n",
    "        self.kernel_regularizer_l2 = None\n",
    "        self.activity_regularizer_l1 = None\n",
    "\n",
    "    def random_init(self):\n",
    "        # Modify this for parameter search\n",
    "        self.feature_extractor = np.random.choice([VGG16, ResNet50, InceptionV3])\n",
    "        self.model_name = self.feature_extractor.__name__\n",
    "        self.learning_rate = 10 ** np.random.uniform(-5, -3)\n",
    "        self.dropout = np.random.uniform(0.0, 0.5)\n",
    "        self.kernel_regularizer_l2 = 10 ** np.random.uniform(-5, -2)\n",
    "        self.activity_regularizer_l1 = 10 ** np.random.uniform(-5, -2)\n",
    "                \n",
    "    \n",
    "    def serialize(self):\n",
    "        self.model_name = self.feature_extractor.__name__\n",
    "        return ' '.join([\n",
    "            ':'.join([key, str(value)]) \n",
    "            for key, value in [\n",
    "                (\"ID\", self.id),\n",
    "                (\"DS\", self.data_name),\n",
    "                (\"BS\", self.batch_size),\n",
    "                (\"AC\", self.activation),\n",
    "                (\"MO\", self.model_name),\n",
    "                (\"HE\", \"{:d}x{:d}\".format(self.num_hidden_layers, self.num_hidden_neurons)),\n",
    "                (\"TR\", 'all' if self.train_features else 'heads'),\n",
    "                (\"FT\", str(self.pretrained_weights)),\n",
    "                (\"LF\", self.loss.__name__ if callable(self.loss) else str(self.loss)),\n",
    "                (\"LR\", \"{:.1e}\".format(self.learning_rate)),\n",
    "                (\"AR\", \"{:.1e}\".format(self.activity_regularizer_l1 or 0)),\n",
    "                (\"KR\", \"{:.1e}\".format(self.kernel_regularizer_l2 or 0)),\n",
    "                (\"DO\", \"{:.2f}\".format(self.dropout or 0)),\n",
    "                (\"PO\", str(self.pool)),\n",
    "                (\"CW\", str(None) \\\n",
    "                           if self.class_weights is None\\\n",
    "                           else ','.join([\"{:d}={:.2f}\".format(k, v) for k, v in self.class_weights.items()]))\n",
    "            ]])\n",
    "    \n",
    "    def deserialize(self, string):\n",
    "        strs = dict(([tuple(field.split(':')) for field in string.split(' ') if len(field.split(':')) == 2]))\n",
    "        self.id = strs['ID']\n",
    "        self.model_name = strs['MO']\n",
    "        self.batch_size = int(strs['BS'])\n",
    "        self.num_hidden_layers, self.num_hidden_neurons = [int(s) for s in strs['HE'].split('x')]\n",
    "        self.train_features = strs['TR'] == 'all'\n",
    "        self.pretrained_weights = None if strs['FT'] == 'None' else strs['FT']\n",
    "        self.activation = strs['AC']\n",
    "        self.loss = strs['LF']\n",
    "        self.learning_rate = float(strs['LR'])\n",
    "        self.activity_regularizer_l1 = float(strs['KR'])\n",
    "        self.kernel_regularizer_l2 = float(strs['AR'])\n",
    "        self.dropout = float(strs['DO'])\n",
    "        self.pool = strs['PO']\n",
    "        if strs['CW'].lower() in ['none', 'false', '0']:\n",
    "            self.class_weights = None\n",
    "        else:\n",
    "            self.class_weights = {\n",
    "                int(field.split('=')[0]): float(field.split('=')[1])\n",
    "                for field in strs['CW'].split(',')\n",
    "            }\n",
    "        model_map = {\n",
    "            'VGG16': VGG16,\n",
    "            'ResNet50': ResNet50,\n",
    "            'InceptionV3': InceptionV3,\n",
    "        }\n",
    "        if strs['MO'].startswith('simple,'):\n",
    "            fields = strs['MO'].split(',')[1:]\n",
    "            model_map[strs['MO']] = simple_model_factory(int(fields[0]), float(fields[1]), float(fields[2]))\n",
    "        self.feature_extractor = model_map[strs['MO']]\n",
    "    \n",
    "    def describe(self):\n",
    "        return self.serialize().replace(' ', \"\\n\").replace(\":\", \": \")\n",
    "    \n",
    "    def make_model(self, weights=None, parallel=False):\n",
    "        self.model = None\n",
    "        K.clear_session()\n",
    "        add_reg = 'simple' not in self.model_name\n",
    "        self.model = create_new_head(\n",
    "            self.feature_extractor(\n",
    "                include_top=False,\n",
    "                weights=self.pretrained_weights,\n",
    "                input_shape=self.input_shape),\n",
    "            self, add_reg=add_reg\n",
    "        )\n",
    "        if weights:\n",
    "            self.model.set_weights(weights)\n",
    "        else:\n",
    "            freeze = []\n",
    "            if self.pretrained_weights is None:\n",
    "                transfer_vgg(self.model)\n",
    "                freeze = [layer.name for layer in self.model.layers if 'conv' in layer.name.lower()][0:1]\n",
    "            elif self.pretrained_weights == 'imagenet':\n",
    "                if self.feature_extractor.__name__ =='VGG16':\n",
    "                    freeze = [layer.name for layer in self.model.layers if 'conv' in layer.name.lower()][0:3]\n",
    "                elif self.feature_extractor.__name__ =='ResNet50':\n",
    "                    freeze = [layer.name for layer in self.model.layers if 'conv' in layer.name.lower()][0:3]\n",
    "                elif self.feature_extractor.__name__ =='InceptionV3':\n",
    "                    freeze = [layer.name for layer in self.model.layers if 'conv' in layer.name.lower()][0:3]\n",
    "            for name in freeze:\n",
    "                layer = self.model.get_layer(name=name)\n",
    "                print(\"locking\", layer.name)\n",
    "                layer.trainable = False\n",
    "        if parallel:\n",
    "            from keras.utils import multi_gpu_model\n",
    "            self.model2 = multi_gpu_model(self.model, gpus=2)#, cpu_merge=True, cpu_relocation=False)            \n",
    "    \n",
    "    def get_train_model(self):\n",
    "        return self.model2 or self.model\n",
    "            \n",
    "    def compile_model(self, opt_params=None):\n",
    "        if not opt_params:\n",
    "            opt_params={'optimizer': 'nadam'}\n",
    "        opt_params['loss'] = self.loss\n",
    "        self.get_train_model().compile(**opt_params, metrics=['binary_accuracy', 'categorical_accuracy'])\n",
    "        \n",
    "    def set_logdir(self, logdir):\n",
    "        self.log_dir = logdir\n",
    "        self.model_def_path = os.path.join(log_dir, \"model_def.json\")\n",
    "        self.model_weights_path = os.path.join(log_dir, \"model_weights.h5\")\n",
    "        self.model_info_path = os.path.join(log_dir, \"model.json\")\n",
    "        self.model_plot_path = os.path.join(log_dir, \"precision-recall.png\")\n",
    "        \n",
    "    def setup_callbacks(self, schedule=None, hist=False, grads=False):\n",
    "        models_dir = os.path.join(self.log_dir, \"models\")\n",
    "        !mkdir -p \"$models_dir\"\n",
    "        best_path = os.path.join(models_dir, \"best.{epoch:03d}-{val_loss:.4f}.h5\")\n",
    "        self.callbacks = [\n",
    "            TerminateOnNaN(),\n",
    "            ModelCheckpoint(\n",
    "                best_path, monitor='val_loss', verbose=1,\n",
    "                save_best_only=True, save_weights_only=True, mode='auto', period=1),\n",
    "            PRTensorBoard(\n",
    "                log_dir=self.log_dir, \n",
    "                histogram_freq=(hist or 0),\n",
    "                batch_size=10,\n",
    "                write_graph=False,\n",
    "                write_grads=grads,\n",
    "                write_images=False),\n",
    "                EarlyStopping(\n",
    "                    monitor='val_loss', min_delta=0.0, patience=50, verbose=1, mode='auto'),\n",
    "    #         clr_callback.CyclicLR(base_lr=1e-4, max_lr=0.1, min_lr=0.01, step_size=(2*steps_per_epoch))\n",
    "            \n",
    "        ]\n",
    "        if schedule:\n",
    "            self.callbacks.append(schedule)\n",
    "\n",
    "    def go(self, epochs, initial_epoch=0, val_data=None):\n",
    "        steps_per_epoch = len(dataset['ids']['train']) // self.batch_size\n",
    "        steps_per_epoch_val = 100 // self.batch_size #len(dataset['ids']['val']) // batch_size\n",
    "        print(\"Steps per epoch:\", steps_per_epoch)\n",
    "        print(\"Steps per steps_per_epoch_val:\", steps_per_epoch_val)\n",
    "\n",
    "        source = data_source('train')\n",
    "        common = {\n",
    "            \"class_weight\": dataset['class_weights']['train'],\n",
    "            \"callbacks\": self.callbacks,\n",
    "            \"epochs\": epochs,\n",
    "            \"verbose\": 1,\n",
    "            \"initial_epoch\": initial_epoch,\n",
    "#             \"steps_per_epoch\": steps_per_epoch\n",
    "        }\n",
    "        if isinstance(source, list): # Cached data\n",
    "            print(\"Training cached data\")\n",
    "            self.history = self.get_train_model().fit(\n",
    "                x=source[0], y=source[1],\n",
    "                batch_size=self.batch_size,\n",
    "                validation_data=tuple(data_source('val')),\n",
    "                shuffle=True,\n",
    "                **common)\n",
    "        elif val_data:\n",
    "            # Generator training with static val data\n",
    "            self.history = self.get_train_model().fit_generator(\n",
    "                batching_gen(source, batch_size=exp.batch_size),\n",
    "                steps_per_epoch=steps_per_epoch,\n",
    "                validation_steps=steps_per_epoch_val,\n",
    "                validation_data=val_data,\n",
    "                workers=10,\n",
    "                **common)\n",
    "        else:\n",
    "            self.history = self.get_train_model().fit_generator(\n",
    "                batching_gen(source, batch_size=exp.batch_size),\n",
    "                validation_data=batching_gen(\n",
    "                    data_source('val'), batch_size=exp.batch_size),\n",
    "                steps_per_epoch=steps_per_epoch,\n",
    "                validation_steps=steps_per_epoch_val,\n",
    "                workers=10,\n",
    "                **common)\n",
    "        return self.history\n",
    "\n",
    "    def evaluate_model(self, test_data, thresh=0.5):\n",
    "        def multi_label_decision(y_true, y_pred):\n",
    "            return (y_true > thresh) == (y_pred > thresh)\n",
    "        def single_label_decision(y_true, y_pred):\n",
    "            return np.argmax(y_true, axis=-1) == np.argmax(y_pred, axis=-1)\n",
    "        decision_function = single_label_decision if caption_type == 'single' else multi_label_decision\n",
    "\n",
    "        Y_true = test_data[1]\n",
    "        Y_pred = self.get_train_model().predict(test_data[0])\n",
    "        TP = decision_function(Y_true, Y_pred)\n",
    "        acc = np.count_nonzero(TP) / TP.size\n",
    "\n",
    "        print(\"Test using {:d} samples:\".format(len(test_data[0])))\n",
    "        print(\"accuracy\", acc)\n",
    "        return Y_true, Y_pred, TP\n",
    "    \n",
    "\n",
    "    def save_model(\n",
    "            self, class_map_r, prediction_type,\n",
    "            test_metrics=None, description=\"\"):\n",
    "        from abyss.utils import JsonNumpyEncoder\n",
    "        def merged(a, b):\n",
    "            merged = dict(a)\n",
    "            merged.update(b)\n",
    "            return merged\n",
    "\n",
    "        model_info = {\n",
    "            \"name\": self.serialize(),\n",
    "            \"description\": description,\n",
    "            \"weights\": self.model_weights_path,\n",
    "            \"prediction_type\": caption_type,\n",
    "            \"model\": self.model_def_path,\n",
    "            \"classes\": class_map_r,\n",
    "            \"architecture\": {\n",
    "                \"backbone\": \"inceptionv3\", # TODO IMPORTANT\n",
    "                \"logit_activation\": self.model.get_layer(\"class_logits\").activation.__name__,\n",
    "                \"input_shape\": self.input_shape\n",
    "            }\n",
    "\n",
    "        }\n",
    "        if self.history:\n",
    "            model_info['metrics'] = {\n",
    "                \"loss_function\": str(self.history.model.loss),\n",
    "                \"train\": merged(\n",
    "                    self.history,\n",
    "                    {\n",
    "                        \"epoch\": self.history.epoch,\n",
    "                        \"params\": self.history.params\n",
    "                    })\n",
    "            }\n",
    "            if test_metrics:\n",
    "                model_info['metrics']['test'] = test_metrics\n",
    "\n",
    "        print(\"Writing model def to \" + self.model_def_path)\n",
    "        with open(self.model_def_path, \"w\") as file:\n",
    "            file.write(self.model.to_json())\n",
    "\n",
    "        print(\"Writing model weights to \" + self.model_weights_path)\n",
    "        self.model.save_weights(self.model_weights_path)\n",
    "\n",
    "        print(\"Writing model info to \" + self.model_info_path)\n",
    "        with open(self.model_info_path, \"w\") as file:\n",
    "            file.write(json.dumps(model_info, cls=JsonNumpyEncoder))\n",
    "\n",
    "    \n",
    "    def display_performance(self, Y_true, Y_pred, TP):\n",
    "        precision = dict()\n",
    "        recall = dict()\n",
    "        average_precision = dict()\n",
    "        for i in range(self.num_classes):\n",
    "            precision[i], recall[i], _ = precision_recall_curve(Y_true[:, i],\n",
    "                                                                Y_pred[:, i])\n",
    "            average_precision[i] = average_precision_score(Y_true[:, i], Y_pred[:, i])\n",
    "\n",
    "        # A \"micro-average\": quantifying score on all classes jointly\n",
    "        precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(Y_true.ravel(),\n",
    "            Y_pred.ravel())\n",
    "        average_precision[\"micro\"] = average_precision_score(Y_true, Y_pred,\n",
    "                                                             average=\"micro\")\n",
    "        print('Average precision score, micro-averaged over all classes: {0:0.2f}'\n",
    "              .format(average_precision[\"micro\"]))\n",
    "\n",
    "        z = np.all((Y_pred > 0.5) == Y_true, axis=1)\n",
    "        acc = np.count_nonzero(z) / z.size\n",
    "        print(\"exact accuracy\", acc)\n",
    "        z = ((Y_pred > 0.5) == Y_true)\n",
    "        acc = np.count_nonzero(z) / z.size\n",
    "        print(\"binary accuracy\", acc)\n",
    "\n",
    "        # setup plot details\n",
    "        colors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal'])\n",
    "\n",
    "        plt.figure(figsize=(8, 10))\n",
    "        f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "        lines = []\n",
    "        labels = []\n",
    "        for f_score in f_scores:\n",
    "            x = np.linspace(0.01, 1)\n",
    "            y = f_score * x / (2 * x - f_score)\n",
    "            l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "            plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "\n",
    "        lines.append(l)\n",
    "        labels.append('iso-f1 curves')\n",
    "        l, = plt.plot(recall[\"micro\"], precision[\"micro\"], color='gold', lw=2)\n",
    "        lines.append(l)\n",
    "        labels.append('micro-average Precision-recall (area = {0:0.2f})'\n",
    "                      ''.format(average_precision[\"micro\"]))\n",
    "\n",
    "        for i, color in zip(range(self.num_classes), colors):\n",
    "            l, = plt.plot(recall[i], precision[i], color=color, lw=2)\n",
    "            lines.append(l)\n",
    "            labels.append('{0} (area = {1:0.2f})'\n",
    "                          ''.format(caption_map_r[i], average_precision[i]))\n",
    "\n",
    "        fig = plt.gcf()\n",
    "        fig.subplots_adjust(bottom=0.25)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title('Micro Average Precision vs. Recall')\n",
    "        plt.legend(lines, labels, loc=(0, -.4), prop=dict(size=14))\n",
    "        plt.show()\n",
    "        plt.savefig(self.model_plot_path, dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_val_data = len(dataset['ids']['val'])\n",
    "if use_cached:\n",
    "    print(\"use_cached\")\n",
    "    test_data = dataset['data']['test']\n",
    "    val_data = dataset['data']['val']\n",
    "else:\n",
    "    val_data = gen_dump_data(dataset['gens']['val'], num_val_data)\n",
    "    test_data = gen_dump_data(dataset['gens']['test'], len(dataset['ids']['test']))\n",
    "    \n",
    "bg_percent = 1- np.count_nonzero(test_data[1].sum(axis=1)) / test_data[1].shape[0]\n",
    "print(\"Backgound images: {:.1f}%\".format(bg_percent*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_transfer_map = {\n",
    "    'block1_conv1': 'layer0_conv0',\n",
    "}\n",
    "imagenet_weights = {}\n",
    "\n",
    "# with K.tf.device(\"/cpu:0\"):\n",
    "#     with K.tf.Session(config=K.tf.ConfigProto(device_count={'GPU': 0})):\n",
    "source = VGG16(weights='imagenet', include_top=False, input_shape=image_dims)\n",
    "for key_s, key_d in weights_transfer_map.items():\n",
    "    s = source.get_layer(name=key_s)\n",
    "    imagenet_weights[key_s] = s.get_weights()\n",
    "source.summary()\n",
    "print(class_weights)\n",
    "# class_weights[2] *= 4\n",
    "del source\n",
    "K.clear_session()\n",
    "\n",
    "def transfer_vgg(dest):\n",
    "    for key_s, key_d in weights_transfer_map.items():\n",
    "        d = dest.get_layer(name=key_d)\n",
    "        d.set_weights(imagenet_weights[key_s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(class_weights)\n",
    "search_output_dir = \"/data/log/cnn/fd/large-fromCF/monday2\" # Change this output dir.\n",
    "num_epochs_train = 200\n",
    "model_instance = None\n",
    "# model_instance = \"ID:4704 DS:alltogether BS:5 AC:sigmoid MO:simple-2 HE:0x32 TR:all FT:None LF:binary_crossentropy LR:3.7e-04 AR:8.6e-02 KR:2.9e-02 DO:0.00 PO:avg CW:0=2.10,1=0.58,2=0.59,3=0.49,4=1.41,5=1.66,6=3.74,7=1.98 method:step\"\n",
    "history_data = {}\n",
    "\n",
    "def lr_schedule_exp(epoch, base_lr=1e-3, gamma=0.98):\n",
    "    return base_lr * gamma ** epoch\n",
    "\n",
    "def lr_schedule_step(epoch, base_lr, steps):\n",
    "    lr = base_lr\n",
    "    for step_epoch, lr_mult in steps.items():\n",
    "        if epoch >= step_epoch:\n",
    "            lr *= lr_mult\n",
    "    return lr\n",
    "\n",
    "def init_schedule():\n",
    "    ### Pick one\n",
    "    schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=10, verbose=1)\n",
    "#     schedule = LearningRateScheduler(\n",
    "# #         lambda epoch, lr: lr_schedule_exp(epoch, base_lr=5e-3, gamma=0.98) # Exponential decay\n",
    "#         lambda epoch, lr: lr_schedule_step(epoch, base_lr=5e-3, steps={1: 0.1, 5: 0.1}) # Step decay\n",
    "#     )\n",
    "#     schedule = None\n",
    "    return schedule\n",
    "\n",
    "for attempt_no in range(1):\n",
    "    K.clear_session()\n",
    "    exp = Experiment(dataset_name, image_dims)\n",
    "    exp.class_weights = class_weights\n",
    "    exp.batch_size = 6 # keep this divisible by the amount of GPUs\n",
    "    if model_instance:  # If loading network structure from model_instance (not weights)\n",
    "        exp.deserialize(model_instance)\n",
    "        exp.id = str(np.random.randint(0, 999))\n",
    "    else:\n",
    "        exp.random_init()\n",
    "        exp.pretrained_weights = 'imagenet'\n",
    "        exp.train_features = True\n",
    "        exp.dropout = 0\n",
    "        exp.learning_rate = 3e-5 #10**np.random.uniform(-7, -2)\n",
    "        exp.activity_regularizer_l1 = 0e-4 #10**np.random.uniform(-4, 1)\n",
    "        exp.kernel_regularizer_l2 = 0 #10**np.random.uniform(-4, 1)\n",
    "        exp.feature_extractor = InceptionV3 #simple_model_factory(4, exp.activity_regularizer_l1, exp.kernel_regularizer_l2)\n",
    "    experiment_name = exp.serialize() + \" method:plateau\"\n",
    "    print('=' * 80)\n",
    "    print(exp.describe())\n",
    "\n",
    "    log_dir = os.path.join(search_output_dir, experiment_name)\n",
    "    exp.set_logdir(log_dir)\n",
    "\n",
    "    \n",
    "    print(experiment_name)\n",
    "    print(log_dir)\n",
    "    try:\n",
    "        current_epoch = 0\n",
    "        # Initialize heads with high LR then train whole model with low LR\n",
    "        for i, (epochs, lr) in enumerate(zip([num_epochs_train], [exp.learning_rate])):\n",
    "            weights = None if i == 0 else exp.model.get_weights()\n",
    "#             exp.train_features = False#i > 0\n",
    "            exp.make_model(weights, parallel=True)\n",
    "#             for layer in exp.model.layers:\n",
    "#                 layer.trainable = True\n",
    "            exp.compile_model()\n",
    "            exp.setup_callbacks(schedule=init_schedule(), hist=0, grads=True)\n",
    "            \n",
    "            K.set_value(exp.get_train_model().optimizer.lr, lr)\n",
    "            print(\"Training: {:d} layers\".format(len([1 for layer in exp.model.layers if layer.trainable])))\n",
    "            exp.model.summary()\n",
    "            history_data[experiment_name] = exp.go(\n",
    "                current_epoch + epochs,\n",
    "                initial_epoch=current_epoch,\n",
    "                val_data=val_data)\n",
    "            current_epoch += epochs\n",
    "    except KeyboardInterrupt:\n",
    "        history_data[experiment_name] = None\n",
    "    except:\n",
    "        raise\n",
    "    finally:\n",
    "        exp.save_model(\n",
    "            class_map_r=caption_map_r, prediction_type=caption_type,\n",
    "            test_metrics=None,\n",
    "            description=\"Test model for 5 FDs\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.display_performance(*exp.evaluate_model(test_data, thresh=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"Stop run all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if history_data:\n",
    "    with open(os.path.join(search_output_dir, \"history-{:d}epoch.pkl\".format(num_epochs_train)), \"wb\") as file:\n",
    "        pickle.dump({key: history.history for key, history in history_data.items()}, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Should you need to load this pkl:\n",
    "# # with open(os.path.join(search_output_dir, \"history-100epoch.pkl\"), \"rb\") as file:\n",
    "# #     history = pickle.load(file)\n",
    "\n",
    "# lrs = [key[1] for key, val  in sorted(history_data.items(), key=lambda x: x[0])]\n",
    "# val_loss = [history.history['val_loss'][-1] for key, history  in sorted(history_data.items(), key=lambda x: x[0])]\n",
    "# loss = [history.history['loss'][-1] for lr, history  in sorted(history_data.items(), key=lambda x: x[0])]\n",
    "# acc = [history.history['binary_accuracy'][-1] for lr, history  in sorted(history_data.items(), key=lambda x: x[0])]\n",
    "# val_acc = [history.history['val_binary_accuracy'][-1] for lr, history  in sorted(history_data.items(), key=lambda x: x[0])]\n",
    "\n",
    "# plt.figure()\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.semilogx(lrs, loss, '.b', label='loss')\n",
    "# plt.semilogx(lrs, val_loss, '.r', label='val_loss')\n",
    "# plt.legend()\n",
    "# plt.title(\"Loss Vs. LR (100 Epoch)\")\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.semilogx(lrs, acc, '.b', label='binary_accuracy')\n",
    "# plt.semilogx(lrs, val_acc, '.r', label='val_binary_accuracy')\n",
    "# plt.legend()\n",
    "# plt.title(\"Accuracy Vs. LR (100 Epoch)\")\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"/data/log/cnn/fd/large-fromCF/thursday/ID:3379 DS:alltogether-unique BS:7 AC:sigmoid MO:InceptionV3 HE:0x0 TR:all FT:imagenet LF:binary_crossentropy LR:2.7e-05 AR:1.8e-05 KR:3.1e-05 DO:0.00 PO:avg CW:0=3.05,1=0.54,2=1.02,3=0.54,4=4.57,5=0.76,6=2.29 method:plateau\"\n",
    "exp.model_def_path = os.path.join(log_dir, \"model_def.json\")\n",
    "exp.model_weights_path = os.path.join(log_dir, \"model_weights.h5\")\n",
    "exp.model_info_path = os.path.join(log_dir, \"model.json\")\n",
    "exp.model_plot_path = os.path.join(log_dir, \"precision-recall.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_instance = \"ID:3379 DS:alltogether-unique BS:7 AC:sigmoid MO:InceptionV3 HE:0x0 TR:all FT:imagenet LF:binary_crossentropy LR:2.7e-05 AR:1.8e-05 KR:3.1e-05 DO:0.00 PO:avg CW:0=3.05,1=0.54,2=1.02,3=0.54,4=4.57,5=0.76,6=2.29 method:plateau\"\n",
    "# exp = Experiment(dataset_name, image_dims)\n",
    "# exp.deserialize(model_instance)\n",
    "# exp.make_model(parallel=True)\n",
    "# exp.get_train_model().load_weights(\"/data/log/cnn/fd/large-fromCF/thursday/ID:3379 DS:alltogether-unique BS:7 AC:sigmoid MO:InceptionV3 HE:0x0 TR:all FT:imagenet LF:binary_crossentropy LR:2.7e-05 AR:1.8e-05 KR:3.1e-05 DO:0.00 PO:avg CW:0=3.05,1=0.54,2=1.02,3=0.54,4=4.57,5=0.76,6=2.29 method:plateau/models/best.040-0.2809.h5\")\n",
    "# # exp.id = \"3379\"\n",
    "# exp.set_logdir(os.path.join(search_output_dir, 'fritatta'))\n",
    "exp.display_performance(*exp.evaluate_model(test_data, thresh=0.5))\n",
    "# exp.history = dict()\n",
    "# exp.save_model(\n",
    "#     caption_map_r,\n",
    "#     prediction_type=caption_type, test_metrics=None, description=\"Test model for 5 FDs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue training specific model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "K.clear_session()\n",
    "search_output_dir = \"/data/log/cnn/fd/large-fromCF/thursday\" # Change this output dir.\n",
    "model_instance = \"ID:3379 DS:alltogether-unique BS:7 AC:sigmoid MO:InceptionV3 HE:0x0 TR:all FT:imagenet LF:binary_crossentropy LR:2.7e-05 AR:1.8e-05 KR:3.1e-05 DO:0.00 PO:avg CW:0=3.05,1=0.54,2=1.02,3=0.54,4=4.57,5=0.76,6=2.29 method:plateau\"\n",
    "model_best_weight = \"model_weights.h5\"\n",
    "\n",
    "### Don't set below\n",
    "exp = Experiment(dataset_name, image_dims)\n",
    "exp.deserialize(model_instance)\n",
    "exp.id = \"{:d}\".format(np.random.randint(0, 999))\n",
    "exp.dropout = 0.5\n",
    "\n",
    "experiment_name = exp.serialize()\n",
    "\n",
    "model_weights_in_path = os.path.join(search_output_dir, model_instance, model_best_weight)\n",
    "log_dir = os.path.join(search_output_dir, model_instance, \"continued\", experiment_name)\n",
    "best_path = os.path.join(log_dir, \"models/best.{epoch:03d}-{val_loss:.4f}.h5\")\n",
    "model_def_path = os.path.join(log_dir, \"model_def.json\")\n",
    "model_info_path = os.path.join(log_dir, \"model.json\")\n",
    "model_plot_path = os.path.join(log_dir, \"precision-recall.png\")\n",
    "\n",
    "print(model_weights_in_path)\n",
    "if os.path.exists(model_weights_in_path):\n",
    "    !mkdir -p \"$log_dir/models\"\n",
    "else:\n",
    "    raise OSError(\"path does not exist\")\n",
    "    \n",
    "print(\"loading\")\n",
    "print(os.path.join(search_output_dir, model_instance, \"model.json\"))\n",
    "exp.model = Inference(os.path.join(search_output_dir, model_instance, \"model.json\")).model\n",
    "# base_model = p.feature_extractor(\n",
    "#     include_top=False, weights=p.pretrained_weights, input_shape=image_dims)\n",
    "# model = create_new_head(\n",
    "#     base_model, p.num_classes, p.caption_type, p, \n",
    "#     opt_params={'optimizer': 'nadam'}\n",
    "# )\n",
    "exp.model.load_weights(model_weights_in_path)\n",
    "# model = add_model_regularization(model, params.kernel_regularizer_l2, params.activity_regularizer_l1)\n",
    "\n",
    "# for layer in model.layers:\n",
    "#     layer.trainable = True\n",
    "\n",
    "exp.model.compile( # TODO, load this from JSON, manually change this if you are doing single label\n",
    "    'nadam',\n",
    "    loss=exp.loss,\n",
    "    metrics=['binary_accuracy', 'categorical_accuracy'])\n",
    "\n",
    "print(experiment_name)\n",
    "print(log_dir)\n",
    "print(\"Training: {:d} layers\".format(len([1 for layer in exp.model.layers if layer.trainable])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epoch = 2\n",
    "num_epoch = 200 # cumulative with initial_epoch\n",
    "class_weights = None # Can't currently resume training with imbalance data #TODO\n",
    "new_learning_rate = 1e-6\n",
    "### Pick one schedule\n",
    "schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.85, patience=3, cooldown=0, verbose=1)\n",
    "# schedule = lambda epoch, lr: lr_schedule_exp(epoch, base_lr=5e-5, gamma=0.98) # Exponential decay\n",
    "# schedule = None\n",
    "\n",
    "\n",
    "#### Don't set below\n",
    "\n",
    "K.set_value(exp.model.optimizer.lr, new_learning_rate)\n",
    "callbacks = setup_callbacks(log_dir, schedule=schedule, hist=2, grads=True)\n",
    "history_data[experiment_name] = go(exp.model, callbacks, num_epoch, exp.class_weights, initial_epoch=initial_epoch)\n",
    "\n",
    "(Y_true, Y_pred, TP) = evaluate_model(exp.model, test_data, thresh=0.5)\n",
    "display_performance(Y_true, Y_pred, TP)\n",
    "\n",
    "save_model(\n",
    "    exp.model, name=experiment_name,\n",
    "    class_map_r=caption_map_r, prediction_type=caption_type,\n",
    "    model_weights_path=model_weights_path, model_def_path=model_def_path, model_info_path=model_info_path,\n",
    "    test_metrics=None, history=history_data[experiment_name],\n",
    "    description=\"Test model for 5 FDs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LrSearch():\n",
    "    def __init__(self, instance_str):\n",
    "        self.model_init_w = None\n",
    "        self.model_instance = instance_str\n",
    "        self.exp = None\n",
    "        self.history = {}\n",
    "        \n",
    "    def new_model(self):\n",
    "        K.clear_session()\n",
    "        self.exp = Experiment(dataset_name, image_dims)\n",
    "        self.exp.deserialize(self.model_instance)\n",
    "        self.exp.make_model(self.model_init_w, parallel=True)\n",
    "        self.exp.compile_model()\n",
    "#         self.exp.setup_callbacks(schedule=init_schedule(), hist=0)\n",
    "        print(\"Trainable layers: {:d}\".format(sum([layer.trainable for layer in self.exp.model.layers])))\n",
    "        if self.model_init_w is None:\n",
    "            self.model_init_w = self.exp.model.get_weights()\n",
    "        \n",
    "        \n",
    "    def go(self, epochs, num_steps=10, lr_range=(-9, 0)):\n",
    "        for base_lr in 10 ** np.random.uniform(*lr_range, size=num_steps):\n",
    "            print(\"Learning rate = {:.3e}\".format(base_lr))\n",
    "            self.new_model()\n",
    "            K.set_value(self.exp.get_train_model().optimizer.lr, base_lr)\n",
    "            self.history[base_lr] = self.exp.go(epochs)\n",
    "        return self.history\n",
    "    \n",
    "    def plot(self):\n",
    "        df = []\n",
    "        for lr, h in self.history.items():\n",
    "            metrics = np.vstack([np.array(h.history['binary_accuracy']), np.array(h.history['loss'])])\n",
    "            diff = (metrics[:, -1] - metrics[:, 0])[np.newaxis]\n",
    "            metrics = np.vstack([metrics[:, -1], diff]).ravel()\n",
    "            metrics = np.array([lr] + metrics.tolist())\n",
    "            dfh = pd.DataFrame(\n",
    "                    data=pd.Series(\n",
    "                        data=metrics,\n",
    "                        index=['lr', 'binary_accuracy', 'loss', 'diff_acc', 'diff_loss'])\n",
    "                ).T.set_index('lr')\n",
    "            df.append(dfh)\n",
    "        df = pd.concat(df).sort_index()\n",
    "        df.plot(logx=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_instance = \"ID:5740 DS:alltogether BS:4 AC:sigmoid MO:VGG16 HE:0x64 TR:all FT:imagenet LF:binary_crossentropy LR:5.1e-04 AR:1.0e-06 KR:1.0e-06 DO:0.00 PO:avg CW:0=4.26,1=0.64,2=0.49,3=7.11 method:plateau\"\n",
    "search = LrSearch(model_instance)\n",
    "# Hexpo: 4.5e-4 (1e-4 to 6e-4)\n",
    "# VGG Heads only: 4.4e-5 to 2e-2\n",
    "# VGG Imagenet: 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.go(5, num_steps=4, lr_range=(-6, -4))\n",
    "search.plot()\n",
    "# del search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below is untested with new changes, don't use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_gradients(model):\n",
    "#     \"\"\"Get the gradients of the loss with respect to the weights.\"\"\"\n",
    "#     weights = [tensor for tensor in model.trainable_weights \n",
    "#                if model.trainable_weights]\n",
    "#     return weights, model.optimizer.get_gradients(model.total_loss, weights)\n",
    "\n",
    "        \n",
    "# def hamming_loss(y_true, y_pred):\n",
    "#     return K.mean(y_true * (1 - y_pred) + (1 - y_true) * y_pred)\n",
    "\n",
    "\n",
    "# def check_gradients(model):\n",
    "#     grad_test = None\n",
    "#     for image, label in train_gen:\n",
    "#         grad_test = (image, label)\n",
    "#         break\n",
    "#     rates = []\n",
    "#     weights, grads = get_gradients(model)\n",
    "#     feed_dict = {\n",
    "#         \"class_logits_sample_weights:0\": np.ones(2),\n",
    "#         \"input_1:0\": grad_test[0][np.newaxis, ...],\n",
    "#         \"class_logits_target:0\": grad_test[1][np.newaxis, ...]\n",
    "#     }\n",
    "#     for i, (w, g) in enumerate(zip(weights, grads)):\n",
    "#         if 'bias' in w.name:\n",
    "#             continue\n",
    "#         grad_norm = np.linalg.norm(g.eval(feed_dict, K.get_session()))\n",
    "#         weight_norm = np.linalg.norm(w.eval(K.get_session()))\n",
    "#         rate = grad_norm / weight_norm\n",
    "#         rates.append(rate)\n",
    "#     if np.mean(rates) < 5e-4 or np.mean(rates) > 3e-1: # These values change with network structure\n",
    "#         print(\"Bad gradients ({:.3e}).\".format(np.mean(rates)))\n",
    "#         return False\n",
    "#     return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer=model.layers[-3]\n",
    "# print(layer)\n",
    "# for weight in layer.weights:\n",
    "#     weight.initializer.run(session=K.get_session())\n",
    "# # w = layer.get_weights()\n",
    "# # plt.figure()\n",
    "# # plt.hist(w[0].ravel(), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # More training\n",
    "# K.set_value(model.optimizer.lr, 1e-4)\n",
    "\n",
    "# go(300, class_weights, initial_epoch=200)\n",
    "# (Y_true, Y_pred, TP) = evaluate_model(model, test_data, thresh=0.5)\n",
    "# display_performance(Y_true, Y_pred, TP)\n",
    "\n",
    "# # save_model(\n",
    "# #     model, name=experiment_name + \"-second\",\n",
    "# #     class_map_r=caption_map_r, prediction_type=caption_type,\n",
    "# #     model_weights_path=model_weights_path, model_def_path=model_def_path, model_info_path=model_info_path,\n",
    "# #     test_metrics=None, history=history_data[experiment_name],\n",
    "# #     description=\"Test model for 5 FDs\"\n",
    "# # )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from herbicide.utils import vis_square\n",
    "TP_mask = np.logical_and.reduce(TP, axis=1)\n",
    "right = test_data[0][TP_mask]\n",
    "wrong = test_data[0][~TP_mask]\n",
    "wrong.shape\n",
    "plt.figure()\n",
    "vis_square(wrong)\n",
    "plt.title(\"Incorrectly Predicted\")\n",
    "plt.figure()\n",
    "vis_square(right)\n",
    "plt.title(\"Correctly Predicted\")\n",
    "\n",
    "# Binary coded the labels then count them wrt TP/FP\n",
    "print(\"num labels\", test_data[1].sum(axis=0))\n",
    "coded = np.sum(test_data[1][~TP_mask] * 2 ** np.arange(num_classes)[::-1], axis=1).astype(int)\n",
    "print(\"binary coded class error count:\", dict(sorted(Counter(coded).items(), key=lambda x: x[0])))\n",
    "coded = np.sum(test_data[1][TP_mask] * 2 ** np.arange(num_classes)[::-1], axis=1).astype(int)\n",
    "print(\"binary coded class correct count:\", dict(sorted(Counter(coded).items(), key=lambda x: x[0])))\n",
    "print(Y_pred[TP_mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def learning_curve(dataset, lr, steps, val_data, log_dir):\n",
    "#     def save_model(path):\n",
    "#         print(\"Saving\", path)\n",
    "#         os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "#         model.save_weights(path)\n",
    "#     def setup_callbacks():\n",
    "#         return [\n",
    "# #                 ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, cooldown=5, verbose=1),\n",
    "# #                 ModelCheckpoint(\n",
    "# #                     model_best_path, monitor='val_loss', verbose=1,\n",
    "# #                     save_best_only=True, save_weights_only=True, mode='auto', period=1),\n",
    "# #                 ModelCheckpoint(\n",
    "# #                     best_path, monitor='val_loss', verbose=1,\n",
    "# #                     save_best_only=False, save_weights_only=True, mode='auto', period=50),\n",
    "#                 PRTensorBoard(\n",
    "#                     log_dir=model_log_dir,\n",
    "#                     histogram_freq=0,\n",
    "#                     batch_size=batch_size,\n",
    "#                     write_graph=False,\n",
    "#                     write_grads=False,\n",
    "#                     write_images=False),\n",
    "#         #         EarlyStopping(\n",
    "#         #             monitor='val_loss', min_delta=0.0, patience=40, verbose=1, mode='auto')\n",
    "#         ]\n",
    "#     def create_new_model(load_base=False):\n",
    "#         clear_session()\n",
    "#         model = create_new_head(\n",
    "#             InceptionV3(include_top=False, weights='imagenet', input_shape=image_dims),\n",
    "#             num_classes, caption_type, opt_params={'optimizer': Nadam()},\n",
    "#             class_weights=None, train_features=False, l2_reg=None)\n",
    "#         if load_base:\n",
    "#             print(\"Loading base model\")\n",
    "#             model.load_weights(base_model_path, by_name=True)\n",
    "#         return model\n",
    "\n",
    "#     def train():\n",
    "#         print(\"Training\")\n",
    "#         K.set_value(model.optimizer.lr, lr)\n",
    "#         history[subset_size] = model.fit_generator(\n",
    "#             batching_gen(gen, batch_size=batch_size),\n",
    "#             validation_data=tuple(val_data),\n",
    "#             steps_per_epoch=(subset_size // batch_size),\n",
    "#             validation_steps=steps_per_epoch_val,\n",
    "#             class_weight=model_class_weights,\n",
    "#             callbacks=setup_callbacks(), \n",
    "#             epochs=50,\n",
    "#             verbose=1)\n",
    "#     model_class_weights = None\n",
    "#     model = None\n",
    "#     model_path = None\n",
    "#     image_ids = [image['id'] for image in dataset.imgs.values()]\n",
    "#     np.random.shuffle(image_ids)\n",
    "#     num_images = len(image_ids)\n",
    "#     print(\"num_images\", num_images)\n",
    "#     history = {}\n",
    "#     base_model_path = os.path.join(log_dir, \"base\", \"weights.h5\")\n",
    "#     model_path = base_model_path\n",
    "#     for subset_size in np.linspace(0, num_images, steps + 1).astype(int):\n",
    "#         if subset_size > 0:\n",
    "#             imgIds = image_ids[:subset_size]\n",
    "#             gen = pipeline(\n",
    "#                 dataset.generator(shuffle_ids=False, imgIds=imgIds),\n",
    "#                 aug_config=None)\n",
    "#             model_class_weights = calc_class_weights(gen, dataset) # TODO\n",
    "\n",
    "#             model_path = os.path.join(log_dir, \"subset-of-{:d}/weights.h5\".format(subset_size))\n",
    "#             model_log_dir = os.path.dirname(model_path)\n",
    "#             model_best_path = os.path.join(log_dir, \"subset-of-{:d}/best.h5\".format(subset_size))\n",
    "#             os.makedirs(model_log_dir, exist_ok=True)\n",
    "\n",
    "#             print(\"learning curve(lr={:.3e}, size={:d})\".format(lr, subset_size))\n",
    "#             print(\"model_log_dir\", model_log_dir)\n",
    "#             print(\"training class weights\")\n",
    "#             print(model_class_weights)\n",
    "#         model = create_new_model(load_base=(subset_size > 0))\n",
    "#         if subset_size:\n",
    "#             train()\n",
    "#         save_model(model_path)\n",
    "#     return history\n",
    "\n",
    "# model = None\n",
    "# lr = 1e-5\n",
    "# learning_curve_dir = \"/data/log/cnn/fd/learning_curve_5--{:.2e}\".format(lr)\n",
    "# lc_history = learning_curve(coco_train, lr, 5, val_data, learning_curve_dir)\n",
    "# val_loss = np.array([(size, h.history['val_loss'][-1]) for size, h in lc_history.items()])\n",
    "# train_loss = np.array([(size, h.history['loss'][-1]) for size, h in lc_history.items()])\n",
    "# plt.figure()\n",
    "# plt.plot(train_loss[:, 0], train_loss[:, 1], 'b.')\n",
    "# plt.plot(val_loss[:, 0], val_loss[:, 1], 'r.')\n",
    "# plt.xlabel(\"Number of Training Samples\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.savefig(os.path.join(learning_curve_dir, \"plot.png\"), dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -R /data/log/cnn/fd/learning-curve/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = None\n",
    "# for images, labels in batching_gen(train_gen, batch_size=batch_size):\n",
    "#     print(images.shape, labels.shape)\n",
    "    \n",
    "#     pred = model.predict(images)\n",
    "#     print(labels)\n",
    "#     print(pred)\n",
    "#     print(K.eval(K.tf.losses.sigmoid_cross_entropy(labels, pred)))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for unique_label in np.unique(val_data[1], axis=0):\n",
    "#     unique_data = [val_data[0][i] for i in range(len(val_data[0])) if np.all(val_data[1][i] == unique_label)]\n",
    "#     num_data = len(unique_data)\n",
    "#     print(unique_label, num_data)\n",
    "#     plt.figure()\n",
    "#     vis_square(np.array(unique_data))\n",
    "#     plt.title(unique_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Update/Weight Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for i, (w, g) in enumerate(zip(weights, grads)):\n",
    "#     grad_norm = np.linalg.norm(g.eval(feed_dict, K.get_session()))\n",
    "#     weight_norm = np.linalg.norm(w.eval(K.get_session()))\n",
    "#     rate = grad_norm / weight_norm\n",
    "#     print(i, rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from herbicide.utils import vis_square\n",
    "for layer in model.layers:\n",
    "    if not layer.trainable_weights:\n",
    "        continue\n",
    "    for weight in layer.trainable_weights: #  Assumes FD is not trainable\n",
    "        if 'kernel' not in weight.name:\n",
    "            continue\n",
    "        print(weight.name)\n",
    "        value = K.eval(weight.value())\n",
    "        print(value.shape)\n",
    "    \n",
    "    plt.figure()\n",
    "    vis_square(value.transpose((3, 0, 1, 2)))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_stats = []\n",
    "for i, (image, caption) in enumerate(coco_train.generator(imgIds=balanced_image_ids_train)):\n",
    "    plt.figure()\n",
    "    plt.imshow(image)\n",
    "    plt.title(str(caption))\n",
    "    if i == 10:\n",
    "        break\n",
    "    caption_stats.append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
