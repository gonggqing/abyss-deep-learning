{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings\n",
    "  * general\n",
    "    * batch size should be at least 1x(number of classes) in order to take advantage of MTL\n",
    "    * Always start training a model with dropout=ker_reg=act_reg=0, look at bias and variance then add until good fit\n",
    "    * No pooling gives very fast results but strong overfitting and large model size\n",
    "    * Don't worry about class weights unless heavily (20x or more) imbalanced\n",
    "    * head:0x** works best, any more layers underfits\n",
    "    * training with dropout makes val_loss very noisy and auto saving doesn't work well, enable regular saving\n",
    "    * Don't use both Dropout and Batch Norm together (if you do use very small dropout, https://arxiv.org/pdf/1801.05134.pdf)\n",
    "    \n",
    "  * multi-label output\n",
    "    * sigmoid output makes model very sensitive to learning_rate.\n",
    "      * I have found with VGG16 around 5e-5 is a good start\n",
    "      * Use eg setup_callbacks(hist=2, grads=True) to enable gradient outputs; check if class_logits_out is becoming spread between 0 and 1, check that gradients are not 0 (should be around 1e-3).\n",
    "  * single-label output\n",
    "    * Pretty stable with any architecture\n",
    "    \n",
    "\n",
    "## Bugs\n",
    "  * There may be a GPU memory leak somewhere... keras does not recycle models properly. I'll try to find this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import os\n",
    "# os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda/lib64:' + os.environ['LD_LIBRARY_PATH']\n",
    "from abyss_deep_learning.utils import config_gpu\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "config_gpu([1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup application specifics\n",
    "Configure below cell:\n",
    "* Training augmentation (AUG_CONFIG)\n",
    "* Pre- and post- processing of data\n",
    "* Various arguments\n",
    "* Caption map and caption translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_args():\n",
    "    from keras.applications.resnet50 import preprocess_input\n",
    "    from bidict import bidict\n",
    "    from imgaug import augmenters as iaa\n",
    "    from imgaug.parameters import Normal\n",
    "    from skimage.transform import resize\n",
    "    \n",
    "    from abyss_deep_learning.datasets.translators import AnnotationTranslator\n",
    "\n",
    "    def preprocess_data(image):\n",
    "        '''Transform the image before (possibly caching) and input to the network.'''\n",
    "        image = resize(image, args['image_dims'], preserve_range=True, mode='constant')\n",
    "        return preprocess_input(image.astype(args['nn_dtype']), mode='tf')\n",
    "\n",
    "    def postprocess_data(image):\n",
    "        '''Inverse transform of preprocess_data, used when trying to visualize images out of the dataset.'''\n",
    "        return ((image + 1) * 127.5).astype(np.uint8)\n",
    "\n",
    "    def pipeline(gen, aug_config=None):\n",
    "        '''The pipeline to run the dataset generator through.'''\n",
    "        from abyss_deep_learning.keras.classification import multihot_gen, augmentation_gen\n",
    "\n",
    "        return (\n",
    "            augmentation_gen(\n",
    "                multihot_gen(gen, num_classes=args['num_classes'])\n",
    "            , aug_config, enable=(aug_config is not None))\n",
    "        )\n",
    "\n",
    "    class CaptionMapper(AnnotationTranslator):\n",
    "        '''Transform JSON string CSV caption annotations into a list of integer captions'''\n",
    "        def __init__(self, caption_map):\n",
    "            self.caption_map = caption_map\n",
    "            self.num_classes = len(caption_map)\n",
    "\n",
    "        def filter(self, annotation):\n",
    "            return 'caption' in annotation\n",
    "\n",
    "        def translate(self, annotation):\n",
    "            return [\n",
    "                self.caption_map[caption]\n",
    "                for caption in annotation['caption'].split(',')\n",
    "                if caption in self.caption_map]\n",
    "        \n",
    "    caption_map = bidict({\n",
    "        \"IP\": 0,\n",
    "        \"ED\": 1\n",
    "    })\n",
    "    \n",
    "    augmentation_config = iaa.Sequential([ \n",
    "        iaa.Fliplr(0.5),\n",
    "        iaa.Flipud(0.5),\n",
    "        iaa.Affine(\n",
    "            scale=(0.8, 1.2),\n",
    "            translate_percent=(-0.2, 0.2), \n",
    "            rotate=(-22.5, 22.5),\n",
    "            mode='constant', cval=0, order=0\n",
    "        ),\n",
    "        iaa.Sequential([ # Colour aug\n",
    "            iaa.ChangeColorspace(from_colorspace=\"RGB\", to_colorspace=\"HSV\"),\n",
    "            iaa.WithChannels(0, iaa.Add(Normal(0, 256 / 6))),\n",
    "            iaa.WithChannels(1, iaa.Add(Normal(0, 256 / 6))),\n",
    "            iaa.WithChannels(2, iaa.Add(Normal(0, 256 / 6))),\n",
    "            iaa.ChangeColorspace(from_colorspace=\"HSV\", to_colorspace=\"RGB\")\n",
    "        ])\n",
    "    ])\n",
    "\n",
    "    args = {\n",
    "        'annotation_translator': CaptionMapper(caption_map),\n",
    "        'augmentation': augmentation_config,    # Training augmentation\n",
    "        'caption_map': caption_map,             # Captio\n",
    "        'caption_type': ['single', 'multi'][1], # Caption type can be either \"single\" or \"multi\".\n",
    "                                                # This sets up various other parameters in the system.\n",
    "        'data': {\n",
    "            'base_dir': \"/data/abyss/projectmax/feature-detection/large-fromCF\",\n",
    "            'name': \"alltogether-unique\",\n",
    "            'sets': ('train', 'val', 'test')\n",
    "        },\n",
    "        'image_dims': (480//2, 640//2, 3),    # What to resize images to before CNN\n",
    "        'nn_dtype': np.float32,         # Pretrained networks are in float32\n",
    "        'num_classes': len(caption_map),\n",
    "        'use_balanced_set': False,      # Force the use of the largest class-balanced dataset\n",
    "        'use_cached': False,            # Cache the dataset in memory\n",
    "        'use_class_weights': True,      # Use class population to weight in the training loss\n",
    "        'use_parallel': False,          # Use multiple GPUs\n",
    "        'preprocess_data': preprocess_data,\n",
    "        'postprocess_data': postprocess_data,\n",
    "        'pipeline': pipeline\n",
    "    }\n",
    "    \n",
    "    return args\n",
    "ARGS = setup_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_datasets(args):\n",
    "    from abyss_deep_learning.datasets.coco import ImageClassificationDataset\n",
    "    \n",
    "    dataset = dict()\n",
    "    for set_name in args['data']['sets']:\n",
    "        path = os.path.join(args['data']['base_dir'], \"{:s}/{:s}.json\".format(args['data']['name'], set_name))\n",
    "        dataset[set_name] = ImageClassificationDataset(\n",
    "            path,\n",
    "            translator=args['annotation_translator'],\n",
    "            cached=args['use_cached'],\n",
    "            preprocess_data=args['preprocess_data'])\n",
    "        print(\"\\n\", set_name)\n",
    "        dataset[set_name].print_class_stats()\n",
    "\n",
    "\n",
    "    print(\"\\nNumber of classes:\", args['num_classes'])\n",
    "    print(\"captions:\")\n",
    "    print(args['caption_map'])\n",
    "    return dataset\n",
    "DATASET = setup_datasets(ARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataset_speed(set_name='train'):\n",
    "    image, target = DATASET['train'].sample()\n",
    "    if np.sum(target) == 0:\n",
    "        print(\"BG\")\n",
    "    else:\n",
    "        print(image.shape)\n",
    "        print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit -n5 -r1\n",
    "test_dataset_speed('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_dataset_samples(num_rows=2):\n",
    "    plt.figure()\n",
    "    print(\"Column-wise left to right, bottom row:\")\n",
    "    for i, (name, ds) in enumerate(DATASET.items()):\n",
    "        print(name, end=' ')\n",
    "        for j, (image, label) in enumerate(ARGS['pipeline'](ds.generator(shuffle_ids=True))):\n",
    "            print(label)\n",
    "            plt.subplot(num_rows, 3, 3 * j + i + 1)\n",
    "            plt.imshow(ARGS['postprocess_data'](image))\n",
    "            plt.title(', '.join([ARGS['caption_map'].inv[int(cap_id)] for cap_id in np.argwhere(label)]))\n",
    "            plt.axis('off')\n",
    "            if j + 1 == num_rows:\n",
    "                break\n",
    "        print('shape: {}, label: {}, min: {:.1f}, mean: {:.1f}, max: {:.1f}'.format(\n",
    "            image.shape, label, image.min(), image.mean(), image.max()))\n",
    "\n",
    "view_dataset_samples(num_rows=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell intentionally left blank due to display bug above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit -n1 -r1\n",
    "# Takes 1.52 seconds with pipeline, use_cached=False\n",
    "# Takes 1.52 seconds without pipeline, use_cached=False\n",
    "# Takes  seconds with pipeline, use_cached=True\n",
    "# Takes  seconds without pipeline, use_cached=True\n",
    "def dump_dataset(dataset, num_data, aug_config=None):\n",
    "    data = np.empty((num_data,) + tuple(ARGS['image_dims']), dtype=ARGS['nn_dtype'])\n",
    "    targets = np.empty((num_data, ARGS['num_classes']), dtype=ARGS['nn_dtype'])\n",
    "    for i, (datum, target) in enumerate(ARGS['pipeline'](dataset.generator(), aug_config)):\n",
    "        data[i], targets[i] = datum, target\n",
    "        if i + 1 == num_data:\n",
    "            break\n",
    "    return data, targets\n",
    "\n",
    "VAL_DATA = dump_dataset(DATASET['val'], num_data=len(DATASET['val'].data_ids), aug_config=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify this class to change search parameters\n",
    "class Experiment(object):\n",
    "    def __init__(self, data_name, input_shape):\n",
    "        from keras.applications.vgg16 import VGG16\n",
    "\n",
    "        self.callbacks = None\n",
    "        self.history = dict()\n",
    "        self.data_name = ARGS['data']['name']\n",
    "        self.num_classes = ARGS['num_classes']\n",
    "        self.caption_type = ARGS['caption_type']\n",
    "        self.model = None\n",
    "        self.model2 = None\n",
    "        self.input_shape = input_shape\n",
    "        self.batch_size = ARGS['num_classes']\n",
    "        self.id = str(np.random.randint(0, 9999))\n",
    "        if self.caption_type == 'single':\n",
    "            self.activation = 'softmax'\n",
    "            self.loss = 'categorical_crossentropy'\n",
    "        else:\n",
    "            self.activation = 'sigmoid'\n",
    "            self.loss = 'binary_crossentropy'\n",
    "            \n",
    "        self.backbone = 'VGG16'\n",
    "        self.learning_rate = 5e-5\n",
    "        self.dropout = None\n",
    "        self.train_features = True\n",
    "        self.pool = 'avg'\n",
    "        self.num_hidden_layers = 0\n",
    "        self.num_hidden_neurons = 0\n",
    "        self.pretrained_weights = 'imagenet'\n",
    "        self.class_weights = None\n",
    "        self.kernel_regularizer_l2 = None\n",
    "        self.activity_regularizer_l1 = None\n",
    "        \n",
    "    @property\n",
    "    def backbone(self):\n",
    "        return self._backbone\n",
    "    \n",
    "    @backbone.setter\n",
    "    def backbone(self, value):\n",
    "        self._backbone = value\n",
    "        self.model_name = value\n",
    "\n",
    "    def random_init(self):\n",
    "        # Modify this for parameter search\n",
    "        self.backbone = np.random.choice(['VGG16', 'ResNet50', 'InceptionV3'])\n",
    "        \n",
    "        self.learning_rate = 10 ** np.random.uniform(-5, -3)\n",
    "        self.dropout = np.random.uniform(0.0, 0.5)\n",
    "        self.kernel_regularizer_l2 = 10 ** np.random.uniform(-5, -2)\n",
    "        self.activity_regularizer_l1 = 10 ** np.random.uniform(-5, -2)\n",
    "                \n",
    "    \n",
    "    def serialize(self):\n",
    "        return ' '.join([\n",
    "            ':'.join([key, str(value)]) \n",
    "            for key, value in [\n",
    "                (\"ID\", self.id),\n",
    "                (\"DS\", self.data_name),\n",
    "                (\"BS\", self.batch_size),\n",
    "                (\"AC\", self.activation),\n",
    "                (\"MO\", self.model_name),\n",
    "                (\"HE\", \"{:d}x{:d}\".format(self.num_hidden_layers, self.num_hidden_neurons)),\n",
    "                (\"TR\", 'all' if self.train_features else 'heads'),\n",
    "                (\"FT\", str(self.pretrained_weights)),\n",
    "                (\"LF\", self.loss.__name__ if callable(self.loss) else str(self.loss)),\n",
    "                (\"LR\", \"{:.1e}\".format(self.learning_rate)),\n",
    "                (\"AR\", \"{:.1e}\".format(self.activity_regularizer_l1 or 0)),\n",
    "                (\"KR\", \"{:.1e}\".format(self.kernel_regularizer_l2 or 0)),\n",
    "                (\"DO\", \"{:.2f}\".format(self.dropout or 0)),\n",
    "                (\"PO\", str(self.pool)),\n",
    "                (\"CW\", str(None) \\\n",
    "                           if self.class_weights is None\\\n",
    "                           else ','.join([\"{:d}={:.2f}\".format(k, v) for k, v in self.class_weights.items()]))\n",
    "            ]])\n",
    "    \n",
    "    def deserialize(self, string):\n",
    "        strs = dict(([tuple(field.split(':')) for field in string.split(' ') if len(field.split(':')) == 2]))\n",
    "        self.id = strs['ID']\n",
    "        self.model_name = strs['MO']\n",
    "        self.batch_size = int(strs['BS'])\n",
    "        self.num_hidden_layers, self.num_hidden_neurons = [int(s) for s in strs['HE'].split('x')]\n",
    "        self.train_features = strs['TR'] == 'all'\n",
    "        self.pretrained_weights = None if strs['FT'] == 'None' else strs['FT']\n",
    "        self.activation = strs['AC']\n",
    "        self.loss = strs['LF']\n",
    "        self.learning_rate = float(strs['LR'])\n",
    "        self.activity_regularizer_l1 = float(strs['KR'])\n",
    "        self.kernel_regularizer_l2 = float(strs['AR'])\n",
    "        self.dropout = float(strs['DO'])\n",
    "        self.pool = strs['PO']\n",
    "        if strs['CW'].lower() in ['none', 'false', '0']:\n",
    "            self.class_weights = None\n",
    "        else:\n",
    "            self.class_weights = {\n",
    "                int(field.split('=')[0]): float(field.split('=')[1])\n",
    "                for field in strs['CW'].split(',')\n",
    "            }\n",
    "        model_map = {\n",
    "            'VGG16': VGG16,\n",
    "            'ResNet50': ResNet50,\n",
    "            'InceptionV3': InceptionV3,\n",
    "        }\n",
    "#         if strs['MO'].startswith('simple,'):\n",
    "#             fields = strs['MO'].split(',')[1:]\n",
    "#             model_map[strs['MO']] = simple_model_factory(int(fields[0]), float(fields[1]), float(fields[2]))\n",
    "        self.backbone = model_map[strs['MO']]\n",
    "    \n",
    "    def describe(self):\n",
    "        return self.serialize().replace(' ', \"\\n\").replace(\":\", \": \")\n",
    "    \n",
    "    def make_backbone(self, with_reg=False):\n",
    "        from keras.models import Model\n",
    "        from keras.applications.inception_v3 import InceptionV3\n",
    "        from keras.applications.resnet50 import ResNet50\n",
    "        from keras.applications.vgg16 import VGG16\n",
    "        \n",
    "        def add_model_regularization(model, activity_regularizer_l1, kernel_regularizer_l2):\n",
    "            '''Assign regularization to layers, save the model structure and reload it to make the losses contribute.'''\n",
    "            from keras.regularizers import l1_l2\n",
    "\n",
    "            for layer in model.layers: \n",
    "                if not layer.trainable or 'batch_norm' in layer.name:\n",
    "                    continue\n",
    "                if hasattr(layer, 'kernel_regularizer') and kernel_regularizer_l2:\n",
    "                    print(\"kernel_regularizer: \", layer.name)\n",
    "                    if 'kernel' in layer.weights[0].name:\n",
    "                        size = np.product(layer.weights[0].shape.as_list())\n",
    "                        if size:\n",
    "                            layer.kernel_regularizer = l1_l2(0, kernel_regularizer_l2 / size)\n",
    "                if hasattr(layer, 'activity_regularizer') and activity_regularizer_l1 and layer.name != 'predictions':\n",
    "                    print(\"activity_regularizer: \", layer.name)\n",
    "                    size = np.product(layer.get_output_shape_at(0)[1:])\n",
    "                    if size:\n",
    "                        layer.activity_regularizer = l1_l2(activity_regularizer_l1 / size, 0)\n",
    "\n",
    "            # Suspect this is where GPU memory leak is coming from\n",
    "            model_config = model.get_config()\n",
    "            model_weights = model.get_weights()\n",
    "            model = None\n",
    "            K.clear_session()\n",
    "            model = Model.from_config(model_config)\n",
    "            model.set_weights(model_weights)\n",
    "            return model\n",
    "\n",
    "\n",
    "        backbone_t = {\n",
    "            'VGG16': VGG16,\n",
    "            'ResNet50': ResNet50,\n",
    "            'InceptionV3': InceptionV3,\n",
    "        }[self.backbone]\n",
    "        model = backbone_t(\n",
    "            include_top=False,\n",
    "            weights=self.pretrained_weights,\n",
    "            input_shape=self.input_shape,\n",
    "            pooling=None\n",
    "        )\n",
    "        model.name = \"backbone\"\n",
    "        if with_reg:\n",
    "            model = add_model_regularization(model, self.activity_regularizer_l1, self.kernel_regularizer_l2)\n",
    "        return model\n",
    "\n",
    "    def make_head(self, feature_shape):\n",
    "        '''make sure base_model has include_top=False. If loss=None then it is determined.\n",
    "        We don't use batch norm incase the backbone doesn't utilise it. We use Dropout instead.'''\n",
    "        from keras.models import Model\n",
    "        import keras.layers as layers\n",
    "        from keras.layers.advanced_activations import PReLU\n",
    "        from keras.regularizers import l1_l2\n",
    "\n",
    "        if self.activation == None:\n",
    "            self.activation = \"sigmoid\" if self.caption_type == \"multi\" else \"softmax\"\n",
    "\n",
    "        features_input = layers.Input(shape=feature_shape, name='head_input')\n",
    "        x = layers.GlobalAveragePooling2D()(features_input)\n",
    "\n",
    "        for i in range(self.num_hidden_layers):\n",
    "            x = layers.Dropout(self.dropout)(x)\n",
    "            x = layers.Dense(\n",
    "                self.num_hidden_neurons,\n",
    "                kernel_regularizer=l1_l2(0, self.kernel_regularizer_l2),\n",
    "                activity_regularizer=l1_l2(self.activity_regularizer_l1, 0))(x)\n",
    "            x = PReLU()(x)\n",
    "            \n",
    "        x = layers.Dropout(self.dropout, name='class_logits')(x)\n",
    "        predictions = layers.Dense(\n",
    "            self.num_classes,\n",
    "            activation=self.activation,\n",
    "            kernel_regularizer=l1_l2(0, self.kernel_regularizer_l2),\n",
    "            kernel_initializer='uniform',\n",
    "            name='predictions')(x)\n",
    "        return Model(inputs=features_input, outputs=predictions, name='classify_head')\n",
    "    \n",
    "    def make_model(self, weights=None, backbone_layer=None, parallel=False):\n",
    "        from keras.models import Model, Sequential\n",
    "        self.model = None\n",
    "        K.clear_session()\n",
    "        add_reg = 'simple' not in self.model_name\n",
    "        backbone = self.make_backbone(with_reg=False)\n",
    "        if backbone_layer:\n",
    "            backbone = Model(backbone.input, backbone.get_layer(backbone_layer).output, name='backbone')\n",
    "        backbone.trainable = self.train_features\n",
    "        head = self.make_head(backbone.output.shape.as_list()[1:])\n",
    "        \n",
    "        self.model = Model(backbone.inputs, head(backbone(backbone.input)))\n",
    "        if weights:\n",
    "            self.model.set_weights(weights)\n",
    "#         else:\n",
    "#             freeze = []\n",
    "#             if self.pretrained_weights is None:\n",
    "#                 transfer_vgg(self.model)\n",
    "#                 freeze = [layer.name for layer in self.model.layers if 'conv' in layer.name.lower()][0:1]\n",
    "#             elif self.pretrained_weights == 'imagenet':\n",
    "#                 if self.feature_extractor.__name__ =='VGG16':\n",
    "#                     freeze = [layer.name for layer in self.model.layers if 'conv' in layer.name.lower()][0:3]\n",
    "#                 elif self.feature_extractor.__name__ =='ResNet50':\n",
    "#                     freeze = [layer.name for layer in self.model.layers if 'conv' in layer.name.lower()][0:3]\n",
    "#                 elif self.feature_extractor.__name__ =='InceptionV3':\n",
    "#                     freeze = [layer.name for layer in self.model.layers if 'conv' in layer.name.lower()][0:3]\n",
    "#             for name in freeze:\n",
    "#                 layer = self.model.get_layer(name=name)\n",
    "#                 print(\"locking\", layer.name)\n",
    "#                 layer.trainable = False\n",
    "        if parallel:\n",
    "            from keras.utils import multi_gpu_model\n",
    "            self.model2 = multi_gpu_model(self.model, gpus=2)#, cpu_merge=True, cpu_relocation=False)            \n",
    "    \n",
    "    def get_train_model(self):\n",
    "        return self.model2 or self.model\n",
    "            \n",
    "    def compile_model(self, opt_params=None):\n",
    "        if not opt_params:\n",
    "            opt_params={'optimizer': 'nadam'}\n",
    "        opt_params['loss'] = self.loss\n",
    "        self.get_train_model().compile(**opt_params, metrics=['binary_accuracy', 'categorical_accuracy'])\n",
    "        \n",
    "    def set_logdir(self, log_dir):\n",
    "        self.log_dir = log_dir\n",
    "        self.model_def_path = os.path.join(log_dir, \"model_def.json\")\n",
    "        self.model_weights_path = os.path.join(log_dir, \"model_weights.h5\")\n",
    "        self.model_info_path = os.path.join(log_dir, \"model.json\")\n",
    "        self.model_plot_path = os.path.join(log_dir, \"precision-recall.png\")\n",
    "        \n",
    "    def setup_callbacks(self, schedule=None, hist=False, grads=False):\n",
    "        from keras.callbacks import TerminateOnNaN, ModelCheckpoint, EarlyStopping\n",
    "        from abyss_deep_learning.keras.classification import PRTensorBoard\n",
    "        \n",
    "        models_dir = os.path.join(self.log_dir, \"models\")\n",
    "        !mkdir -p \"$models_dir\"\n",
    "        best_path = os.path.join(models_dir, \"best.{epoch:03d}-{val_loss:.4f}.h5\")\n",
    "        self.callbacks = [\n",
    "            TerminateOnNaN(),\n",
    "            ModelCheckpoint(\n",
    "                best_path, monitor='val_loss', verbose=1,\n",
    "                save_best_only=True, save_weights_only=True, mode='auto', period=1),\n",
    "            PRTensorBoard(\n",
    "                log_dir=self.log_dir, \n",
    "                histogram_freq=(hist or 0),\n",
    "                batch_size=10,\n",
    "                write_graph=False,\n",
    "                write_grads=grads,\n",
    "                write_images=False,\n",
    "                embeddings_freq=(hist or 0),\n",
    "                embeddings_layer_names=['class_logits/cond/Merge:0'],\n",
    "                embeddings_metadata='metadata.tsv',\n",
    "                embeddings_data=VAL_DATA[0]\n",
    "            ),\n",
    "                EarlyStopping(\n",
    "                    monitor='val_loss', min_delta=0.0, patience=50, verbose=1, mode='auto'),\n",
    "    #         clr_callback.CyclicLR(base_lr=1e-4, max_lr=0.1, min_lr=0.01, step_size=(2*steps_per_epoch))\n",
    "            \n",
    "        ]\n",
    "        if schedule:\n",
    "            self.callbacks.append(schedule)\n",
    "\n",
    "    def go(self, epochs, initial_epoch=0, val_data=None):\n",
    "        from abyss_deep_learning.keras.utils import batching_gen\n",
    "        \n",
    "        steps_per_epoch = len(DATASET['train'].data_ids) // self.batch_size\n",
    "        steps_per_epoch_val = VAL_DATA[0].shape[0] // self.batch_size \n",
    "        print(\"Steps per epoch:\", steps_per_epoch)\n",
    "        print(\"Steps per steps_per_epoch_val:\", steps_per_epoch_val)\n",
    "\n",
    "        train_gen = ARGS['pipeline'](DATASET['train'].generator(\n",
    "            shuffle_ids=True, endless=True), aug_config=ARGS['augmentation'])\n",
    "        common = {\n",
    "            \"class_weight\": DATASET['train'].class_weights,\n",
    "            \"callbacks\": self.callbacks,\n",
    "            \"epochs\": epochs,\n",
    "            \"verbose\": 1,\n",
    "            \"initial_epoch\": initial_epoch,\n",
    "        }\n",
    "\n",
    "        self.history = self.get_train_model().fit_generator(\n",
    "            batching_gen(train_gen, batch_size=self.batch_size),\n",
    "#             validation_data=batching_gen(\n",
    "#                 data_source('val'), batch_size=exp.batch_size),\n",
    "            validation_data=val_data,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            validation_steps=steps_per_epoch_val,\n",
    "            workers=10,\n",
    "            **common)\n",
    "        return self.history\n",
    "\n",
    "    def evaluate_model(self, test_data, thresh=0.5):\n",
    "        def multi_label_decision(y_true, y_pred):\n",
    "            return (y_true > thresh) == (y_pred > thresh)\n",
    "        def single_label_decision(y_true, y_pred):\n",
    "            return np.argmax(y_true, axis=-1) == np.argmax(y_pred, axis=-1)\n",
    "        decision_function = single_label_decision if caption_type == 'single' else multi_label_decision\n",
    "\n",
    "        Y_true = test_data[1]\n",
    "        Y_pred = self.get_train_model().predict(test_data[0])\n",
    "        TP = decision_function(Y_true, Y_pred)\n",
    "        acc = np.count_nonzero(TP) / TP.size\n",
    "\n",
    "        print(\"Test using {:d} samples:\".format(len(test_data[0])))\n",
    "        print(\"accuracy\", acc)\n",
    "        return Y_true, Y_pred, TP\n",
    "    \n",
    "\n",
    "    def save_model(\n",
    "            self, class_map_r, prediction_type,\n",
    "            test_metrics=None, description=\"\"):\n",
    "        if self.model == None:\n",
    "            return\n",
    "        import json\n",
    "        from abyss.utils import JsonNumpyEncoder\n",
    "        \n",
    "        def merged(a, b):\n",
    "            merged = dict(a)\n",
    "            merged.update(b)\n",
    "            return merged\n",
    "\n",
    "        model_info = {\n",
    "            \"name\": self.serialize(),\n",
    "            \"description\": description,\n",
    "            \"weights\": self.model_weights_path,\n",
    "            \"prediction_type\": ARGS['caption_type'],\n",
    "            \"model\": self.model_def_path,\n",
    "            \"classes\": class_map_r,\n",
    "            \"architecture\": {\n",
    "                \"backbone\": \"inceptionv3\", # TODO IMPORTANT\n",
    "                \"logit_activation\": self.model.get_layer(\"classify_head\").get_layer(\"predictions\").activation.__name__,\n",
    "                \"input_shape\": self.input_shape\n",
    "            }\n",
    "\n",
    "        }\n",
    "        if self.history:\n",
    "            model_info['metrics'] = {\n",
    "                \"loss_function\": str(self.history.model.loss),\n",
    "                \"train\": merged(\n",
    "                    self.history.history,\n",
    "                    {\n",
    "                        \"epoch\": self.history.epoch,\n",
    "                        \"params\": self.history.params\n",
    "                    })\n",
    "            }\n",
    "            if test_metrics:\n",
    "                model_info['metrics']['test'] = test_metrics\n",
    "\n",
    "        print(\"Writing model def to \" + self.model_def_path)\n",
    "        with open(self.model_def_path, \"w\") as file:\n",
    "            file.write(self.model.to_json())\n",
    "\n",
    "        print(\"Writing model weights to \" + self.model_weights_path)\n",
    "        self.model.save_weights(self.model_weights_path)\n",
    "\n",
    "        print(\"Writing model info to \" + self.model_info_path)\n",
    "        with open(self.model_info_path, \"w\") as file:\n",
    "            file.write(json.dumps(model_info, cls=JsonNumpyEncoder))\n",
    "\n",
    "    \n",
    "    def display_performance(self, Y_true, Y_pred, TP):\n",
    "        from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "        precision = dict()\n",
    "        recall = dict()\n",
    "        average_precision = dict()\n",
    "        for i in range(self.num_classes):\n",
    "            precision[i], recall[i], _ = precision_recall_curve(Y_true[:, i],\n",
    "                                                                Y_pred[:, i])\n",
    "            average_precision[i] = average_precision_score(Y_true[:, i], Y_pred[:, i])\n",
    "\n",
    "        # A \"micro-average\": quantifying score on all classes jointly\n",
    "        precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(Y_true.ravel(),\n",
    "            Y_pred.ravel())\n",
    "        average_precision[\"micro\"] = average_precision_score(Y_true, Y_pred,\n",
    "                                                             average=\"micro\")\n",
    "        print('Average precision score, micro-averaged over all classes: {0:0.2f}'\n",
    "              .format(average_precision[\"micro\"]))\n",
    "\n",
    "        z = np.all((Y_pred > 0.5) == Y_true, axis=1)\n",
    "        acc = np.count_nonzero(z) / z.size\n",
    "        print(\"exact accuracy\", acc)\n",
    "        z = ((Y_pred > 0.5) == Y_true)\n",
    "        acc = np.count_nonzero(z) / z.size\n",
    "        print(\"binary accuracy\", acc)\n",
    "\n",
    "        # setup plot details\n",
    "        colors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal'])\n",
    "\n",
    "        plt.figure(figsize=(8, 10))\n",
    "        f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "        lines = []\n",
    "        labels = []\n",
    "        for f_score in f_scores:\n",
    "            x = np.linspace(0.01, 1)\n",
    "            y = f_score * x / (2 * x - f_score)\n",
    "            l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "            plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "\n",
    "        lines.append(l)\n",
    "        labels.append('iso-f1 curves')\n",
    "        l, = plt.plot(recall[\"micro\"], precision[\"micro\"], color='gold', lw=2)\n",
    "        lines.append(l)\n",
    "        labels.append('micro-average Precision-recall (area = {0:0.2f})'\n",
    "                      ''.format(average_precision[\"micro\"]))\n",
    "\n",
    "        for i, color in zip(range(self.num_classes), colors):\n",
    "            l, = plt.plot(recall[i], precision[i], color=color, lw=2)\n",
    "            lines.append(l)\n",
    "            labels.append('{0} (area = {1:0.2f})'\n",
    "                          ''.format(ARGS['caption_map'].inv[i], average_precision[i]))\n",
    "\n",
    "        fig = plt.gcf()\n",
    "        fig.subplots_adjust(bottom=0.25)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title('Micro Average Precision vs. Recall')\n",
    "        plt.legend(lines, labels, loc=(0, -.4), prop=dict(size=14))\n",
    "        plt.show()\n",
    "        plt.savefig(self.model_plot_path, dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection and training\n",
    "You may have to change these callbacks to suit the dataset and model\n",
    "Note that calculating gradients and histogram on large layered networks (resnet and inception) takes a long time (5 minutes per epoch calculated) so you may only want to do this infrequently or not at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rates for VGG given output layer:\n",
    "* VGG16:block4_conv3 0x0: 5e-3\n",
    "* VGG16:block3_conv3 0x0: 1e-3\n",
    "* VGG16:block3_conv3 1x512: 5e-4 (rough) 2.5e-4 (smooth but worse)\n",
    "* VGG16:block3_conv3 2x512: 5e-4 (rough) 2.5e-4 (smooth but worse)\n",
    "\n",
    "Regularization:\n",
    "* L2 kernel doesn't really kick in till > 1e-2\n",
    "* L1 activity doesn't really kick in till > 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_experiment(val_data):\n",
    "    global exp\n",
    "    search_output_dir = \"/data/log/cnn/fd/large-fromCF/thursday2\" # Change this output dir.\n",
    "    num_epochs_train = [20, 40]\n",
    "    learning_rate_multipliers = [1, 0.02]\n",
    "    model_instance = None\n",
    "    # model_instance = \"ID:4704 DS:alltogether BS:5 AC:sigmoid MO:simple-2 HE:0x32 TR:all FT:None LF:binary_crossentropy LR:3.7e-04 AR:8.6e-02 KR:2.9e-02 DO:0.00 PO:avg CW:0=2.10,1=0.58,2=0.59,3=0.49,4=1.41,5=1.66,6=3.74,7=1.98 method:step\"\n",
    "    history_data = {}\n",
    "\n",
    "    def lr_schedule_exp(epoch, base_lr=1e-3, gamma=0.98):\n",
    "        return base_lr * gamma ** epoch\n",
    "\n",
    "    def lr_schedule_step(epoch, base_lr, steps):\n",
    "        lr = base_lr\n",
    "        for step_epoch, lr_mult in steps.items():\n",
    "            if epoch >= step_epoch:\n",
    "                lr *= lr_mult\n",
    "        return lr\n",
    "\n",
    "    def init_schedule():\n",
    "        from keras.callbacks import ReduceLROnPlateau, LearningRateScheduler\n",
    "        ### Pick one\n",
    "        schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=20, cooldown=10, verbose=1)\n",
    "    #     schedule = LearningRateScheduler(\n",
    "    # #         lambda epoch, lr: lr_schedule_exp(epoch, base_lr=5e-3, gamma=0.98) # Exponential decay\n",
    "    #         lambda epoch, lr: lr_schedule_step(epoch, base_lr=5e-3, steps={1: 0.1, 5: 0.1}) # Step decay\n",
    "    #     )\n",
    "    #     schedule = None\n",
    "        return schedule\n",
    "\n",
    "    backbone_layers = [\n",
    "        (\"block2_conv2\", 5e-4),\n",
    "        (\"block3_conv1\", 1e-3),\n",
    "        (\"block3_conv2\", 1e-3),\n",
    "        (\"block3_conv3\", 1e-3),\n",
    "        (\"block4_conv1\", 5e-3),\n",
    "        (\"block4_conv2\", 5e-3),\n",
    "        (\"block4_conv3\", 5e-3),\n",
    "        (\"block5_conv1\", 1e-2),\n",
    "        (\"block5_conv2\", 1e-2),\n",
    "        (\"block5_conv3\", 1e-2),\n",
    "    ]\n",
    "    for attempt_no, (backbone_layer, lr) in enumerate(backbone_layers):\n",
    "        K.clear_session()\n",
    "        exp = Experiment(ARGS['data']['name'], ARGS['image_dims'])\n",
    "        exp.class_weights = DATASET['train'].class_weights\n",
    "        exp.batch_size = 6 # keep this divisible by the amount of GPUs\n",
    "        if model_instance:  # If loading network structure from model_instance (not weights)\n",
    "            exp.deserialize(model_instance)\n",
    "            exp.id = str(np.random.randint(0, 999))\n",
    "        else:\n",
    "            exp.random_init()\n",
    "            exp.pretrained_weights = 'imagenet'\n",
    "            exp.train_features = False\n",
    "            exp.dropout = 0.5\n",
    "            exp.pool = 'avg'\n",
    "            exp.num_hidden_layers = 1\n",
    "            exp.num_hidden_neurons = 256\n",
    "            exp.learning_rate = lr #10**np.random.uniform(-7, -2)\n",
    "            exp.activity_regularizer_l1 = 1e-4 #10**np.random.uniform(-4, 1)\n",
    "            exp.kernel_regularizer_l2 = 1e-4 #10**np.random.uniform(-4, 1)\n",
    "            exp.backbone = 'VGG16' #simple_model_factory(4, exp.activity_regularizer_l1, exp.kernel_regularizer_l2)\n",
    "        experiment_name = exp.serialize() + \" BBL:\" + backbone_layer #+ \" method:plateau\"\n",
    "        print('=' * 80)\n",
    "        print(exp.describe())\n",
    "\n",
    "        log_dir = os.path.join(search_output_dir, experiment_name)\n",
    "        exp.set_logdir(log_dir)\n",
    "\n",
    "        print(experiment_name)\n",
    "        print(log_dir)\n",
    "        try:\n",
    "            current_epoch = 0\n",
    "            # Initialize heads with high LR then train whole model with low LR\n",
    "            for i, (epochs, lr_mult) in enumerate(zip(num_epochs_train, learning_rate_multipliers)):\n",
    "                weights = None if i == 0 else exp.model.get_weights()\n",
    "    #             exp.train_features = False#i > 0\n",
    "                exp.make_model(weights, backbone_layer=backbone_layer, parallel=ARGS['use_parallel'])\n",
    "                if i > 0:\n",
    "                    backbone = exp.get_train_model().get_layer('backbone')\n",
    "                    backbone.trainable = True\n",
    "                exp.compile_model()\n",
    "                exp.setup_callbacks(schedule=init_schedule(), hist=5, grads=True)\n",
    "                exp.get_train_model().summary()\n",
    "                raise ValueError(\"DED\")\n",
    "\n",
    "                K.set_value(exp.get_train_model().optimizer.lr, lr * lr_mult)\n",
    "                print(\"Training: {:d} layers\".format(len([1 for layer in exp.model.layers if layer.trainable])))\n",
    "                exp.model.summary()\n",
    "                exp.model.get_layer(\"backbone\").summary()\n",
    "                exp.model.get_layer(\"classify_head\").summary()\n",
    "                history_data[experiment_name] = exp.go(\n",
    "                    current_epoch + epochs,\n",
    "                    initial_epoch=current_epoch,\n",
    "                    val_data=val_data)\n",
    "                current_epoch += epochs\n",
    "        except KeyboardInterrupt:\n",
    "            history_data[experiment_name] = None\n",
    "        except:\n",
    "            raise\n",
    "        finally:\n",
    "            exp.save_model(\n",
    "                class_map_r=dict(ARGS['caption_map'].inv),\n",
    "                prediction_type=ARGS['caption_type'],\n",
    "                test_metrics=None,\n",
    "                description=\"Test model for 5 FDs\"\n",
    "            )\n",
    "try_experiment(val_data=VAL_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.model.get_layer(\"classify_head/class_logits\").output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.display_performance(*exp.evaluate_model(test_data, thresh=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"Stop run all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if history_data:\n",
    "    with open(os.path.join(search_output_dir, \"history-{:d}epoch.pkl\".format(num_epochs_train)), \"wb\") as file:\n",
    "        pickle.dump({key: history.history for key, history in history_data.items()}, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Should you need to load this pkl:\n",
    "# # with open(os.path.join(search_output_dir, \"history-100epoch.pkl\"), \"rb\") as file:\n",
    "# #     history = pickle.load(file)\n",
    "\n",
    "# lrs = [key[1] for key, val  in sorted(history_data.items(), key=lambda x: x[0])]\n",
    "# val_loss = [history.history['val_loss'][-1] for key, history  in sorted(history_data.items(), key=lambda x: x[0])]\n",
    "# loss = [history.history['loss'][-1] for lr, history  in sorted(history_data.items(), key=lambda x: x[0])]\n",
    "# acc = [history.history['binary_accuracy'][-1] for lr, history  in sorted(history_data.items(), key=lambda x: x[0])]\n",
    "# val_acc = [history.history['val_binary_accuracy'][-1] for lr, history  in sorted(history_data.items(), key=lambda x: x[0])]\n",
    "\n",
    "# plt.figure()\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.semilogx(lrs, loss, '.b', label='loss')\n",
    "# plt.semilogx(lrs, val_loss, '.r', label='val_loss')\n",
    "# plt.legend()\n",
    "# plt.title(\"Loss Vs. LR (100 Epoch)\")\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.semilogx(lrs, acc, '.b', label='binary_accuracy')\n",
    "# plt.semilogx(lrs, val_acc, '.r', label='val_binary_accuracy')\n",
    "# plt.legend()\n",
    "# plt.title(\"Accuracy Vs. LR (100 Epoch)\")\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"/data/log/cnn/fd/large-fromCF/thursday/ID:3379 DS:alltogether-unique BS:7 AC:sigmoid MO:InceptionV3 HE:0x0 TR:all FT:imagenet LF:binary_crossentropy LR:2.7e-05 AR:1.8e-05 KR:3.1e-05 DO:0.00 PO:avg CW:0=3.05,1=0.54,2=1.02,3=0.54,4=4.57,5=0.76,6=2.29 method:plateau\"\n",
    "exp.model_def_path = os.path.join(log_dir, \"model_def.json\")\n",
    "exp.model_weights_path = os.path.join(log_dir, \"model_weights.h5\")\n",
    "exp.model_info_path = os.path.join(log_dir, \"model.json\")\n",
    "exp.model_plot_path = os.path.join(log_dir, \"precision-recall.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_instance = \"ID:3379 DS:alltogether-unique BS:7 AC:sigmoid MO:InceptionV3 HE:0x0 TR:all FT:imagenet LF:binary_crossentropy LR:2.7e-05 AR:1.8e-05 KR:3.1e-05 DO:0.00 PO:avg CW:0=3.05,1=0.54,2=1.02,3=0.54,4=4.57,5=0.76,6=2.29 method:plateau\"\n",
    "# exp = Experiment(dataset_name, image_dims)\n",
    "# exp.deserialize(model_instance)\n",
    "# exp.make_model(parallel=True)\n",
    "# exp.get_train_model().load_weights(\"/data/log/cnn/fd/large-fromCF/thursday/ID:3379 DS:alltogether-unique BS:7 AC:sigmoid MO:InceptionV3 HE:0x0 TR:all FT:imagenet LF:binary_crossentropy LR:2.7e-05 AR:1.8e-05 KR:3.1e-05 DO:0.00 PO:avg CW:0=3.05,1=0.54,2=1.02,3=0.54,4=4.57,5=0.76,6=2.29 method:plateau/models/best.040-0.2809.h5\")\n",
    "# # exp.id = \"3379\"\n",
    "# exp.set_logdir(os.path.join(search_output_dir, 'fritatta'))\n",
    "exp.display_performance(*exp.evaluate_model(test_data, thresh=0.5))\n",
    "# exp.history = dict()\n",
    "# exp.save_model(\n",
    "#     caption_map_r,\n",
    "#     prediction_type=caption_type, test_metrics=None, description=\"Test model for 5 FDs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue training specific model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "K.clear_session()\n",
    "search_output_dir = \"/data/log/cnn/fd/large-fromCF/thursday\" # Change this output dir.\n",
    "model_instance = \"ID:3379 DS:alltogether-unique BS:7 AC:sigmoid MO:InceptionV3 HE:0x0 TR:all FT:imagenet LF:binary_crossentropy LR:2.7e-05 AR:1.8e-05 KR:3.1e-05 DO:0.00 PO:avg CW:0=3.05,1=0.54,2=1.02,3=0.54,4=4.57,5=0.76,6=2.29 method:plateau\"\n",
    "model_best_weight = \"model_weights.h5\"\n",
    "\n",
    "### Don't set below\n",
    "exp = Experiment(dataset_name, image_dims)\n",
    "exp.deserialize(model_instance)\n",
    "exp.id = \"{:d}\".format(np.random.randint(0, 999))\n",
    "exp.dropout = 0.5\n",
    "\n",
    "experiment_name = exp.serialize()\n",
    "\n",
    "model_weights_in_path = os.path.join(search_output_dir, model_instance, model_best_weight)\n",
    "log_dir = os.path.join(search_output_dir, model_instance, \"continued\", experiment_name)\n",
    "best_path = os.path.join(log_dir, \"models/best.{epoch:03d}-{val_loss:.4f}.h5\")\n",
    "model_def_path = os.path.join(log_dir, \"model_def.json\")\n",
    "model_info_path = os.path.join(log_dir, \"model.json\")\n",
    "model_plot_path = os.path.join(log_dir, \"precision-recall.png\")\n",
    "\n",
    "print(model_weights_in_path)\n",
    "if os.path.exists(model_weights_in_path):\n",
    "    !mkdir -p \"$log_dir/models\"\n",
    "else:\n",
    "    raise OSError(\"path does not exist\")\n",
    "    \n",
    "print(\"loading\")\n",
    "print(os.path.join(search_output_dir, model_instance, \"model.json\"))\n",
    "exp.model = Inference(os.path.join(search_output_dir, model_instance, \"model.json\")).model\n",
    "# base_model = p.feature_extractor(\n",
    "#     include_top=False, weights=p.pretrained_weights, input_shape=image_dims)\n",
    "# model = create_new_head(\n",
    "#     base_model, p.num_classes, p.caption_type, p, \n",
    "#     opt_params={'optimizer': 'nadam'}\n",
    "# )\n",
    "exp.model.load_weights(model_weights_in_path)\n",
    "# model = add_model_regularization(model, params.kernel_regularizer_l2, params.activity_regularizer_l1)\n",
    "\n",
    "# for layer in model.layers:\n",
    "#     layer.trainable = True\n",
    "\n",
    "exp.model.compile( # TODO, load this from JSON, manually change this if you are doing single label\n",
    "    'nadam',\n",
    "    loss=exp.loss,\n",
    "    metrics=['binary_accuracy', 'categorical_accuracy'])\n",
    "\n",
    "print(experiment_name)\n",
    "print(log_dir)\n",
    "print(\"Training: {:d} layers\".format(len([1 for layer in exp.model.layers if layer.trainable])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epoch = 2\n",
    "num_epoch = 200 # cumulative with initial_epoch\n",
    "class_weights = None # Can't currently resume training with imbalance data #TODO\n",
    "new_learning_rate = 1e-6\n",
    "### Pick one schedule\n",
    "schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.85, patience=3, cooldown=0, verbose=1)\n",
    "# schedule = lambda epoch, lr: lr_schedule_exp(epoch, base_lr=5e-5, gamma=0.98) # Exponential decay\n",
    "# schedule = None\n",
    "\n",
    "\n",
    "#### Don't set below\n",
    "\n",
    "K.set_value(exp.model.optimizer.lr, new_learning_rate)\n",
    "callbacks = setup_callbacks(log_dir, schedule=schedule, hist=2, grads=True)\n",
    "history_data[experiment_name] = go(exp.model, callbacks, num_epoch, exp.class_weights, initial_epoch=initial_epoch)\n",
    "\n",
    "(Y_true, Y_pred, TP) = evaluate_model(exp.model, test_data, thresh=0.5)\n",
    "display_performance(Y_true, Y_pred, TP)\n",
    "\n",
    "save_model(\n",
    "    exp.model, name=experiment_name,\n",
    "    class_map_r=dict(ARGS['caption_map'].inv), prediction_type=ARGS['caption_type'],\n",
    "    model_weights_path=model_weights_path, model_def_path=model_def_path, model_info_path=model_info_path,\n",
    "    test_metrics=None, history=history_data[experiment_name],\n",
    "    description=\"Test model for 5 FDs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LrSearch():\n",
    "    def __init__(self, instance_str):\n",
    "        self.model_init_w = None\n",
    "        self.model_instance = instance_str\n",
    "        self.exp = None\n",
    "        self.history = {}\n",
    "        \n",
    "    def new_model(self):\n",
    "        K.clear_session()\n",
    "        self.exp = Experiment(dataset_name, image_dims)\n",
    "        self.exp.deserialize(self.model_instance)\n",
    "        self.exp.make_model(self.model_init_w, parallel=True)\n",
    "        self.exp.compile_model()\n",
    "#         self.exp.setup_callbacks(schedule=init_schedule(), hist=0)\n",
    "        print(\"Trainable layers: {:d}\".format(sum([layer.trainable for layer in self.exp.model.layers])))\n",
    "        if self.model_init_w is None:\n",
    "            self.model_init_w = self.exp.model.get_weights()\n",
    "        \n",
    "        \n",
    "    def go(self, epochs, num_steps=10, lr_range=(-9, 0)):\n",
    "        for base_lr in 10 ** np.random.uniform(*lr_range, size=num_steps):\n",
    "            print(\"Learning rate = {:.3e}\".format(base_lr))\n",
    "            self.new_model()\n",
    "            K.set_value(self.exp.get_train_model().optimizer.lr, base_lr)\n",
    "            self.history[base_lr] = self.exp.go(epochs)\n",
    "        return self.history\n",
    "    \n",
    "    def plot(self):\n",
    "        df = []\n",
    "        for lr, h in self.history.items():\n",
    "            metrics = np.vstack([np.array(h.history['binary_accuracy']), np.array(h.history['loss'])])\n",
    "            diff = (metrics[:, -1] - metrics[:, 0])[np.newaxis]\n",
    "            metrics = np.vstack([metrics[:, -1], diff]).ravel()\n",
    "            metrics = np.array([lr] + metrics.tolist())\n",
    "            dfh = pd.DataFrame(\n",
    "                    data=pd.Series(\n",
    "                        data=metrics,\n",
    "                        index=['lr', 'binary_accuracy', 'loss', 'diff_acc', 'diff_loss'])\n",
    "                ).T.set_index('lr')\n",
    "            df.append(dfh)\n",
    "        df = pd.concat(df).sort_index()\n",
    "        df.plot(logx=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_instance = \"ID:5740 DS:alltogether BS:4 AC:sigmoid MO:VGG16 HE:0x64 TR:all FT:imagenet LF:binary_crossentropy LR:5.1e-04 AR:1.0e-06 KR:1.0e-06 DO:0.00 PO:avg CW:0=4.26,1=0.64,2=0.49,3=7.11 method:plateau\"\n",
    "search = LrSearch(model_instance)\n",
    "# Hexpo: 4.5e-4 (1e-4 to 6e-4)\n",
    "# VGG Heads only: 4.4e-5 to 2e-2\n",
    "# VGG Imagenet: 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.go(5, num_steps=4, lr_range=(-6, -4))\n",
    "search.plot()\n",
    "# del search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below is untested with new changes, don't use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_gradients(model):\n",
    "#     \"\"\"Get the gradients of the loss with respect to the weights.\"\"\"\n",
    "#     weights = [tensor for tensor in model.trainable_weights \n",
    "#                if model.trainable_weights]\n",
    "#     return weights, model.optimizer.get_gradients(model.total_loss, weights)\n",
    "\n",
    "        \n",
    "# def hamming_loss(y_true, y_pred):\n",
    "#     return K.mean(y_true * (1 - y_pred) + (1 - y_true) * y_pred)\n",
    "\n",
    "\n",
    "# def check_gradients(model):\n",
    "#     grad_test = None\n",
    "#     for image, label in train_gen:\n",
    "#         grad_test = (image, label)\n",
    "#         break\n",
    "#     rates = []\n",
    "#     weights, grads = get_gradients(model)\n",
    "#     feed_dict = {\n",
    "#         \"class_logits_sample_weights:0\": np.ones(2),\n",
    "#         \"input_1:0\": grad_test[0][np.newaxis, ...],\n",
    "#         \"class_logits_target:0\": grad_test[1][np.newaxis, ...]\n",
    "#     }\n",
    "#     for i, (w, g) in enumerate(zip(weights, grads)):\n",
    "#         if 'bias' in w.name:\n",
    "#             continue\n",
    "#         grad_norm = np.linalg.norm(g.eval(feed_dict, K.get_session()))\n",
    "#         weight_norm = np.linalg.norm(w.eval(K.get_session()))\n",
    "#         rate = grad_norm / weight_norm\n",
    "#         rates.append(rate)\n",
    "#     if np.mean(rates) < 5e-4 or np.mean(rates) > 3e-1: # These values change with network structure\n",
    "#         print(\"Bad gradients ({:.3e}).\".format(np.mean(rates)))\n",
    "#         return False\n",
    "#     return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer=model.layers[-3]\n",
    "# print(layer)\n",
    "# for weight in layer.weights:\n",
    "#     weight.initializer.run(session=K.get_session())\n",
    "# # w = layer.get_weights()\n",
    "# # plt.figure()\n",
    "# # plt.hist(w[0].ravel(), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # More training\n",
    "# K.set_value(model.optimizer.lr, 1e-4)\n",
    "\n",
    "# go(300, class_weights, initial_epoch=200)\n",
    "# (Y_true, Y_pred, TP) = evaluate_model(model, test_data, thresh=0.5)\n",
    "# display_performance(Y_true, Y_pred, TP)\n",
    "\n",
    "# # save_model(\n",
    "# #     model, name=experiment_name + \"-second\",\n",
    "# #     class_map_r=caption_map_r, prediction_type=caption_type,\n",
    "# #     model_weights_path=model_weights_path, model_def_path=model_def_path, model_info_path=model_info_path,\n",
    "# #     test_metrics=None, history=history_data[experiment_name],\n",
    "# #     description=\"Test model for 5 FDs\"\n",
    "# # )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from herbicide.utils import vis_square\n",
    "TP_mask = np.logical_and.reduce(TP, axis=1)\n",
    "right = test_data[0][TP_mask]\n",
    "wrong = test_data[0][~TP_mask]\n",
    "wrong.shape\n",
    "plt.figure()\n",
    "vis_square(wrong)\n",
    "plt.title(\"Incorrectly Predicted\")\n",
    "plt.figure()\n",
    "vis_square(right)\n",
    "plt.title(\"Correctly Predicted\")\n",
    "\n",
    "# Binary coded the labels then count them wrt TP/FP\n",
    "print(\"num labels\", test_data[1].sum(axis=0))\n",
    "coded = np.sum(test_data[1][~TP_mask] * 2 ** np.arange(num_classes)[::-1], axis=1).astype(int)\n",
    "print(\"binary coded class error count:\", dict(sorted(Counter(coded).items(), key=lambda x: x[0])))\n",
    "coded = np.sum(test_data[1][TP_mask] * 2 ** np.arange(num_classes)[::-1], axis=1).astype(int)\n",
    "print(\"binary coded class correct count:\", dict(sorted(Counter(coded).items(), key=lambda x: x[0])))\n",
    "print(Y_pred[TP_mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def learning_curve(dataset, lr, steps, val_data, log_dir):\n",
    "#     def save_model(path):\n",
    "#         print(\"Saving\", path)\n",
    "#         os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "#         model.save_weights(path)\n",
    "#     def setup_callbacks():\n",
    "#         return [\n",
    "# #                 ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, cooldown=5, verbose=1),\n",
    "# #                 ModelCheckpoint(\n",
    "# #                     model_best_path, monitor='val_loss', verbose=1,\n",
    "# #                     save_best_only=True, save_weights_only=True, mode='auto', period=1),\n",
    "# #                 ModelCheckpoint(\n",
    "# #                     best_path, monitor='val_loss', verbose=1,\n",
    "# #                     save_best_only=False, save_weights_only=True, mode='auto', period=50),\n",
    "#                 PRTensorBoard(\n",
    "#                     log_dir=model_log_dir,\n",
    "#                     histogram_freq=0,\n",
    "#                     batch_size=batch_size,\n",
    "#                     write_graph=False,\n",
    "#                     write_grads=False,\n",
    "#                     write_images=False),\n",
    "#         #         EarlyStopping(\n",
    "#         #             monitor='val_loss', min_delta=0.0, patience=40, verbose=1, mode='auto')\n",
    "#         ]\n",
    "#     def create_new_model(load_base=False):\n",
    "#         clear_session()\n",
    "#         model = create_new_head(\n",
    "#             InceptionV3(include_top=False, weights='imagenet', input_shape=image_dims),\n",
    "#             num_classes, caption_type, opt_params={'optimizer': Nadam()},\n",
    "#             class_weights=None, train_features=False, l2_reg=None)\n",
    "#         if load_base:\n",
    "#             print(\"Loading base model\")\n",
    "#             model.load_weights(base_model_path, by_name=True)\n",
    "#         return model\n",
    "\n",
    "#     def train():\n",
    "#         print(\"Training\")\n",
    "#         K.set_value(model.optimizer.lr, lr)\n",
    "#         history[subset_size] = model.fit_generator(\n",
    "#             batching_gen(gen, batch_size=batch_size),\n",
    "#             validation_data=tuple(val_data),\n",
    "#             steps_per_epoch=(subset_size // batch_size),\n",
    "#             validation_steps=steps_per_epoch_val,\n",
    "#             class_weight=model_class_weights,\n",
    "#             callbacks=setup_callbacks(), \n",
    "#             epochs=50,\n",
    "#             verbose=1)\n",
    "#     model_class_weights = None\n",
    "#     model = None\n",
    "#     model_path = None\n",
    "#     image_ids = [image['id'] for image in dataset.imgs.values()]\n",
    "#     np.random.shuffle(image_ids)\n",
    "#     num_images = len(image_ids)\n",
    "#     print(\"num_images\", num_images)\n",
    "#     history = {}\n",
    "#     base_model_path = os.path.join(log_dir, \"base\", \"weights.h5\")\n",
    "#     model_path = base_model_path\n",
    "#     for subset_size in np.linspace(0, num_images, steps + 1).astype(int):\n",
    "#         if subset_size > 0:\n",
    "#             imgIds = image_ids[:subset_size]\n",
    "#             gen = pipeline(\n",
    "#                 dataset.generator(shuffle_ids=False, imgIds=imgIds),\n",
    "#                 aug_config=None)\n",
    "#             model_class_weights = calc_class_weights(gen, dataset) # TODO\n",
    "\n",
    "#             model_path = os.path.join(log_dir, \"subset-of-{:d}/weights.h5\".format(subset_size))\n",
    "#             model_log_dir = os.path.dirname(model_path)\n",
    "#             model_best_path = os.path.join(log_dir, \"subset-of-{:d}/best.h5\".format(subset_size))\n",
    "#             os.makedirs(model_log_dir, exist_ok=True)\n",
    "\n",
    "#             print(\"learning curve(lr={:.3e}, size={:d})\".format(lr, subset_size))\n",
    "#             print(\"model_log_dir\", model_log_dir)\n",
    "#             print(\"training class weights\")\n",
    "#             print(model_class_weights)\n",
    "#         model = create_new_model(load_base=(subset_size > 0))\n",
    "#         if subset_size:\n",
    "#             train()\n",
    "#         save_model(model_path)\n",
    "#     return history\n",
    "\n",
    "# model = None\n",
    "# lr = 1e-5\n",
    "# learning_curve_dir = \"/data/log/cnn/fd/learning_curve_5--{:.2e}\".format(lr)\n",
    "# lc_history = learning_curve(coco_train, lr, 5, val_data, learning_curve_dir)\n",
    "# val_loss = np.array([(size, h.history['val_loss'][-1]) for size, h in lc_history.items()])\n",
    "# train_loss = np.array([(size, h.history['loss'][-1]) for size, h in lc_history.items()])\n",
    "# plt.figure()\n",
    "# plt.plot(train_loss[:, 0], train_loss[:, 1], 'b.')\n",
    "# plt.plot(val_loss[:, 0], val_loss[:, 1], 'r.')\n",
    "# plt.xlabel(\"Number of Training Samples\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.savefig(os.path.join(learning_curve_dir, \"plot.png\"), dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -R /data/log/cnn/fd/learning-curve/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = None\n",
    "# for images, labels in batching_gen(train_gen, batch_size=batch_size):\n",
    "#     print(images.shape, labels.shape)\n",
    "    \n",
    "#     pred = model.predict(images)\n",
    "#     print(labels)\n",
    "#     print(pred)\n",
    "#     print(K.eval(K.tf.losses.sigmoid_cross_entropy(labels, pred)))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for unique_label in np.unique(val_data[1], axis=0):\n",
    "#     unique_data = [val_data[0][i] for i in range(len(val_data[0])) if np.all(val_data[1][i] == unique_label)]\n",
    "#     num_data = len(unique_data)\n",
    "#     print(unique_label, num_data)\n",
    "#     plt.figure()\n",
    "#     vis_square(np.array(unique_data))\n",
    "#     plt.title(unique_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Update/Weight Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for i, (w, g) in enumerate(zip(weights, grads)):\n",
    "#     grad_norm = np.linalg.norm(g.eval(feed_dict, K.get_session()))\n",
    "#     weight_norm = np.linalg.norm(w.eval(K.get_session()))\n",
    "#     rate = grad_norm / weight_norm\n",
    "#     print(i, rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from herbicide.utils import vis_square\n",
    "for layer in model.layers:\n",
    "    if not layer.trainable_weights:\n",
    "        continue\n",
    "    for weight in layer.trainable_weights: #  Assumes FD is not trainable\n",
    "        if 'kernel' not in weight.name:\n",
    "            continue\n",
    "        print(weight.name)\n",
    "        value = K.eval(weight.value())\n",
    "        print(value.shape)\n",
    "    \n",
    "    plt.figure()\n",
    "    vis_square(value.transpose((3, 0, 1, 2)))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_stats = []\n",
    "for i, (image, caption) in enumerate(coco_train.generator(imgIds=balanced_image_ids_train)):\n",
    "    plt.figure()\n",
    "    plt.imshow(image)\n",
    "    plt.title(str(caption))\n",
    "    if i == 10:\n",
    "        break\n",
    "    caption_stats.append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
