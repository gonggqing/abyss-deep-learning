{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import os\n",
    "import sys\n",
    "from pprint import pprint\n",
    "\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from abyss_deep_learning.base.datasets import DatasetTaskBase\n",
    "from abyss_deep_learning.datasets.coco import (CocoDataset, CocoInterface,\n",
    "                                               ImageDatatype)\n",
    "from abyss_deep_learning.keras.tensorboard import ImprovedTensorBoard\n",
    "#augmentation_gen, jaccard_index\n",
    "from abyss_deep_learning.keras.utils import (initialize_conv_transpose2d,\n",
    "                                             lambda_gen, tiling_gen, skip_empty_gen)\n",
    "from abyss_deep_learning.utils import config_gpu, detile\n",
    "from crfrnn.crfrnn_model import get_crfrnn_model_def\n",
    "from keras import Model\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from keras.optimizers import Nadam\n",
    "from keras.utils import to_categorical\n",
    "from skimage.color import label2rgb\n",
    "from skimage.io import imread, imsave\n",
    "from skimage.transform import resize\n",
    "\n",
    "config_gpu([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### If you don't have the imagenet weights below it will auto download them\n",
    "if not os.path.exists(\"~/.keras/models/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\"):\n",
    "    from keras.applications.vgg16 import VGG16\n",
    "    vgg = VGG16(include_top=False)\n",
    "    del vgg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def _noop(*args):\n",
    "    return args if len(args) > 1 else args[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imgaug.parameters import Deterministic\n",
    "import imgaug.augmenters as iaa\n",
    "def augmentation_gen(gen, common_aug, image_aug, enable=True):\n",
    "    '''\n",
    "    Data augmentation for segmentation task.\n",
    "    A common augmentation list is applied to both images and masks and should not contain colour augmentation, \n",
    "    and should ensure order=0 is used for all geometric transforms.\n",
    "    An image augmentation list is then applied to only the image, this should contain no geometric augmentations.\n",
    "    '''\n",
    "    if enable:\n",
    "        common_seq = iaa.Sequential(common_aug)\n",
    "        image_seq = iaa.Sequential(image_aug)\n",
    "        for image, target in gen:\n",
    "            common_seq_det = common_seq.to_deterministic()\n",
    "            image_c = common_seq_det.augment_image(image)\n",
    "            masks_c = common_seq_det.augment_image(target)\n",
    "            yield image_c, masks_c\n",
    "    else:\n",
    "        yield from gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "def preprocess_data(image):\n",
    "    '''Transform the image before (possibly caching) and input to the network.'''\n",
    "#     image = resize(image, ARGS['image_dims'], preserve_range=True, mode='constant')\n",
    "    return preprocess_input(image.astype(ARGS['nn_dtype']), mode='tf')\n",
    "#     return image.astype(ARGS['nn_dtype'])\n",
    "\n",
    "def preprocess_targets(image):\n",
    "    '''Transform the mask before (possibly caching) and input to the network.'''\n",
    "#     image = resize(image, ARGS['image_dims'][0:2], preserve_range=True, mode='constant')\n",
    "    return image.astype(ARGS['nn_dtype'])\n",
    "\n",
    "def postprocess_data(image):\n",
    "    '''Inverse transform of preprocess_data, used when trying to visualize images out of the dataset.'''\n",
    "    return ((image + 1) * 127.5).astype(np.uint8)\n",
    "\n",
    "\n",
    "from abyss_deep_learning.datasets.translators import AnnotationTranslator\n",
    "class AnnotationMapper(AnnotationTranslator):\n",
    "        '''Transform COCO JSON annotations in any way you want. This one maps source to dest classes.'''\n",
    "        def __init__(self, class_map=None):\n",
    "            self.class_map = class_map\n",
    "#             self.num_classes = len(class_map)\n",
    "\n",
    "#         def filter(self, annotation):\n",
    "#             return (\n",
    "#                 'segmentation' in annotation \n",
    "#                 and annotation['annotation_type'] in ['poly']\n",
    "#                 and annotation['area'] > 30 ** 2)\n",
    "        \n",
    "        def filter(self, annotation):\n",
    "            return (\n",
    "                'segmentation' in annotation \n",
    "                and annotation['area'] > 30 ** 2)\n",
    "\n",
    "        def translate(self, annotation):\n",
    "            output = dict(annotation)\n",
    "            if self.class_map:\n",
    "                output['category_id'] = self.class_map[annotation['category_id']]\n",
    "            return annotation\n",
    "\n",
    "def pipeline(gen, aug_config=None):\n",
    "    '''The pipeline to run the dataset generator through.'''\n",
    "#         from abyss_deep_learning.keras.classification import augmentation_gen\n",
    "    if not aug_config:\n",
    "        aug_config = (None, None)\n",
    "    return \\\n",
    "        skip_empty_gen(\n",
    "            tiling_gen(\n",
    "                augmentation_gen(gen, *aug_config, enable=(aug_config[0] is not None))\n",
    "            , window_size=(500, 500))\n",
    "        , min_area=1000)\n",
    "        \n",
    "def setup_args():\n",
    "    from bidict import bidict\n",
    "    from imgaug import augmenters as iaa\n",
    "    from imgaug.parameters import Normal\n",
    "    \n",
    "    class_map = bidict({ # or give a bidict mapping source->dest category_id\n",
    "        0: 0,\n",
    "        1: 1,\n",
    "#         2: 2,\n",
    "#         3: 3,\n",
    "#         4: 4,\n",
    "    })\n",
    "    \n",
    "    augmentation_common = \\\n",
    "    iaa.Sequential([ \n",
    "        iaa.Fliplr(0.5),\n",
    "        iaa.Flipud(0.5),\n",
    "        iaa.Affine(\n",
    "            scale=(0.8, 1.2),\n",
    "            translate_percent=(-0.2, 0.2), \n",
    "            rotate=(-22.5, 22.5),\n",
    "            mode='constant', cval=0, order=0\n",
    "        ),\n",
    "        \n",
    "    ])\n",
    "    augmentation_image = iaa.Sequential([ # Colour aug\n",
    "        iaa.ChangeColorspace(from_colorspace=\"RGB\", to_colorspace=\"HSV\"),\n",
    "        iaa.WithChannels(0, iaa.Add(Normal(0, 256 / 6))),\n",
    "        iaa.WithChannels(1, iaa.Add(Normal(0, 256 / 6))),\n",
    "        iaa.WithChannels(2, iaa.Add(Normal(0, 256 / 6))),\n",
    "        iaa.ChangeColorspace(from_colorspace=\"HSV\", to_colorspace=\"RGB\")\n",
    "    ])\n",
    "#     augmentation_image = None\n",
    "\n",
    "    args = {\n",
    "        'annotation_translator': AnnotationMapper(class_map),\n",
    "        'augmentation': (augmentation_common, augmentation_image),    # Training augmentation\n",
    "        'class_map': class_map,             # class_map\n",
    "        'data': {\n",
    "            'base_dir': \"/data/acfr/ladybird/labelbox/hashed\",\n",
    "            'name': \"first\",\n",
    "            'sets': ('train', 'val', 'test')\n",
    "        },\n",
    "        'image_dims': (500, 500, 3),    # What to resize images to before CNN\n",
    "        'nn_dtype': np.float32,         # Pretrained networks are in float32\n",
    "        'num_classes': len(class_map),\n",
    "        'use_balanced_set': False,      # Force the use of the largest class-balanced dataset\n",
    "        'use_cached': False,            # Cache the dataset in memory\n",
    "        'use_class_weights': True,      # Use class population to weight in the training loss\n",
    "        'use_parallel': False,          # Use multiple GPUs\n",
    "        'preprocess_data': preprocess_data,\n",
    "        'preprocess_targets': preprocess_targets,\n",
    "        'postprocess_data': postprocess_data,\n",
    "        'pipeline': pipeline\n",
    "    }\n",
    "    \n",
    "    return args\n",
    "ARGS = setup_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_datasets(args):\n",
    "    from abyss_deep_learning.datasets.coco import ImageSemanticSegmentationDataset\n",
    "    \n",
    "    dataset = dict()\n",
    "    for set_name in args['data']['sets']:\n",
    "        path = os.path.join(args['data']['base_dir'], \"{:s}/{:s}.json\".format(args['data']['name'], set_name))\n",
    "        dataset[set_name] = ImageSemanticSegmentationDataset(\n",
    "            path,\n",
    "            translator=args['annotation_translator'],\n",
    "            cached=args['use_cached'],\n",
    "            preprocess_data=args['preprocess_data'],\n",
    "            preprocess_targets=args['preprocess_targets'])\n",
    "        print(\"\\n\", set_name)\n",
    "        dataset[set_name].print_class_stats()\n",
    "\n",
    "\n",
    "    print(\"\\nNumber of classes:\", args['num_classes'])\n",
    "    print(\"captions:\")\n",
    "    print(args['class_map'])\n",
    "    return dataset\n",
    "DATASET = setup_datasets(ARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abyss_deep_learning.visualize import draw_semantic_seg\n",
    "from collections import Counter\n",
    "from skimage.morphology import label\n",
    "\n",
    "def view_dataset_samples(num_rows=2):\n",
    "    plt.figure()\n",
    "    print(\"Column-wise left to right, bottom row:\")\n",
    "    for i, (name, ds) in enumerate(DATASET.items()):\n",
    "        print(name, end=' ')\n",
    "        for j, (image, label) in enumerate(\n",
    "                ARGS['pipeline'](ds.generator(shuffle_ids=True))):\n",
    "            plt.subplot(num_rows, 3, 3 * j + i + 1)\n",
    "            plt.imshow(draw_semantic_seg(label, ARGS['postprocess_data'](image)))\n",
    "            plt.axis('off')\n",
    "            if j + 1 == num_rows:\n",
    "                break\n",
    "        print('Image: shape: {}, min: {:.1f}, mean: {:.1f}, max: {:.1f}'.format(\n",
    "            image.shape, image.min(), image.mean(), image.max()))\n",
    "        print('Label: shape: {}, min: {:.1f}, mean: {:.1f}, max: {:.1f}'.format(\n",
    "            label.shape, label.min(), label.mean(), label.max()))\n",
    "\n",
    "view_dataset_samples(num_rows=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment(object):\n",
    "    def __init__(self):\n",
    "        self.batch_size = 1\n",
    "        self.model_parallel = None\n",
    "        self.has_crf = False\n",
    "    \n",
    "    def get_train_model(self):\n",
    "        return self.model_parallel or self.model\n",
    "        \n",
    "    def plot_test(self, output_fn=np.argmax):\n",
    "        gen = DATASET['test'].generator(shuffle_ids=True)\n",
    "        for i, (rgb, target) in enumerate(gen):\n",
    "            print(\"rgb.shape\", rgb.shape)\n",
    "            print(\"rgb min/max\", np.min(rgb), np.max(rgb))\n",
    "            print(\"target.shape\", target.shape)\n",
    "            print(\"target min/max\", np.min(target), np.max(target))\n",
    "\n",
    "            rgb8 = ((rgb + 1) / 2 * 255).astype(np.uint8)\n",
    "            Y_pred = self.model.predict(rgb[np.newaxis, ...])\n",
    "            plt.figure()\n",
    "            plt.subplot(2, 2, 1)\n",
    "            plt.imshow(rgb8)\n",
    "            plt.subplot(2, 2, 2)\n",
    "            plt.imshow(label2rgb(target.argmax(-1), rgb8, bg_label=0))\n",
    "            plt.subplot(2, 2, (3, 4))\n",
    "            plt.imshow(label2rgb(Y_pred[0].argmax(-1), rgb8, bg_label=0))\n",
    "#             plt.title(\"CRF IoU={:.2f}\".format(\n",
    "#                 jaccard_index(output_fn(target, axis=-1), np.argmax(Y_pred[0], axis=-1))))\n",
    "            plt.tight_layout()\n",
    "            break\n",
    "\n",
    "    def init_crf(self):\n",
    "        crf_params = ['crfrnn/spatial_ker_weights:0',\n",
    "                  'crfrnn/bilateral_ker_weights:0',\n",
    "                  'crfrnn/compatibility_matrix:0']\n",
    "        n = ARGS['num_classes']\n",
    "        self.model.get_layer(name='crfrnn').set_weights([\n",
    "            np.eye(n), np.eye(n), 1 - np.eye(n)\n",
    "        ])\n",
    "        \n",
    "    def create_model(self, num_classes, image_dims, upsample='new', num_iterations=0):\n",
    "        '''\n",
    "        crf can be one of 'new', 'load', 'train' or 'none'\n",
    "        upsample can be one of 'new', 'load' or 'train\n",
    "        '''\n",
    "        self.has_crf = num_iterations > 0\n",
    "        self.model = None\n",
    "        self.model_parallel = None\n",
    "        K.clear_session()\n",
    "        print(\"Making model with {:d} classes and {} input shape\".format(num_classes, str(image_dims)))\n",
    "        self.model = get_crfrnn_model_def(\n",
    "            num_classes=num_classes, input_shape=image_dims,\n",
    "            num_iterations=num_iterations, with_crf=self.has_crf)\n",
    "        self.model.summary()\n",
    "        if upsample in ['bilinear', 'train']:\n",
    "            print(\"initializing conv tranpose kernels\")\n",
    "            initialize_conv_transpose2d(\n",
    "                self.model,\n",
    "                ['score2', 'score4', 'upsample'],\n",
    "                trainable=(upsample == 'train'))\n",
    "        if self.has_crf:\n",
    "            print(\"initializing CRF\")\n",
    "            self.init_crf()\n",
    "            \n",
    "    def load_model(self, model_path):\n",
    "        if model_path: \n",
    "            self.model.load_weights(model_path, by_name=True)\n",
    "        if self.has_crf:\n",
    "            self.init_crf()\n",
    "\n",
    "    def compile_model(self, train_layers=None, parallel=False):\n",
    "        from abyss_deep_learning.keras.metrics import mpca_factory, auc_factory\n",
    "        weights = np.ones((1, 1, 1, 1))\n",
    "        \n",
    "        if train_layers:\n",
    "            for layer in self.model.layers:\n",
    "                layer.trainable = layer.name in train_layers\n",
    "        if parallel:\n",
    "            from keras.utils import multi_gpu_model\n",
    "            self.model_parallel = multi_gpu_model(self.model, gpus=2)#, cpu_merge=True, cpu_relocation=False)\n",
    "        self.get_train_model().compile(\n",
    "            optimizer='nadam',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=[\n",
    "                'accuracy',\n",
    "#                  auc_factory(\"PR\", weights),\n",
    "#                  auc_factory(\"ROC\", weights)\n",
    "            ])\n",
    "\n",
    "    def train(self, epochs, initial_epoch=0, val_data=None):\n",
    "        from abyss_deep_learning.keras.utils import batching_gen\n",
    "        steps_per_epoch = len(DATASET['train'].data_ids) // self.batch_size\n",
    "        steps_per_epoch_val = val_data[0].shape[0] // self.batch_size // 10\n",
    "        print(\"Steps per epoch:\", steps_per_epoch)\n",
    "        print(\"Steps per steps_per_epoch_val:\", steps_per_epoch_val)\n",
    "\n",
    "        train_gen = ARGS['pipeline'](\n",
    "            DATASET['train'].generator(shuffle_ids=True, endless=True),\n",
    "            aug_config=ARGS['augmentation'])\n",
    "        common = {\n",
    "            \"class_weight\": DATASET['train'].class_weights if ARGS['use_class_weights'] else None,\n",
    "            \"callbacks\": self.callbacks,\n",
    "            \"epochs\": epochs,\n",
    "            \"verbose\": 1,\n",
    "            \"initial_epoch\": initial_epoch,\n",
    "        }\n",
    "\n",
    "        self.history = self.get_train_model().fit_generator(\n",
    "            batching_gen(train_gen, batch_size=self.batch_size),\n",
    "            validation_data=val_data,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            validation_steps=steps_per_epoch_val,\n",
    "            workers=4,\n",
    "            use_multiprocessing=True,\n",
    "            **common)\n",
    "        return self.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_dataset(dataset, max_data=None):\n",
    "    inputs, targets = [], []\n",
    "    for i, (datum, target) in enumerate(\n",
    "            ARGS['pipeline'](dataset.generator(endless=False), None)):\n",
    "        if max_data is not None and i >= max_data:\n",
    "            break\n",
    "        inputs.append(datum[None, ...])\n",
    "        targets.append(target[None, ...])\n",
    "    return np.concatenate(inputs), np.concatenate(targets)\n",
    "VAL_DATA = dump_dataset(DATASET['val'], max_data=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First train only Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [\n",
    "#     [\n",
    "#         (\"scratch\", 1.4e-2): (9.00, 9.00),\n",
    "#         (\"scratch\", 1.6e-3): (0.55, 0.57),\n",
    "#         (\"scratch\", 6.4e-4): (0.56, 0.54),\n",
    "#         (\"scratch\", 1.5e-6): (1.11, 1.38),\n",
    "#     ],\n",
    "#     [\n",
    "#         (\"scratch prelu\", 1.0e-3): (7.67, 9.37),\n",
    "#         (\"scratch prelu\", 1.6e-3): (5.50, 5.9),\n",
    "#         (\"scratch prelu\", 3.5e-4): (0.59, 0.57),\n",
    "#         (\"scratch prelu\", 1.6e-3): (0.59, 0.55),\n",
    "#         (\"scratch prelu\", 3.1e-4): (0.61, 0.59),\n",
    "        \n",
    "#     ]\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_prelu_value(value):\n",
    "    for layer in exp.model.layers:\n",
    "        if 'prelu' in layer.name:\n",
    "            weights = layer.get_weights()\n",
    "            layer.set_weights([value * np.ones_like(weights[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_weights = None#\"/home/docker/.keras/models/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "logdir = os.path.join(\"/data/log/fcn-crfrnn/labelbox-seg/tile-scratch/crf_5-ups_train-aug-{:04d}\".format(np.random.randint(0, 9999)))\n",
    "!mkdir -p \"$logdir/models\"\n",
    "best_path = os.path.join(logdir, \"models/best.{epoch:03d}-{val_loss:.4f}.h5\")\n",
    "print(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = None\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abyss_deep_learning.keras.tensorboard import kernel_sparsity, avg_update_ratio\n",
    "\n",
    "exp = None\n",
    "K.clear_session()\n",
    "exp = Experiment()\n",
    "exp.batch_size = 1\n",
    "exp.create_model(\n",
    "    ARGS['num_classes'], ARGS['image_dims'],\n",
    "    upsample='train', # bilnear breaks the network - \n",
    "    num_iterations=5)\n",
    "if imagenet_weights:\n",
    "    exp.load_model(imagenet_weights)\n",
    "    set_prelu_value(1e-5) ############ Note if scratch training don't do this\n",
    "\n",
    "exclude = []#['score2', 'score4', 'upsample', 'crfrnn']\n",
    "layers = [layer.name for layer in exp.model.layers]\n",
    "# train_layers = layers[layers.index('fc6'):]\n",
    "# train_layers = exclude\n",
    "train_layers = list(set(layers) - set(exclude))#[layer for layer in layers if '_prelu' in layer]\n",
    "print(\"Training layers:\")\n",
    "print(train_layers)\n",
    "predictions_kernel = exp.model.get_layer(name='fc6').trainable_weights[0] # Used in a scalar callback\n",
    "\n",
    "exp.compile_model(train_layers=train_layers, parallel=ARGS['use_parallel'])\n",
    "exp.callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            best_path, monitor='val_loss', mode='min', verbose=1,\n",
    "            save_best_only=True, save_weights_only=True, period=1),\n",
    "        TensorBoard(log_dir=logdir, write_grads=False, write_graph=False, write_images=False),\n",
    "#         ImprovedTensorBoard(\n",
    "#             log_dir=logdir,\n",
    "#             scalars={\n",
    "#                 'learning_rate': exp.get_train_model().optimizer.lr,\n",
    "#                 'feature_sparsity': kernel_sparsity(exp.get_train_model()),\n",
    "#                 'prediction_UW_ratio': avg_update_ratio(exp.get_train_model(), predictions_kernel)\n",
    "#             },\n",
    "# #             groups={\n",
    "# #                 'performance': {\n",
    "# #                     'loss': ['loss', 'val_loss'],\n",
    "# #                     'accuracy': [r'.*acc.*'],\n",
    "# # #                     'Mean Per-Class Average Accuracy': [r'.*mpca.*'],\n",
    "# # #                     'Mean Avg Precision': [r'.*PR.*'],\n",
    "# # #                     'ROC AUC': [r'.*ROC.*']\n",
    "# #                 }\n",
    "# #             },\n",
    "#             pr_curve=False,\n",
    "#             num_classes=VAL_DATA[1].shape[1],\n",
    "#             histogram_freq=None,\n",
    "#             batch_size=exp.batch_size,\n",
    "#             write_grads=False,\n",
    "#         ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss', mode='min', factor=0.2, patience=5, cooldown=5, verbose=1),\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss', mode='min',\n",
    "            min_delta=0.0, patience=30, verbose=1, restore_best_weights=True)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp.model.load_weights(\"/data/log/fcn-crfrnn/oceaneering/tile-imagenet/crf_5-ups_train-aug-PR-7417/best.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abyss_deep_learning.keras.utils import LRSearch, batching_gen\n",
    "train_gen = ARGS['pipeline'](DATASET['train'].generator(\n",
    "            shuffle_ids=True, endless=True), aug_config=ARGS['augmentation'])\n",
    "search = LRSearch(\n",
    "    exp.model,\n",
    "    x=batching_gen(train_gen, batch_size=1), batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(self):\n",
    "    x, y = list(self.results.keys()), list(self.results.values())\n",
    "    plt.figure()\n",
    "    plt.semilogx(x, y, '.')\n",
    "    \n",
    "search.fit(n_lrs=10, n_epochs=4, lr_power_range=(-7, -2), steps_per_epoch=10)\n",
    "plot(search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del search\n",
    "# exp.model.save_weights(os.path.join(logdir, \"best.h5\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training now...\")\n",
    "exp.model.save_weights('/data/tmp/blah_weights.h5')\n",
    "try:\n",
    "    K.set_value(exp.get_train_model().optimizer.lr, 2e-5)\n",
    "    exp.train(400,\n",
    "              val_data=VAL_DATA,\n",
    "#               batching_gen(ARGS['pipeline'](\n",
    "#             DATASET['val'].generator(shuffle_ids=True, endless=True),\n",
    "#             aug_config=None), batch_size=exp.batch_size),\n",
    "              initial_epoch=15)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "except:\n",
    "    raise\n",
    "# Save the weights and epoch for next training step\n",
    "# initial_epoch = exp.callbacks[3].stopped_epoch + 1 if exp.callbacks[3].stopped_epoch else 16\n",
    "# initial_lr = K.eval(exp.get_train_model().optimizer.lr)\n",
    "exp.model.save_weights('/data/tmp/blah_weights2.h5')\n",
    "# raise RuntimeError(\"Stop Run All\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    initial_epoch = 0\n",
    "    for lr in [1e-5, 1e-6, 1e-7]:\n",
    "        K.set_value(exp.get_train_model().optimizer.lr, lr*100)\n",
    "        exp.train(200, val_data=VAL_DATA, initial_epoch=initial_epoch)\n",
    "        initial_epoch = exp.callbacks[-1].stopped_epoch + 1\n",
    "        del exp.callbacks[-1]\n",
    "        exp.callbacks.append(EarlyStopping(\n",
    "            monitor='val_loss', mode='min',\n",
    "            min_delta=0.0, patience=15, verbose=1, restore_best_weights=True))\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "except:\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prelu from scratch: 5e-5 for 200, LR plateau 10/5\n",
    "#Prelu from scratch: 5e-5 for 200, LR plateau 10/5\n",
    "#Prelu from imagenet: 1e-5 for 200, LR plateau 10/5\n",
    "raise Exception(\"Stop Run All Cells\")\n",
    "# exp.model.get_layer(name='fc6').trainable_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in exp.model.layers:\n",
    "    if '_prelu' in layer.name:\n",
    "        print(layer.weights)\n",
    "        weight = layer.weights[0]\n",
    "        value = weight.eval(session=K.get_session())\n",
    "        plt.figure()\n",
    "        plt.hist(value.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "def plot_test(model, window_size=(500, 500)):\n",
    "    from abyss_deep_learning.utils import tile_gen, detile\n",
    "#     from abyss_deep_learning.visualize import label2rgb\n",
    "    from skimage.color import label2rgb\n",
    "    images, targets = [], []\n",
    "    \n",
    "#     for i in range(1):\n",
    "#         sample = DATASET['train'].sample()\n",
    "#         images.append(sample[0][np.newaxis, ...])\n",
    "#         targets.append(sample[1][np.newaxis, ...])\n",
    "#     images = np.concatenate(images)\n",
    "#     targets = np.concatenate(targets)\n",
    "    \n",
    "    \n",
    "    for j, (image, target) in enumerate(\n",
    "            DATASET['test'].generator(data_ids=None)):\n",
    "        s = (1,) + image.shape\n",
    "        output = np.zeros((s[0] * s[1], s[2] * 3, 3))#, dtype=np.uint8)\n",
    "        tiles_target = [\n",
    "            model.predict(\n",
    "                tile[np.newaxis, ...])[0]\n",
    "            for tile in tile_gen(image, window_size)]\n",
    "        image = detile([tile for tile in tile_gen(image, window_size)], window_size, image.shape)\n",
    "        prediction = detile(tiles_target, window_size, target.shape)\n",
    "        image = postprocess_data(image) / 255\n",
    "        print(image.shape, target.shape, prediction.shape)\n",
    "        i = 0\n",
    "        plt.figure()\n",
    "#         output[i * s[1] : (i + 1) * s[1], 0:s[2], :] = image\n",
    "#         output[i * s[1] : (i + 1) * s[1], s[2]:2*s[2], :] = label2rgb(target.argmax(-1), image, bg_label=0)\n",
    "#         output[i * s[1] : (i + 1) * s[1], s[2]*2:3*s[2], :] = label2rgb(prediction.argmax(-1), image, bg_label=0)\n",
    "        output = label2rgb(prediction.argmax(-1), image, bg_label=0)\n",
    "        plt.imshow(output)\n",
    "        if j == 2: \n",
    "            break\n",
    "    return output\n",
    "\n",
    "# plt.figure(figsize=(6, 10))\n",
    "plt.imshow(plot_test(exp.model))\n",
    "# plt.tight_layout()\n",
    "# plt.title(\"{:s}{:^50}{:s}\".format('rgb', 'ground truth', 'predicted'))\n",
    "# plt.gca().xaxis.set_visible(False)\n",
    "# plt.gca().yaxis.set_visible(False)\n",
    "# # exp.plot_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "y_pred = exp.model.predict(x=test_data[0], batch_size=exp.batch_size)\n",
    "ap_micro = average_precision_score(\n",
    "    test_data[1][..., 1:].reshape((-1, test_data[1].shape[-1] - 1)),\n",
    "    y_pred[..., 1:].reshape((-1, test_data[1].shape[-1] - 1)),\n",
    "    average='micro')\n",
    "print(\"Micro average precision of FG classes is {:.4f}\".format(ap_micro))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Trained Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FCN Upsample Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at upsampling weights\n",
    "# exp.model.load_weights(\"/data/log/fcn-crfrnn/oceaneering/tile-imagenet-crf0/2323/models/best.019-0.2982.h5\", by_name=True)\n",
    "layer_names = ['score2', 'score4', 'upsample']\n",
    "for name in layer_names:\n",
    "    layer = exp.model.get_layer(name=name)\n",
    "    v = layer.get_weights()[0]\n",
    "    print(v.shape, v.mean(), v.std(), v.min(), v.max())\n",
    "    plt.figure()\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.imshow(v[:, :, 0, 0])\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.imshow(v[:, :, 0, 1])\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.imshow(v[:, :, 1, 0])\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.imshow(v[:, :, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRF Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.get_train_model().get_layer(name='crfrnn').get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overlay on images and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "# model_path = \"/data/log/fcn-crfrnn/anadarko/prelu/2635/models/best.071-0.2609.h5\" # First good anadarko\n",
    "model_path = \"/data/log/fcn-crfrnn/oceaneering/tile-imagenet/crf_5-ups_train-aug-PR-7417/best.h5\" # First good anadarko\n",
    "model = get_crfrnn_model_def(\n",
    "    num_classes=ARGS['num_classes'],\n",
    "    input_shape=ARGS['image_dims'],\n",
    "    num_iterations=5, with_crf=True)\n",
    "model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "def predict_image(image, model, window_size=(500, 500)):\n",
    "    from abyss_deep_learning.utils import tile_gen, detile\n",
    "    tiles = [\n",
    "        model.predict(\n",
    "            preprocess_data(tile[np.newaxis, ...]))[0]\n",
    "        for tile in tile_gen(image, window_size)]\n",
    "    return detile(tiles, window_size, image.shape[:2] + (ARGS['num_classes'],))\n",
    "\n",
    "def save_condition(path):\n",
    "    image_dir = os.path.dirname(path)\n",
    "    filename = os.path.basename(path)\n",
    "    return (\n",
    "        '_pred' not in filename\n",
    "        and '_mask' not in filename\n",
    "        and filename.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "    )\n",
    "\n",
    "def save_overlay(path):\n",
    "    image_dir = os.path.dirname(path)\n",
    "    filename = os.path.basename(path)\n",
    "    output_path = os.path.join(image_dir, '.'.join(\n",
    "        filename.split(\".\")[:-1]) + '_pred.jpg')\n",
    "    output_path_mask = os.path.join(image_dir, '.'.join(\n",
    "        filename.split(\".\")[:-1]) + '_mask.jpg')\n",
    "    print(path)\n",
    "\n",
    "    image_full = imread(path)\n",
    "    if image_full.ndim == 1:  # Wierd imread bug\n",
    "        image_full = image_full[0]\n",
    "    pred = predict_image(image_full, model)\n",
    "    pred_upscaled = resize(\n",
    "        pred, image_full.shape[0:2], order=0, preserve_range=True)\n",
    "    mask = pred_upscaled.argmax(-1).astype(np.uint16)\n",
    "    pred_rgb = label2rgb(mask, image_full, bg_label=0)\n",
    "    imsave(output_path, pred_rgb)\n",
    "    imsave(output_path_mask, mask)\n",
    "\n",
    "def foreach_file(image_glob, func, condition=None):\n",
    "    for path in list(glob(image_glob)):\n",
    "        if condition and condition(path):\n",
    "            func(path)\n",
    "\n",
    "\n",
    "image_glob = \"/data/abyss/oceaneering/data/*.JPG\"\n",
    "foreach_file(image_glob, save_overlay, condition=save_condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Module for visualizing various machine learning outputs.\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "COLOR_DICT : dict(str - > tuple(float, float, float))\n",
    "    dict mapping color strings to RGB values in the range [0, 1].\n",
    "DEFAULT_COLORS : list of str\n",
    "    Default keys in COLOR_DICT to use.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "from skimage._shared.utils import warn\n",
    "from skimage.color import rgb_colors, rgb2gray, gray2rgb\n",
    "from skimage.color.colorlabel import _rgb_vector, _match_label_with_color\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from skimage.util import img_as_float\n",
    "\n",
    "from abyss_deep_learning.utils import instance_to_categorical\n",
    "\n",
    "__all__ = ['COLOR_DICT', 'label2rgb', 'DEFAULT_COLORS']\n",
    "\n",
    "\n",
    "COLOR_DICT = {\n",
    "    k: v for k, v in rgb_colors.__dict__.items()\n",
    "    if isinstance(v, tuple)}\n",
    "\n",
    "DEFAULT_COLORS = (\n",
    "    'red', 'blue', 'yellow', 'magenta', 'green',\n",
    "    'indigo', 'darkorange', 'cyan', 'pink', 'yellowgreen')\n",
    "\n",
    "\n",
    "\n",
    "def label2rgb(\n",
    "        label, image=None, colors=None, alpha=0.3,\n",
    "        gray_bg=False, contours='thick',\n",
    "        bg_label=-1, bg_color=(0, 0, 0), image_alpha=1, kind='overlay'):\n",
    "    \"\"\"Return an RGB image where color-coded labels are painted over the image, and optionally contours are painted.\n",
    "    Source: https://github.com/scikit-image/scikit-image/blob/master/skimage/color/colorlabel.py\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    label : array, shape (M, N)\n",
    "        Integer array of labels with the same shape as `image`.\n",
    "    image : array, shape (M, N, 3), optional\n",
    "        Image used as underlay for labels. If the input is an RGB image, it's\n",
    "        converted to grayscale before coloring.\n",
    "    colors : list, optional\n",
    "        List of colors. If the number of labels exceeds the number of colors,\n",
    "        then the colors are cycled.\n",
    "    alpha : float [0, 1], optional\n",
    "        Opacity of colorized labels. Ignored if image is `None`.\n",
    "    gray_bg : bool, optional\n",
    "        Set the background image to grayscale when mode='overlay'.\n",
    "    contours : str, optional\n",
    "        Description\n",
    "    bg_label : int, optional\n",
    "        Label that's treated as the background.\n",
    "    bg_color : str or array, optional\n",
    "        Background color. Must be a name in `COLOR_DICT` or RGB float values\n",
    "        between [0, 1].\n",
    "    image_alpha : float [0, 1], optional\n",
    "        Opacity of the image.\n",
    "    kind : string, one of {'overlay', 'avg'}\n",
    "        The kind of color image desired. 'overlay' cycles over defined colors\n",
    "        and overlays the colored labels over the original image. 'avg' replaces\n",
    "        each labeled segment with its average color, for a stained-class or\n",
    "        pastel painting appearance.\n",
    "    contours, : string in {‘thick’, ‘inner’, ‘outer’, ‘subpixel’}, optional\n",
    "        The mode for finding and drawing class spatial boundaries, use None to not draw contours.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result : array of float, shape (M, N, 3)\n",
    "        The result of blending a cycling colormap (`colors`) for each distinct\n",
    "        value in `label` with the image, at a certain alpha value.\n",
    "    \"\"\"\n",
    "    if kind == 'overlay':\n",
    "        return _label2rgb_overlay(label, image, colors, alpha, bg_label,\n",
    "                                  bg_color, image_alpha, gray_bg=gray_bg, contours=contours)\n",
    "    return _label2rgb_avg(label, image, bg_label, bg_color)\n",
    "\n",
    "def _label2rgb_overlay(label, image=None, colors=None, alpha=0.3,\n",
    "                       bg_label=-1, bg_color=None, image_alpha=1, gray_bg=False, contours=None):\n",
    "    \"\"\"Return an RGB image where color-coded labels are painted over the image.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    label : array, shape (M, N)\n",
    "        Integer array of labels with the same shape as `image`.\n",
    "    image : array, shape (M, N, 3), optional\n",
    "        Image used as underlay for labels. If the input is an RGB image, it's\n",
    "        converted to grayscale before coloring.\n",
    "    colors : list, optional\n",
    "        List of colors. If the number of labels exceeds the number of colors,\n",
    "        then the colors are cycled.\n",
    "    alpha : float [0, 1], optional\n",
    "        Opacity of colorized labels. Ignored if image is `None`.\n",
    "    bg_label : int, optional\n",
    "        Label that's treated as the background.\n",
    "    bg_color : str or array, optional\n",
    "        Background color. Must be a name in `COLOR_DICT` or RGB float values\n",
    "        between [0, 1].\n",
    "    image_alpha : float [0, 1], optional\n",
    "        Opacity of the image.\n",
    "    gray_bg : bool, optional\n",
    "        Set the background image to grayscale when mode='overlay'.\n",
    "    contours : None, optional\n",
    "        Description\n",
    "    contours, : string in {‘thick’, ‘inner’, ‘outer’, ‘subpixel’}, optional\n",
    "        The mode for finding and drawing class spatial boundaries, use None to not draw contours.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result : array of float, shape (M, N, 3)\n",
    "        The result of blending a cycling colormap (`colors`) for each distinct\n",
    "        value in `label` with the image, at a certain alpha value.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        When image and label are not the same shape.\n",
    "    \"\"\"\n",
    "    if colors is None:\n",
    "        colors = DEFAULT_COLORS\n",
    "    colors = [_rgb_vector(c) for c in colors]\n",
    "\n",
    "    if image is None:\n",
    "        image = np.zeros(label.shape + (3,), dtype=np.float64)\n",
    "        # Opacity doesn't make sense if no image exists.\n",
    "        alpha = 1\n",
    "    else:\n",
    "        if not image.shape[:2] == label.shape:\n",
    "            raise ValueError(\"`image` and `label` must be the same shape\")\n",
    "\n",
    "        if image.min() < 0:\n",
    "            warn(\"Negative intensities in `image` are not supported\")\n",
    "        if gray_bg:\n",
    "            image = img_as_float(rgb2gray(image))\n",
    "            image = gray2rgb(image) * image_alpha + (1 - image_alpha)\n",
    "        else:\n",
    "            image = img_as_float(image)\n",
    "\n",
    "    # Ensure that all labels are non-negative so we can index into\n",
    "    # `label_to_color` correctly.\n",
    "    offset = min(label.min(), bg_label)\n",
    "    if offset != 0:\n",
    "        label = label - offset  # Make sure you don't modify the input array.\n",
    "        bg_label -= offset\n",
    "\n",
    "    new_type = np.min_scalar_type(int(label.max()))\n",
    "    if new_type == np.bool:\n",
    "        new_type = np.uint8\n",
    "    label = label.astype(new_type)\n",
    "\n",
    "    mapped_labels_flat, color_cycle = _match_label_with_color(label, colors,\n",
    "                                                              bg_label, bg_color)\n",
    "\n",
    "    if len(mapped_labels_flat) == 0:\n",
    "        return image\n",
    "\n",
    "    dense_labels = range(max(mapped_labels_flat) + 1)\n",
    "\n",
    "    label_to_color = np.array([c for i, c in zip(dense_labels, color_cycle)])\n",
    "\n",
    "    mapped_labels = label\n",
    "    mapped_labels.flat = mapped_labels_flat\n",
    "    if gray_bg:\n",
    "        result = label_to_color[mapped_labels] * alpha + image * (1 - alpha)\n",
    "    else:\n",
    "        result = label_to_color[mapped_labels] * alpha + image * (1 - alpha)\n",
    "\n",
    "    # Remove background label if its color was not specified.\n",
    "    remove_background = 0 in mapped_labels_flat and bg_color is None\n",
    "    if remove_background:\n",
    "        result[label == bg_label] = image[label == bg_label]\n",
    "\n",
    "    if contours:\n",
    "        for label_idx in range(label_to_color.shape[0]):\n",
    "            result = mark_boundaries(\n",
    "                result, label == label_idx, color=label_to_color[label_idx], mode=contours)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset_overlay(coco_path, dataset):\n",
    "    import json\n",
    "    from pycocotools.coco import COCO\n",
    "    from abyss_deep_learning.utils import ann_rle_encode\n",
    "    from abyss_deep_learning.visualize import label2rgb\n",
    "    from skimage.exposure import rescale_intensity\n",
    "    \n",
    "    coco = COCO(coco_path)\n",
    "    for img, (image, target) in zip(coco.imgs.values(), dataset.generator(endless=False, shuffle=False)):\n",
    "        path = img['path']\n",
    "        image = imread(path, plugin='imread')\n",
    "        print(image.shape, image.dtype, target.shape)\n",
    "        image_dir = os.path.dirname(path)\n",
    "        filename = os.path.basename(path)\n",
    "        output_path = os.path.join(image_dir, '.'.join(\n",
    "            filename.split(\".\")[:-1]) + '_gt.jpg')\n",
    "        imsave(\n",
    "            output_path,\n",
    "            (255 * label2rgb(target.argmax(-1), image, bg_label=0)).astype(np.uint8))\n",
    "        \n",
    "\"/data/log/fcn-crfrnn/oceaneering/tile-imagenet-crf0/ups_new-ups_only-reinit_ups2363/models/best.015-0.2838.h5\"\n",
    "save_dataset_overlay(\"/data/abyss/oceaneering/annotations/separation/train.json\", DATASET['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from abyss_deep_learning.utils import image_streamer\n",
    "# for path, seq_no, image_full in image_streamer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
