{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings\n",
    "  * general\n",
    "    * batch size should be at least 1x(number of classes) in order to take advantage of MTL\n",
    "    * Always start training a model with dropout=ker_reg=act_reg=0, look at bias and variance then add until good fit\n",
    "    * No pooling gives very fast results but strong overfitting and large model size\n",
    "    * Don't worry about class weights unless heavily (20x or more) imbalanced\n",
    "    * head:0x1024 works best, any more layers underfits, not much of a difference to 0x512\n",
    "  * multi-label output\n",
    "    * sigmoid output makes model very sensitive to learning_rate.\n",
    "      * I have found with VGG16 around 5e-5 is a good start\n",
    "      * Use eg setup_callbacks(hist=2, grads=True) to enable gradient outputs; check if class_logits_out is becoming spread between 0 and 1, check that gradients are not 0 (should be around 1e-3).\n",
    "  * single-label output\n",
    "    * Pretty stable with any architecture\n",
    "    \n",
    "\n",
    "## Bugs\n",
    "  * There may be a GPU memory leak somewhere... keras does not recycle models properly. I'll try to find this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import cycle\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "from imgaug import augmenters as iaa\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.resnet50 import preprocess_input, ResNet50\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, TensorBoard, EarlyStopping, LearningRateScheduler, Callback\n",
    "from keras.layers import (\n",
    "    Flatten, GlobalAveragePooling2D, GlobalMaxPooling2D, Input, Dense, Dropout)\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l1_l2\n",
    "from keras.metrics import binary_accuracy, categorical_accuracy\n",
    "from pycocotools.coco import COCO\n",
    "from skimage.transform import resize\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "import keras.backend as K\n",
    "import keras.initializers\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from abyss_deep_learning.keras.classification import (\n",
    "    ClassificationDataset, PRTensorBoard, Inference,\n",
    "    caption_map_gen, multihot_gen, augmentation_gen, skip_bg_gen, cached_gen)\n",
    "from abyss_deep_learning.keras.utils import (\n",
    "    batching_gen, lambda_gen, calc_class_weights, count_labels_multi, count_labels_single, gen_dump_data)\n",
    "import abyss_deep_learning.abyss_dataset as dataset_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from abyss_deep_learning.utils import instance_to_caption\n",
    "# train_dict = instance_to_caption(json.load(open(working_dir+\"training.json\",\"r\")))\n",
    "# with open(working_dir+\"train-nb.json\",\"w\") as f:\n",
    "#     json.dump(train_dict,f)\n",
    "# val_dict = instance_to_caption(json.load(open(working_dir+\"validation.json\",\"r\")))\n",
    "# with open(working_dir+\"val-nb.json\",\"w\") as f:\n",
    "#     json.dump(val_dict,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### CONFIGURE ALL VARIABLES IN THIS CELL ########################\n",
    "# num_classes assumed from caption_map entries\n",
    "image_dims = (299, 299, 3) # What to resize images to before CNN\n",
    "batch_size = 10 # Should be at least 1x <number of classes>\n",
    "NN_DTYPE = np.float32 # Pretrained networks are in float32\n",
    "\n",
    "# Caption type can be either \"single\" or \"multi\". This sets up various other parameters in the system.\n",
    "caption_type = \"multi\" \n",
    "\n",
    "# maps caption strings to class numbers (ensure minimal set of class numbers)\n",
    "# eg use {0, 1, 2} not {4, 7, 8}\n",
    "caption_map = {\n",
    "    \"IP\": 0,\n",
    "    \"JD_ML\": 1,\n",
    "    \"DD\": 2,\n",
    "    \"JD_S\": 3,\n",
    "    \"ED_All\": 4\n",
    "}\n",
    "\n",
    "# Import or define the right translator\n",
    "from abyss_deep_learning.datasets.translators import AbyssCaptionTranslator, CloudFactoryCaptionTranslator\n",
    "translator = AbyssCaptionTranslator() # CloudFactoryCaptionTranslator()\n",
    "\n",
    "database_dir = \"/data/abyss/projectmax/feature-detection\"\n",
    "dataset_name = \"ours\"\n",
    "coco_train = ClassificationDataset(\n",
    "    caption_map, translator,\n",
    "    os.path.join(database_dir, \"{:s}/train.json\".format(dataset_name)))\n",
    "coco_val = ClassificationDataset(\n",
    "    caption_map, translator,\n",
    "    os.path.join(database_dir, \"{:s}/val.json\".format(dataset_name)))\n",
    "coco_test = ClassificationDataset(\n",
    "    caption_map, translator,\n",
    "    os.path.join(database_dir, \"{:s}/val.json\".format(dataset_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Combinations of labels present\")\n",
    "print(\"Train:\")\n",
    "print(set([tuple(coco_train.load_caption(image['id'])) for image in coco_train.imgs.values()]))\n",
    "print(\"Val:\")\n",
    "print(set([tuple(coco_val.load_caption(image['id'])) for image in coco_val.imgs.values()]))\n",
    "print(\"Test:\")\n",
    "print(set([tuple(coco_test.load_caption(image['id'])) for image in coco_test.imgs.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find a balanced set\n",
    "def balanced_set(coco):\n",
    "    captions = [caption \n",
    "            for ann in coco.anns.values() if 'caption' in ann\n",
    "           for caption in ann['caption'].split(',') if caption != \"background\"]\n",
    "    smallest_caption, smallest_caption_value = min(Counter(captions).items(), key=lambda x: x[1])\n",
    "    \n",
    "    unique_captions = np.unique(captions)\n",
    "#     print(\"unique_captions\", unique_captions)\n",
    "    # Count how many images are in each label\n",
    "    images_in_caption = {\n",
    "        caption: [ann['image_id'] for ann in coco.anns.values() if caption in ann['caption'].split(',')]\n",
    "        for caption in unique_captions}\n",
    "    print(\"images_in_caption\", {k: len(i) for k, i in images_in_caption.items()})\n",
    "    for images in images_in_caption.values():\n",
    "        np.random.shuffle(images)\n",
    "    \n",
    "    # Count how many captions are in each image\n",
    "    captions_in_image = {\n",
    "        image_id: ([\n",
    "            caption\n",
    "            for ann in coco.anns.values() if ann['image_id'] == image_id and 'caption' in ann\n",
    "            for caption in ann['caption'].split(',') if len(caption) and caption != \"background\"])\n",
    "        for image_id in coco.imgs}\n",
    "    print(\"captions_in_image\")\n",
    "    print([len(captions) for image_id, captions in captions_in_image.items()])\n",
    "    \n",
    "#     print(\"smallest\", smallest_caption, smallest_caption_value)\n",
    "    balanced = []\n",
    "    out = {caption: [] for caption in unique_captions}\n",
    "    \n",
    "    def add_to_counts(image_id):\n",
    "        # Increment counts for all captions in image\n",
    "        for caption in captions_in_image[image_id]:\n",
    "            out[caption].append(image_id)\n",
    "        # Remove image_id from all images_in_caption\n",
    "        for images in images_in_caption.values():\n",
    "            if image_id in images:\n",
    "                images.pop(images.index(image_id))\n",
    "    \n",
    "    while any([len(out[caption]) < smallest_caption_value for caption in unique_captions]):\n",
    "        least = min(out.items(), key=lambda x: len(x[1]))\n",
    "        image_id = images_in_caption[least[0]].pop()\n",
    "        add_to_counts(image_id)\n",
    "        \n",
    "    print(\"balanced images in caption\")\n",
    "    print({k: len(v) for k, v in out.items()})\n",
    "    out = set([j\n",
    "           for i in out.values()\n",
    "          for j in i])\n",
    "\n",
    "    return out\n",
    "\n",
    "balanced_image_ids_train = balanced_set(coco_train)\n",
    "balanced_image_ids_val = balanced_set(coco_val)\n",
    "balanced_image_ids_test = balanced_set(coco_test)\n",
    "print(\"balanced train set size\", len(balanced_image_ids_train))\n",
    "print(\"balanced val set size\", len(balanced_image_ids_val))\n",
    "print(\"balanced test set size\", len(balanced_image_ids_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_map_r = {val: key for key, val in caption_map.items()}\n",
    "num_classes = len(caption_map)\n",
    "steps_per_epoch = coco_train.num_images() // batch_size\n",
    "steps_per_epoch_val = coco_val.num_images() // batch_size\n",
    "print(\"Number of classes:\", num_classes)\n",
    "print(\"Steps per epoch:\", steps_per_epoch)\n",
    "print(\"Steps per steps_per_epoch_val:\", steps_per_epoch_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image, caption):\n",
    "    image = resize(image, image_dims, preserve_range=True)\n",
    "    return preprocess_input(image.astype(NN_DTYPE), mode='tf'), caption\n",
    "\n",
    "def postprocess(image):\n",
    "    return ((image + 1) * 127.5).astype(np.uint8)\n",
    "     \n",
    "def pipeline(gen, aug_config=None):\n",
    "    return (\n",
    "        augmentation_gen(\n",
    "            skip_bg_gen(\n",
    "                multihot_gen(\n",
    "                    lambda_gen(\n",
    "                        caption_map_gen(gen, caption_map)\n",
    "                    , func=preprocess)\n",
    "                , num_classes=num_classes)\n",
    "            )\n",
    "        , aug_config, enable=(aug_config is not None))\n",
    "    )\n",
    "\n",
    "\n",
    "aug_config = {\n",
    "    'flip_lr_percentage': 0.5,\n",
    "    'flip_ud_percentage': 0.5,\n",
    "    'affine': {\n",
    "        \"order\": 1,\n",
    "        'scale': {\n",
    "            \"x\": (0.8, 1.2),\n",
    "            \"y\": (0.8, 1.2)\n",
    "        },\n",
    "        \"rotate\": (-10, 10),\n",
    "        \"shear\": (-5, 5),\n",
    "        \"mode\": 'constant'\n",
    "    },\n",
    "#     'color': {\n",
    "#         'probability': 1.00,\n",
    "#         'hue': (0, 0),\n",
    "#         'saturation': (0, 0),\n",
    "#         'value': (0, 0)\n",
    "#     }\n",
    "}\n",
    "# aug_config = None # Uncomment to remove augmentation (goes around 50% faster but much worse results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cached_gens = False\n",
    "use_balanced_set = False\n",
    "\n",
    "if use_balanced_set:\n",
    "    train_gen = pipeline(\n",
    "        coco_train.generator(imgIds=list(balanced_image_ids_train), shuffle_ids=True),\n",
    "        aug_config=aug_config)\n",
    "    val_gen = pipeline(coco_val.generator(imgIds=list(balanced_image_ids_val), shuffle_ids=True))\n",
    "    test_gen = pipeline(coco_test.generator(imgIds=list(balanced_image_ids_test), shuffle_ids=True))\n",
    "else:\n",
    "    train_gen = pipeline(\n",
    "        coco_train.generator(imgIds=None, shuffle_ids=True),\n",
    "        aug_config=aug_config)\n",
    "    val_gen = pipeline(coco_val.generator(imgIds=None, shuffle_ids=True))\n",
    "    test_gen = pipeline(coco_test.generator(imgIds=None, shuffle_ids=True))\n",
    "\n",
    "if use_cached_gens:\n",
    "    print(\"USING CACHED VAL/TEST DATA\")\n",
    "    if aug_config is None:\n",
    "        print(\"USING CACHED TRAIN DATA\")\n",
    "        train_gen = cached_gen(train_gen, len(balanced_image_ids_train))\n",
    "    val_gen = cached_gen(val_gen, len(balanced_image_ids_val))\n",
    "    test_gen = cached_gen(test_gen, len(balanced_image_ids_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n1 -r1\n",
    "for i, (image, target) in enumerate(caption_map_gen(coco_val.generator(), caption_map)):\n",
    "    print(\"out\", target)\n",
    "    if np.sum(target) == 0:\n",
    "        print(\"BG\")\n",
    "    if i >= 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (train, val, test) in enumerate(zip(train_gen, val_gen, test_gen)):\n",
    "    for data in (train, val, test):\n",
    "        print(data[0].shape, data[1], (np.min(data[0]), np.max(data[0])))\n",
    "    plt.figure()\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(postprocess(train[0]))\n",
    "    plt.title(', '.join([caption_map_r[int(cap_id)] for cap_id in np.argwhere(train[1])]))\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(postprocess(val[0]))\n",
    "    plt.title(', '.join([caption_map_r[int(cap_id)] for cap_id in np.argwhere(val[1])]))\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(postprocess(test[0]))\n",
    "    plt.title(', '.join([caption_map_r[int(cap_id)] for cap_id in np.argwhere(test[1])]))\n",
    "    \n",
    "    if i >= 0:\n",
    "        break\n",
    "print(\"Left to right: ground truth samples from train, val test\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell intentionally left blank due to display bug above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_function = count_labels_single if caption_type == \"single\" else count_labels_multi\n",
    "\n",
    "# for label, gen, coco, balanced_image_ids in zip(\n",
    "#         [\"train\", \"val\", \"test\"],\n",
    "#         [train_gen, val_gen, test_gen],\n",
    "#         [coco_train, coco_val, coco_test],\n",
    "#         [balanced_image_ids_train, balanced_image_ids_val, balanced_image_ids_test]):\n",
    "#     data = gen_dump_data(gen, len(balanced_image_ids))\n",
    "#     counter = count_function(data)\n",
    "#     print(label, counter)\n",
    "\n",
    "val_data = gen_dump_data(val_gen, len(balanced_image_ids_val))\n",
    "test_data = val_data\n",
    "class_weights = None\n",
    "# Uncomment below line to use class weights, not needed if using balanced_set\n",
    "# class_weights = calc_class_weights(gen_dump_data(train_gen, len(balanced_image_ids_train)), caption_type)\n",
    "\n",
    "print(\"training class weights:\")\n",
    "print(class_weights)\n",
    "\n",
    "print(\"Binary accuracy if you were to output all 0s\")\n",
    "acc = binary_accuracy(val_data[1], val_data[1] * 0).eval(session=K.get_session())\n",
    "print(np.mean(acc))\n",
    "print(\"Categorical accuracy if you were to output all 0s\")\n",
    "acc = categorical_accuracy(val_data[1], val_data[1] * 0).eval(session=K.get_session())\n",
    "print(np.mean(acc))\n",
    "print(\"percent of data covered by class\", val_data[1].sum(axis=0) / val_data[1].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_model_regularization(model, kernel_regularizer_l2, activity_regularizer_l1):\n",
    "#     # Add L2 Regularization\n",
    "#     # Skip gamma and beta weights of batch normalization layers.\n",
    "#     if kernel_regularizer_l2:\n",
    "#         reg_losses = [\n",
    "#             keras.regularizers.l2(kernel_regularizer_l2)(w) / tf.cast(tf.size(w), tf.float32)\n",
    "#             for w in model.trainable_weights\n",
    "#             if not any([l in w.name for l in ['gamma', 'beta']])]\n",
    "#         model.add_loss(tf.add_n(reg_losses, name='l2_reg_loss'))\n",
    "#     if activity_regularizer_l1:\n",
    "#         reg_losses = [\n",
    "#             keras.regularizers.l1(activity_regularizer_l1)(\n",
    "#                 layer.get_output_at(0)) / tf.cast(tf.size(layer.get_output_at(0)), tf.float32)\n",
    "#             for layer in model.layers\n",
    "#             if layer.trainable and not any([l in layer.name for l in ['class_logits', 'batch_norm']])]\n",
    "#         model.add_loss(tf.add_n(reg_losses, name='l1_reg_loss'))\n",
    "    for layer in model.layers: #Save \n",
    "        if not layer.trainable or 'batch_norm' in layer.name:\n",
    "            continue\n",
    "        if hasattr(layer, 'kernel_regularizer') and kernel_regularizer_l2:\n",
    "            if 'kernel' in layer.weights[0].name:\n",
    "                size = np.product(layer.weights[0].shape.as_list())\n",
    "                if size:\n",
    "                    layer.kernel_regularizer = l1_l2(0, kernel_regularizer_l2 / size)\n",
    "        if hasattr(layer, 'activity_regularizer') and activity_regularizer_l1:\n",
    "#            if 'class_logits' in layer.name:\n",
    "            size = np.product(layer.get_output_shape_at(0)[1:])\n",
    "            if size:\n",
    "                layer.activity_regularizer = l1_l2(activity_regularizer_l1 / size, 0)\n",
    "            \n",
    "    # Suspect this is where GPU memory leak is coming from\n",
    "    model_config = model.get_config()\n",
    "    model_weights = model.get_weights()\n",
    "    model = None\n",
    "    K.clear_session()\n",
    "    model = Model.from_config(model_config)\n",
    "    model.set_weights(model_weights)\n",
    "    return model\n",
    "    \n",
    "def create_new_head(base_model, model_params, opt_params=None):\n",
    "    '''make sure base_model has include_top=False. If loss=None then it is determined.'''\n",
    "    if not opt_params:\n",
    "        opt_params = {\"optimizer\": 'nadam'}\n",
    "    \n",
    "    if model_params.loss is None:\n",
    "        if model_params.caption_type == \"single\":\n",
    "            opt_params['loss'] = \"categorical_crossentropy\" \n",
    "        elif model_params.caption_type == \"multi\":\n",
    "            # weights = np.array([\n",
    "                # i[1] for i in sorted(model_params.class_weights.items())])[np.newaxis, ...] \\\n",
    "                # if model_params.class_weights else 1.0\n",
    "            opt_params['loss'] = 'binary_crossentropy'\n",
    "    else:\n",
    "        opt_params['loss'] = model_params.loss\n",
    "        \n",
    "    if model_params.activation == None:\n",
    "        if model_params.caption_type == \"single\":\n",
    "            model_params.activation = \"softmax\" \n",
    "        else:\n",
    "            model_params.activation = \"sigmoid\"\n",
    "\n",
    "    x = base_model.output\n",
    "    if model_params.pool == 'avg':\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "    elif model_params.pool == 'max':\n",
    "        x = GlobalMaxPooling2D()(x)\n",
    "    else:\n",
    "        x = Flatten()(x)\n",
    "        \n",
    "    x = BatchNormalization()(x)\n",
    "    if model_params.dropout:\n",
    "        x = Dropout(model_params.dropout)(x)\n",
    "\n",
    "    for _ in range(model_params.num_hidden_layers):\n",
    "        x = Dense(model_params.num_hidden_neurons, activation='relu',\n",
    "                  kernel_initializer=keras.initializers.he_uniform())(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        if model_params.dropout:\n",
    "            x = Dropout(model_params.dropout)(x)\n",
    "\n",
    "    predictions = Dense(\n",
    "        model_params.num_classes,\n",
    "        activation=model_params.activation,\n",
    "        kernel_initializer=keras.initializers.he_uniform(),\n",
    "#         bias_initializer=keras.initializers.he_uniform(),\n",
    "        name='class_logits')(x)\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = model_params.train_features\n",
    "        \n",
    "    model = add_model_regularization(\n",
    "        Model(inputs=base_model.input, outputs=predictions),\n",
    "        model_params.kernel_regularizer_l2,\n",
    "        model_params.activity_regularizer_l1)\n",
    "    \n",
    "    model.compile(**opt_params, metrics=['binary_accuracy', 'categorical_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradients(model):\n",
    "    \"\"\"Get the gradients of the loss with respect to the weights.\"\"\"\n",
    "    weights = [tensor for tensor in model.trainable_weights \n",
    "               if model.trainable_weights]\n",
    "    return weights, model.optimizer.get_gradients(model.total_loss, weights)\n",
    "\n",
    "def evaluate_model(model, test_data, thresh=0.5):\n",
    "    def multi_label_decision(y_true, y_pred):\n",
    "        return (y_true > thresh) == (y_pred > thresh)\n",
    "    def single_label_decision(y_true, y_pred):\n",
    "        return np.argmax(y_true, axis=-1) == np.argmax(y_pred, axis=-1)\n",
    "    decision_function = single_label_decision if caption_type == 'single' else multi_label_decision\n",
    "\n",
    "    Y_true = test_data[1]\n",
    "    Y_pred = model.predict(test_data[0])\n",
    "    TP = decision_function(Y_true, Y_pred)\n",
    "    acc = np.count_nonzero(TP) / TP.size\n",
    "    \n",
    "    print(\"Test using {:d} samples:\".format(len(test_data[0])))\n",
    "    print(\"accuracy\", acc)\n",
    "    return Y_true, Y_pred, TP\n",
    "\n",
    "def display_performance(Y_true, Y_pred, TP):\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    average_precision = dict()\n",
    "    for i in range(num_classes):\n",
    "        precision[i], recall[i], _ = precision_recall_curve(Y_true[:, i],\n",
    "                                                            Y_pred[:, i])\n",
    "        average_precision[i] = average_precision_score(Y_true[:, i], Y_pred[:, i])\n",
    "\n",
    "    # A \"micro-average\": quantifying score on all classes jointly\n",
    "    precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(Y_true.ravel(),\n",
    "        Y_pred.ravel())\n",
    "    average_precision[\"micro\"] = average_precision_score(Y_true, Y_pred,\n",
    "                                                         average=\"micro\")\n",
    "    print('Average precision score, micro-averaged over all classes: {0:0.2f}'\n",
    "          .format(average_precision[\"micro\"]))\n",
    "\n",
    "    z = np.all((Y_pred > 0.5) == Y_true, axis=1)\n",
    "    acc = np.count_nonzero(z) / z.size\n",
    "    print(\"exact accuracy\", acc)\n",
    "    z = ((Y_pred > 0.5) == Y_true)\n",
    "    acc = np.count_nonzero(z) / z.size\n",
    "    print(\"binary accuracy\", acc)\n",
    "    \n",
    "    # setup plot details\n",
    "    colors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal'])\n",
    "\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "    lines = []\n",
    "    labels = []\n",
    "    for f_score in f_scores:\n",
    "        x = np.linspace(0.01, 1)\n",
    "        y = f_score * x / (2 * x - f_score)\n",
    "        l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "        plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "\n",
    "    lines.append(l)\n",
    "    labels.append('iso-f1 curves')\n",
    "    l, = plt.plot(recall[\"micro\"], precision[\"micro\"], color='gold', lw=2)\n",
    "    lines.append(l)\n",
    "    labels.append('micro-average Precision-recall (area = {0:0.2f})'\n",
    "                  ''.format(average_precision[\"micro\"]))\n",
    "\n",
    "    for i, color in zip(range(num_classes), colors):\n",
    "        l, = plt.plot(recall[i], precision[i], color=color, lw=2)\n",
    "        lines.append(l)\n",
    "        labels.append('{0} (area = {1:0.2f})'\n",
    "                      ''.format(caption_map_r[i], average_precision[i]))\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    fig.subplots_adjust(bottom=0.25)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Micro Average Precision vs. Recall')\n",
    "    plt.legend(lines, labels, loc=(0, -.4), prop=dict(size=14))\n",
    "    plt.show()\n",
    "    plt.savefig(model_plot_path, dpi=150)\n",
    "    \n",
    "def save_model(model, name, class_map_r, prediction_type,\n",
    "               model_weights_path, model_def_path, model_info_path, history,\n",
    "               test_metrics=None, description=\"\"):\n",
    "    from abyss.utils import JsonNumpyEncoder\n",
    "    def merged(a, b):\n",
    "        merged = dict(a)\n",
    "        merged.update(b)\n",
    "        return merged\n",
    "        \n",
    "    model_info = {\n",
    "        \"name\": name,\n",
    "        \"description\": description,\n",
    "        \"weights\": model_weights_path,\n",
    "        \"prediction_type\": caption_type,\n",
    "        \"model\": model_def_path,\n",
    "        \"classes\": class_map_r,\n",
    "        \"architecture\": {\n",
    "            \"backbone\": \"inceptionv3\",\n",
    "            \"logit_activation\": model.get_layer(\"class_logits\").activation.__name__,\n",
    "            \"input_shape\": image_dims\n",
    "        },\n",
    "        \"metrics\": {\n",
    "            \"loss_function\": str(history.model.loss),\n",
    "            \"train\": merged(\n",
    "                history.history,\n",
    "                {\n",
    "                    \"epoch\": history.epoch,\n",
    "                    \"params\": history.params\n",
    "                })\n",
    "        }\n",
    "    }\n",
    "    if test_metrics:\n",
    "        model_info['metrics']['test'] = test_metrics\n",
    "    \n",
    "    print(\"Writing model def to \" + model_def_path)\n",
    "    with open(model_def_path, \"w\") as file:\n",
    "        file.write(model.to_json())\n",
    "        \n",
    "    print(\"Writing model weights to \" + model_weights_path)\n",
    "    model.save_weights(model_weights_path)\n",
    "    \n",
    "    print(\"Writing model info to \" + model_info_path)\n",
    "    with open(model_info_path, \"w\") as file:\n",
    "        file.write(json.dumps(model_info, cls=JsonNumpyEncoder))\n",
    "        \n",
    "def hamming_loss(y_true, y_pred):\n",
    "    return K.mean(y_true * (1 - y_pred) + (1 - y_true) * y_pred)\n",
    "\n",
    "\n",
    "def check_gradients(model):\n",
    "    grad_test = None\n",
    "    for image, label in train_gen:\n",
    "        grad_test = (image, label)\n",
    "        break\n",
    "    rates = []\n",
    "    weights, grads = get_gradients(model)\n",
    "    feed_dict = {\n",
    "        \"class_logits_sample_weights:0\": np.ones(2),\n",
    "        \"input_1:0\": grad_test[0][np.newaxis, ...],\n",
    "        \"class_logits_target:0\": grad_test[1][np.newaxis, ...]\n",
    "    }\n",
    "    for i, (w, g) in enumerate(zip(weights, grads)):\n",
    "        if 'bias' in w.name:\n",
    "            continue\n",
    "        grad_norm = np.linalg.norm(g.eval(feed_dict, K.get_session()))\n",
    "        weight_norm = np.linalg.norm(w.eval(K.get_session()))\n",
    "        rate = grad_norm / weight_norm\n",
    "        rates.append(rate)\n",
    "    if np.mean(rates) < 5e-4 or np.mean(rates) > 3e-1: # These values change with network structure\n",
    "        print(\"Bad gradients ({:.3e}).\".format(np.mean(rates)))\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection and training\n",
    "You may have to change these callbacks to suit the dataset and model\n",
    "Note that calculating gradients and histogram on large layered networks (resnet and inception) takes a long time (5 minutes per epoch calculated) so you may only want to do this infrequently or not at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogLR(Callback):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(LogLR, self).__init__()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if not logs:\n",
    "            logs = {'lr': K.get_value(self.model.optimizer.lr)}\n",
    "        elif 'lr' not in logs:\n",
    "            logs['lr'] = K.get_value(self.model.optimizer.lr)\n",
    "\n",
    "def setup_callbacks(log_dir, schedule=None, hist=False, grads=False):\n",
    "    !mkdir -p \"$log_dir/models\"\n",
    "    best_path = os.path.join(log_dir, \"models/best.{epoch:03d}-{val_loss:.4f}.h5\")\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(\n",
    "            best_path, monitor='val_loss', verbose=1,\n",
    "            save_best_only=True, save_weights_only=True, mode='auto', period=1),\n",
    "        PRTensorBoard(\n",
    "            log_dir=log_dir, \n",
    "            histogram_freq=(hist or 0), batch_size=batch_size,\n",
    "            write_graph=False,\n",
    "            write_grads=grads,\n",
    "            write_images=False),\n",
    "#             EarlyStopping(\n",
    "#                 monitor='val_loss', min_delta=0.0, patience=12, verbose=1, mode='auto'),\n",
    "#         clr_callback.CyclicLR(base_lr=1e-4, max_lr=0.1, min_lr=0.01, step_size=(2*steps_per_epoch))\n",
    "        LogLR()\n",
    "    ]\n",
    "    if schedule == 'plateau':\n",
    "        callbacks.append(\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=30, cooldown=0, verbose=1))\n",
    "    else:\n",
    "        callbacks.append(LearningRateScheduler(schedule, verbose=1))\n",
    "    return callbacks\n",
    "\n",
    "def go(model, callbacks, epochs, class_weights, initial_epoch=0):\n",
    "    return model.fit_generator(\n",
    "        batching_gen(train_gen, batch_size=batch_size),\n",
    "        validation_data=tuple(val_data),\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_steps=steps_per_epoch_val,\n",
    "        class_weight=class_weights,\n",
    "        callbacks=callbacks, \n",
    "        epochs=epochs,\n",
    "        verbose=1, initial_epoch=initial_epoch, workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify this class to change search parameters\n",
    "class Experiment(object):\n",
    "    def __init__(self, data_name, batch_size, input_shape):\n",
    "        self.data_name = data_name\n",
    "        self.num_classes = num_classes\n",
    "        self.caption_type = caption_type\n",
    "        self.model = None\n",
    "        self.input_shape = input_shape\n",
    "        self.batch_size = batch_size\n",
    "        self.id = str(np.random.randint(0, 9999))\n",
    "        if self.caption_type == 'single':\n",
    "            self.activation = 'softmax'\n",
    "            self.loss = 'categorical_crossentropy'\n",
    "        else:\n",
    "            self.activation = 'sigmoid'\n",
    "            self.loss = 'binary_crossentropy'\n",
    "            \n",
    "        self.feature_extractor = VGG16\n",
    "        self.model_name = self.feature_extractor.__name__\n",
    "        self.learning_rate = 5e-5\n",
    "        self.dropout = None\n",
    "        self.train_features = False\n",
    "        self.pool = 'avg'\n",
    "        self.num_hidden_layers = 0\n",
    "        self.num_hidden_neurons = 1024\n",
    "        self.pretrained_weights = 'imagenet'\n",
    "        self.class_weights = None\n",
    "        self.kernel_regularizer_l2 = None\n",
    "        self.activity_regularizer_l1 = None\n",
    "\n",
    "    def random_init(self):\n",
    "        self.feature_extractor = np.random.choice([VGG16, ResNet50, InceptionV3])\n",
    "        self.model_name = self.feature_extractor.__name__\n",
    "        self.learning_rate = 10 ** np.random.uniform(-5, -4)\n",
    "        self.dropout = np.random.uniform(0.0, 0.6)\n",
    "        self.train_features = bool(np.random.binomial(1, 0.75))\n",
    "        self.pool = np.random.choice(['avg', 'max'])\n",
    "        self.num_hidden_layers = int(np.random.choice([0, 1], size=1, p=(0.75, 0.25)))\n",
    "        self.num_hidden_neurons = 1024\n",
    "        self.pretrained_weights = np.random.choice(['imagenet', None])\n",
    "        self.class_weights = None\n",
    "        self.kernel_regularizer_l2 = 10 ** np.random.uniform(-5, -3)\n",
    "        self.activity_regularizer_l1 = 10 ** np.random.uniform(-5, -3)\n",
    "                \n",
    "    \n",
    "    def serialize(self):\n",
    "        self.model_name = self.feature_extractor.__name__\n",
    "        return ' '.join([\n",
    "            ':'.join([key, str(value)]) \n",
    "            for key, value in [\n",
    "                (\"id\", self.id),\n",
    "                (\"data\", self.data_name),\n",
    "                (\"batchsize\", self.batch_size),\n",
    "                (\"activation\", self.activation),\n",
    "                (\"model\", self.model_name),\n",
    "                (\"head\", \"{:d}x{:d}\".format(self.num_hidden_layers, self.num_hidden_neurons)),\n",
    "                (\"train\", 'all' if self.train_features else 'heads'),\n",
    "                (\"from\", str(self.pretrained_weights)),\n",
    "                (\"loss\", self.loss.__name__ if callable(self.loss) else str(self.loss)),\n",
    "                (\"init_lr\", \"{:.3e}\".format(self.learning_rate)),\n",
    "                (\"act_reg\", \"{:.3e}\".format(self.activity_regularizer_l1 or 0)),\n",
    "                (\"ker_reg\", \"{:.3e}\".format(self.kernel_regularizer_l2 or 0)),\n",
    "                (\"dropout\", \"{:.1f}\".format(self.dropout or 0)),\n",
    "                (\"pool\", str(self.pool)),\n",
    "                (\"CW\", str(True if self.class_weights else False))]])\n",
    "    \n",
    "    def deserialize(self, string):\n",
    "        strs = dict(([tuple(field.split(':')) for field in string.split(' ') if len(field.split(':')) == 2]))\n",
    "        self.id = strs['id']\n",
    "        self.model_name = strs['model']\n",
    "        self.batch_size = int(strs['batchsize'])\n",
    "        self.num_hidden_layers, self.num_hidden_neurons = [int(s) for s in strs['head'].split('x')]\n",
    "        self.train_features = strs['train'] == 'all'\n",
    "        self.pretrained_weights = strs['from']\n",
    "        self.activation = strs['activation']\n",
    "        self.loss = strs['loss']\n",
    "        self.learning_rate = float(strs['init_lr'])\n",
    "        self.activity_regularizer_l1 = float(strs['ker_reg'])\n",
    "        self.kernel_regularizer_l2 = float(strs['act_reg'])\n",
    "        self.dropout = float(strs['dropout'])\n",
    "        self.pool = bool(strs['pool'])\n",
    "        self.class_weights = None #strs['CW'] == 'True' #TODO\n",
    "        if self.class_weights:\n",
    "            raise NotImplementedError(\"Have not yet serialized class weights\")\n",
    "        self.feature_extractor = {\n",
    "            'VGG16': VGG16,\n",
    "            'ResNet50': ResNet50,\n",
    "            'InceptionV3': InceptionV3\n",
    "        }[strs['model']]\n",
    "    \n",
    "    def describe(self):\n",
    "        return self.serialize().replace(' ', \"\\n\").replace(\":\", \": \")\n",
    "    \n",
    "    def make_model(self):\n",
    "        self.model = None\n",
    "        K.clear_session()\n",
    "        self.model = create_new_head(\n",
    "            self.feature_extractor(\n",
    "                include_top=False,\n",
    "                weights=self.pretrained_weights,\n",
    "                input_shape=self.input_shape),\n",
    "            self, \n",
    "            opt_params={'optimizer': 'nadam'}\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "search_output_dir = \"/data/log/cnn/fd/tuesday3\" # Change this output dir.\n",
    "num_epochs_train = 100\n",
    "model_instance = None\n",
    "# model_instance = \"id:3880 data:ours batchsize:5 activation:sigmoid model:VGG16 head:0x1024 train:all from:imagenet loss:binary_crossentropy init_lr:2e-04 act_reg:1e-5 ker_reg:3e-5 dropout:0.25 pool:avg CW:False\"\n",
    "history_data = {}\n",
    "\n",
    "def lr_schedule_exp(epoch, base_lr=1e-3, gamma=0.98):\n",
    "    return base_lr * gamma ** epoch\n",
    "\n",
    "### Pick one\n",
    "schedule = 'plateau'\n",
    "# schedule = lambda epoch, lr: lr_schedule_exp(epoch, base_lr=5e-5, gamma=0.98) # Exponential decay\n",
    "# schedule = None\n",
    "\n",
    "\n",
    "for attempt_no in range(1):\n",
    "    K.clear_session()\n",
    "    exp = Experiment(dataset_name, batch_size, image_dims)\n",
    "    if model_instance:  # If loading network structure from model_instance (not weights)\n",
    "        exp.deserialize(model_instance)\n",
    "        exp.id = str(np.random.randint(0, 999))\n",
    "    experiment_name = exp.serialize()\n",
    "    print('=' * 80)\n",
    "    print(exp.describe())\n",
    "    exp.make_model() # Comment this out if you want to reuse model in memory, but tensorboard stats will be ruined unless you change initial_epoch\n",
    "\n",
    "    log_dir = os.path.join(search_output_dir, experiment_name)\n",
    "    model_def_path = os.path.join(log_dir, \"model_def.json\")\n",
    "    model_weights_path = os.path.join(log_dir, \"model_weights.h5\")\n",
    "    model_info_path = os.path.join(log_dir, \"model.json\")\n",
    "    model_plot_path = os.path.join(log_dir, \"precision-recall.png\")\n",
    "    \n",
    "    print(experiment_name)\n",
    "    print(log_dir)\n",
    "    print(\"Training: {:d} layers\".format(len([1 for layer in exp.model.layers if layer.trainable])))\n",
    "\n",
    "    K.set_value(exp.model.optimizer.lr, exp.learning_rate)\n",
    "    callbacks = setup_callbacks(log_dir, schedule=schedule, hist=2, grads=True)\n",
    "    history_data[experiment_name] = go(exp.model, callbacks, num_epochs_train, exp.class_weights, initial_epoch=0)\n",
    "\n",
    "    (Y_true, Y_pred, TP) = evaluate_model(exp.model, test_data, thresh=0.5)\n",
    "    display_performance(Y_true, Y_pred, TP)\n",
    "\n",
    "    save_model(\n",
    "        exp.model, name=experiment_name,\n",
    "        class_map_r=caption_map_r, prediction_type=caption_type,\n",
    "        model_weights_path=model_weights_path, model_def_path=model_def_path, model_info_path=model_info_path,\n",
    "        test_metrics=None, history=history_data[experiment_name],\n",
    "        description=\"Test model for 5 FDs\"\n",
    "    )\n",
    "\n",
    "if history_data:\n",
    "    with open(os.path.join(search_output_dir, \"history-{:d}epoch.pkl\".format(num_epochs_train)), \"wb\") as file:\n",
    "        pickle.dump({key: history.history for key, history in history_data.items()}, file)\n",
    "\n",
    "### Should you need to load this pkl:\n",
    "# with open(os.path.join(search_output_dir, \"history-100epoch.pkl\"), \"rb\") as file:\n",
    "#     history = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lrs = [key[1] for key, val  in sorted(history_data.items(), key=lambda x: x[0])]\n",
    "# val_loss = [history.history['val_loss'][-1] for key, history  in sorted(history_data.items(), key=lambda x: x[0])]\n",
    "# loss = [history.history['loss'][-1] for lr, history  in sorted(history_data.items(), key=lambda x: x[0])]\n",
    "# acc = [history.history['binary_accuracy'][-1] for lr, history  in sorted(history_data.items(), key=lambda x: x[0])]\n",
    "# val_acc = [history.history['val_binary_accuracy'][-1] for lr, history  in sorted(history_data.items(), key=lambda x: x[0])]\n",
    "\n",
    "# plt.figure()\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.semilogx(lrs, loss, '.b', label='loss')\n",
    "# plt.semilogx(lrs, val_loss, '.r', label='val_loss')\n",
    "# plt.legend()\n",
    "# plt.title(\"Loss Vs. LR (100 Epoch)\")\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.semilogx(lrs, acc, '.b', label='binary_accuracy')\n",
    "# plt.semilogx(lrs, val_acc, '.r', label='val_binary_accuracy')\n",
    "# plt.legend()\n",
    "# plt.title(\"Accuracy Vs. LR (100 Epoch)\")\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue training specific model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "K.clear_session()\n",
    "search_output_dir = \"/data/log/cnn/fd/tuesday3\" # Change this output dir.\n",
    "model_instance = \"id:2274 data:ours batchsize:50 activation:sigmoid model:VGG16 head:0x1024 train:heads from:imagenet loss:binary_crossentropy init_lr:5.000e-05 act_reg:0.000e+00 ker_reg:0.000e+00 dropout:0.0 pool:avg CW:False\"\n",
    "model_best_weight = \"model_weights.h5\"\n",
    "\n",
    "### Don't set below\n",
    "exp = Experiment(dataset_name, batch_size, image_dims)\n",
    "exp.deserialize(model_instance)\n",
    "exp.id = \"c{:d}\".format(np.random.randint(0,999))\n",
    "experiment_name = exp.serialize()\n",
    "\n",
    "model_weights_in_path = os.path.join(search_output_dir, model_instance, model_best_weight)\n",
    "log_dir = os.path.join(search_output_dir, model_instance, \"continued\", experiment_name)\n",
    "best_path = os.path.join(log_dir, \"models/best.{epoch:03d}-{val_loss:.4f}.h5\")\n",
    "model_def_path = os.path.join(log_dir, \"model_def.json\")\n",
    "model_info_path = os.path.join(log_dir, \"model.json\")\n",
    "model_plot_path = os.path.join(log_dir, \"precision-recall.png\")\n",
    "\n",
    "print(model_weights_in_path)\n",
    "if os.path.exists(model_weights_in_path):\n",
    "    !mkdir -p \"$log_dir/models\"\n",
    "else:\n",
    "    raise OSError(\"path does not exist\")\n",
    "    \n",
    "print(\"loading\")\n",
    "print(os.path.join(search_output_dir, model_instance, \"model.json\"))\n",
    "exp.model = Inference(os.path.join(search_output_dir, model_instance, \"model.json\")).model\n",
    "# base_model = p.feature_extractor(\n",
    "#     include_top=False, weights=p.pretrained_weights, input_shape=image_dims)\n",
    "# model = create_new_head(\n",
    "#     base_model, p.num_classes, p.caption_type, p, \n",
    "#     opt_params={'optimizer': 'nadam'}\n",
    "# )\n",
    "exp.model.load_weights(os.path.join(search_output_dir, model_instance, \"model_weights.h5\"))\n",
    "# model = add_model_regularization(model, params.kernel_regularizer_l2, params.activity_regularizer_l1)\n",
    "\n",
    "# for layer in model.layers:\n",
    "#     layer.trainable = True\n",
    "\n",
    "exp.model.compile( # TODO, load this from JSON, manually change this if you are doing single label\n",
    "    'nadam',\n",
    "    loss=exp.loss,\n",
    "    metrics=['binary_accuracy', 'categorical_accuracy'])\n",
    "\n",
    "print(experiment_name)\n",
    "print(log_dir)\n",
    "print(\"Training: {:d} layers\".format(len([1 for layer in exp.model.layers if layer.trainable])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epoch = 100\n",
    "num_epoch = 200 # cumulative with initial_epoch\n",
    "class_weights = None # Can't currently resume training with imbalance data #TODO\n",
    "new_learning_rate = 5e-4\n",
    "### Pick one schedule\n",
    "schedule = 'plateau'\n",
    "# schedule = lambda epoch, lr: lr_schedule_exp(epoch, base_lr=5e-5, gamma=0.98) # Exponential decay\n",
    "# schedule = None\n",
    "\n",
    "\n",
    "#### Don't set below\n",
    "\n",
    "K.set_value(exp.model.optimizer.lr, new_learning_rate)\n",
    "callbacks = setup_callbacks(log_dir, schedule=schedule, hist=2, grads=True)\n",
    "history_data[experiment_name] = go(exp.model, callbacks, num_epoch, exp.class_weights, initial_epoch=initial_epoch)\n",
    "\n",
    "(Y_true, Y_pred, TP) = evaluate_model(exp.model, test_data, thresh=0.5)\n",
    "display_performance(Y_true, Y_pred, TP)\n",
    "\n",
    "save_model(\n",
    "    exp.model, name=experiment_name,\n",
    "    class_map_r=caption_map_r, prediction_type=caption_type,\n",
    "    model_weights_path=model_weights_path, model_def_path=model_def_path, model_info_path=model_info_path,\n",
    "    test_metrics=None, history=history_data[experiment_name],\n",
    "    description=\"Test model for 5 FDs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class LrSearch():\n",
    "    def __init__(self, instance_str):\n",
    "        self.model = None\n",
    "        self.model_init_w = None\n",
    "        self.model_instance = instance_str\n",
    "        self.params = ExperimentParameters.from_string(model_instance)\n",
    "        self.history = {}\n",
    "        \n",
    "    def new_model(self):\n",
    "        p = self.params\n",
    "        save_weights = self.model == None\n",
    "        \n",
    "        self.model = None\n",
    "#         K.clear_session()\n",
    "        self.model = create_new_head(\n",
    "            p.feature_extractor(\n",
    "            include_top=False, weights=p.pretrained_weights, input_shape=image_dims),\n",
    "            p.num_classes, p.caption_type, p, \n",
    "            opt_params={'optimizer': 'nadam'}\n",
    "        )\n",
    "        \n",
    "        if save_weights:\n",
    "            self.model_init_w = self.model.get_weights()\n",
    "        else:\n",
    "            self.model.set_weights(self.model_init_w)\n",
    "        print(\"Trainable layers: {:d}\".format(sum([layer.trainable for layer in self.model.layers])))\n",
    "        \n",
    "    def go(self, epochs, num_steps=10):\n",
    "        self.history = {}\n",
    "        callbacks = []\n",
    "\n",
    "        for base_lr in 10 ** np.random.uniform(-9, 0, num_steps):\n",
    "            print(\"Learning rate = {:.3e}\".format(base_lr))\n",
    "            self.new_model()\n",
    "            K.set_value(self.model.optimizer.lr, base_lr)\n",
    "            self.history[base_lr] = self.model.fit_generator(\n",
    "                batching_gen(train_gen, batch_size=batch_size),\n",
    "                validation_data=tuple(val_data),\n",
    "                steps_per_epoch=steps_per_epoch,\n",
    "                validation_steps=steps_per_epoch_val,\n",
    "                class_weight=None,\n",
    "                callbacks=[], \n",
    "                epochs=epochs,\n",
    "                verbose=1, workers=10)\n",
    "        return self.history\n",
    "    \n",
    "    def plot(self):\n",
    "        df = []\n",
    "        for lr, h in self.history.items():\n",
    "            metrics = np.vstack([np.array(h.history['binary_accuracy']), np.array(h.history['loss'])])\n",
    "            diff = (metrics[:, -1] - metrics[:, 0])[np.newaxis]\n",
    "            metrics = np.vstack([metrics[:, -1], diff]).ravel()\n",
    "            metrics = np.array([lr] + metrics.tolist())\n",
    "            dfh = pd.DataFrame(\n",
    "                    data=pd.Series(\n",
    "                        data=metrics,\n",
    "                        index=['lr', 'binary_accuracy', 'loss', 'diff_acc', 'diff_loss'])\n",
    "                ).T.set_index('lr')\n",
    "            df.append(dfh)\n",
    "        df = pd.concat(df).sort_index()\n",
    "        df.plot(logx=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_instance = \"id:3880 data:ours batchsize:5 activation:sigmoid model:VGG16 head:0x1024 train:heads from:imagenet loss:binary_crossentropy init_lr:6.582e-05 act_reg:1e-4 ker_reg:1e-4 dropout:0.25 pool:avg CW:False\"\n",
    "search = LrSearch(model_instance)\n",
    "search.go(5, num_steps=50)\n",
    "search.plot()\n",
    "\n",
    "# Heads only: 4.4e-5 to 2e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below is untested with new changes, don't use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer=model.layers[-3]\n",
    "# print(layer)\n",
    "# for weight in layer.weights:\n",
    "#     weight.initializer.run(session=K.get_session())\n",
    "# # w = layer.get_weights()\n",
    "# # plt.figure()\n",
    "# # plt.hist(w[0].ravel(), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # More training\n",
    "# K.set_value(model.optimizer.lr, 1e-4)\n",
    "\n",
    "# go(300, class_weights, initial_epoch=200)\n",
    "# (Y_true, Y_pred, TP) = evaluate_model(model, test_data, thresh=0.5)\n",
    "# display_performance(Y_true, Y_pred, TP)\n",
    "\n",
    "# # save_model(\n",
    "# #     model, name=experiment_name + \"-second\",\n",
    "# #     class_map_r=caption_map_r, prediction_type=caption_type,\n",
    "# #     model_weights_path=model_weights_path, model_def_path=model_def_path, model_info_path=model_info_path,\n",
    "# #     test_metrics=None, history=history_data[experiment_name],\n",
    "# #     description=\"Test model for 5 FDs\"\n",
    "# # )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from herbicide.utils import vis_square\n",
    "TP_mask = np.logical_and.reduce(TP, axis=1)\n",
    "right = test_data[0][TP_mask]\n",
    "wrong = test_data[0][~TP_mask]\n",
    "wrong.shape\n",
    "plt.figure()\n",
    "vis_square(wrong)\n",
    "plt.title(\"Incorrectly Predicted\")\n",
    "plt.figure()\n",
    "vis_square(right)\n",
    "plt.title(\"Correctly Predicted\")\n",
    "\n",
    "# Binary coded the labels then count them wrt TP/FP\n",
    "print(\"num labels\", test_data[1].sum(axis=0))\n",
    "coded = np.sum(test_data[1][~TP_mask] * 2 ** np.arange(num_classes)[::-1], axis=1).astype(int)\n",
    "print(\"binary coded class error count:\", dict(sorted(Counter(coded).items(), key=lambda x: x[0])))\n",
    "coded = np.sum(test_data[1][TP_mask] * 2 ** np.arange(num_classes)[::-1], axis=1).astype(int)\n",
    "print(\"binary coded class correct count:\", dict(sorted(Counter(coded).items(), key=lambda x: x[0])))\n",
    "print(Y_pred[TP_mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def learning_curve(dataset, lr, steps, val_data, log_dir):\n",
    "#     def save_model(path):\n",
    "#         print(\"Saving\", path)\n",
    "#         os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "#         model.save_weights(path)\n",
    "#     def setup_callbacks():\n",
    "#         return [\n",
    "# #                 ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, cooldown=5, verbose=1),\n",
    "# #                 ModelCheckpoint(\n",
    "# #                     model_best_path, monitor='val_loss', verbose=1,\n",
    "# #                     save_best_only=True, save_weights_only=True, mode='auto', period=1),\n",
    "# #                 ModelCheckpoint(\n",
    "# #                     best_path, monitor='val_loss', verbose=1,\n",
    "# #                     save_best_only=False, save_weights_only=True, mode='auto', period=50),\n",
    "#                 PRTensorBoard(\n",
    "#                     log_dir=model_log_dir,\n",
    "#                     histogram_freq=0,\n",
    "#                     batch_size=batch_size,\n",
    "#                     write_graph=False,\n",
    "#                     write_grads=False,\n",
    "#                     write_images=False),\n",
    "#         #         EarlyStopping(\n",
    "#         #             monitor='val_loss', min_delta=0.0, patience=40, verbose=1, mode='auto')\n",
    "#         ]\n",
    "#     def create_new_model(load_base=False):\n",
    "#         clear_session()\n",
    "#         model = create_new_head(\n",
    "#             InceptionV3(include_top=False, weights='imagenet', input_shape=image_dims),\n",
    "#             num_classes, caption_type, opt_params={'optimizer': Nadam()},\n",
    "#             class_weights=None, train_features=False, l2_reg=None)\n",
    "#         if load_base:\n",
    "#             print(\"Loading base model\")\n",
    "#             model.load_weights(base_model_path, by_name=True)\n",
    "#         return model\n",
    "\n",
    "#     def train():\n",
    "#         print(\"Training\")\n",
    "#         K.set_value(model.optimizer.lr, lr)\n",
    "#         history[subset_size] = model.fit_generator(\n",
    "#             batching_gen(gen, batch_size=batch_size),\n",
    "#             validation_data=tuple(val_data),\n",
    "#             steps_per_epoch=(subset_size // batch_size),\n",
    "#             validation_steps=steps_per_epoch_val,\n",
    "#             class_weight=model_class_weights,\n",
    "#             callbacks=setup_callbacks(), \n",
    "#             epochs=50,\n",
    "#             verbose=1)\n",
    "#     model_class_weights = None\n",
    "#     model = None\n",
    "#     model_path = None\n",
    "#     image_ids = [image['id'] for image in dataset.imgs.values()]\n",
    "#     np.random.shuffle(image_ids)\n",
    "#     num_images = len(image_ids)\n",
    "#     print(\"num_images\", num_images)\n",
    "#     history = {}\n",
    "#     base_model_path = os.path.join(log_dir, \"base\", \"weights.h5\")\n",
    "#     model_path = base_model_path\n",
    "#     for subset_size in np.linspace(0, num_images, steps + 1).astype(int):\n",
    "#         if subset_size > 0:\n",
    "#             imgIds = image_ids[:subset_size]\n",
    "#             gen = pipeline(\n",
    "#                 dataset.generator(shuffle_ids=False, imgIds=imgIds),\n",
    "#                 aug_config=None)\n",
    "#             model_class_weights = calc_class_weights(gen, dataset) # TODO\n",
    "\n",
    "#             model_path = os.path.join(log_dir, \"subset-of-{:d}/weights.h5\".format(subset_size))\n",
    "#             model_log_dir = os.path.dirname(model_path)\n",
    "#             model_best_path = os.path.join(log_dir, \"subset-of-{:d}/best.h5\".format(subset_size))\n",
    "#             os.makedirs(model_log_dir, exist_ok=True)\n",
    "\n",
    "#             print(\"learning curve(lr={:.3e}, size={:d})\".format(lr, subset_size))\n",
    "#             print(\"model_log_dir\", model_log_dir)\n",
    "#             print(\"training class weights\")\n",
    "#             print(model_class_weights)\n",
    "#         model = create_new_model(load_base=(subset_size > 0))\n",
    "#         if subset_size:\n",
    "#             train()\n",
    "#         save_model(model_path)\n",
    "#     return history\n",
    "\n",
    "# model = None\n",
    "# lr = 1e-5\n",
    "# learning_curve_dir = \"/data/log/cnn/fd/learning_curve_5--{:.2e}\".format(lr)\n",
    "# lc_history = learning_curve(coco_train, lr, 5, val_data, learning_curve_dir)\n",
    "# val_loss = np.array([(size, h.history['val_loss'][-1]) for size, h in lc_history.items()])\n",
    "# train_loss = np.array([(size, h.history['loss'][-1]) for size, h in lc_history.items()])\n",
    "# plt.figure()\n",
    "# plt.plot(train_loss[:, 0], train_loss[:, 1], 'b.')\n",
    "# plt.plot(val_loss[:, 0], val_loss[:, 1], 'r.')\n",
    "# plt.xlabel(\"Number of Training Samples\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.savefig(os.path.join(learning_curve_dir, \"plot.png\"), dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -R /data/log/cnn/fd/learning-curve/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = None\n",
    "# for images, labels in batching_gen(train_gen, batch_size=batch_size):\n",
    "#     print(images.shape, labels.shape)\n",
    "    \n",
    "#     pred = model.predict(images)\n",
    "#     print(labels)\n",
    "#     print(pred)\n",
    "#     print(K.eval(K.tf.losses.sigmoid_cross_entropy(labels, pred)))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for unique_label in np.unique(val_data[1], axis=0):\n",
    "#     unique_data = [val_data[0][i] for i in range(len(val_data[0])) if np.all(val_data[1][i] == unique_label)]\n",
    "#     num_data = len(unique_data)\n",
    "#     print(unique_label, num_data)\n",
    "#     plt.figure()\n",
    "#     vis_square(np.array(unique_data))\n",
    "#     plt.title(unique_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Update/Weight Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# for i, (w, g) in enumerate(zip(weights, grads)):\n",
    "#     grad_norm = np.linalg.norm(g.eval(feed_dict, K.get_session()))\n",
    "#     weight_norm = np.linalg.norm(w.eval(K.get_session()))\n",
    "#     rate = grad_norm / weight_norm\n",
    "#     print(i, rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from herbicide.utils import vis_square\n",
    "for layer in model.layers:\n",
    "    if not layer.trainable_weights:\n",
    "        continue\n",
    "    for weight in layer.trainable_weights: #  Assumes FD is not trainable\n",
    "        if 'kernel' not in weight.name:\n",
    "            continue\n",
    "        print(weight.name)\n",
    "        value = K.eval(weight.value())\n",
    "        print(value.shape)\n",
    "    \n",
    "    plt.figure()\n",
    "    vis_square(value.transpose((3, 0, 1, 2)))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "caption_stats = []\n",
    "for i, (image, caption) in enumerate(coco_train.generator(imgIds=balanced_image_ids_train)):\n",
    "    plt.figure()\n",
    "    plt.imshow(image)\n",
    "    plt.title(str(caption))\n",
    "    if i == 10:\n",
    "        break\n",
    "    caption_stats.append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
