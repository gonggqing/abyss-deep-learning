{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import cycle\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "\n",
    "from imgaug import augmenters as iaa\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.resnet50 import preprocess_input, ResNet50\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from keras.layers import (\n",
    "    Flatten, GlobalAveragePooling2D, GlobalMaxPooling2D, Input, Dense, Dropout)\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Nadam\n",
    "from pycocotools.coco import COCO\n",
    "from skimage.transform import resize\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "import keras.backend as K\n",
    "import keras.initializers\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# from herbicide.utils import vis_square\n",
    "\n",
    "from abyss_deep_learning.keras.classification import (\n",
    "    ClassificationDataset, caption_map_gen, multihot_gen, augmentation_gen, PRTensorBoard, Inference)\n",
    "from abyss_deep_learning.keras.utils import (\n",
    "    batching_gen, lambda_gen, calc_class_weights, count_labels_multi, count_labels_single, gen_dump_data)\n",
    "import abyss_deep_learning.abyss_dataset as dataset_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instance_to_caption(coco):\n",
    "    caption_map_r = {cat['id']: cat['name'] for cat in coco['categories']}\n",
    "    annotations = {}\n",
    "    for image in coco['images']:\n",
    "        image_id = image['id']\n",
    "        anns = [\n",
    "            annotation for annotation in coco['annotations'] \n",
    "            if 'category_id' in annotation and annotation['image_id'] == image_id]\n",
    "        anns_str = set([caption_map_r[ann['category_id']] for ann in anns]) if anns else {'background'}\n",
    "        annotations[image_id] = {\n",
    "          \"caption\": ','.join(list(anns_str)),\n",
    "          \"id\": image_id,\n",
    "          \"image_id\": image_id,\n",
    "          \"type\": 'class_labels'\n",
    "        }\n",
    "    coco['annotations'] = list(annotations.values())\n",
    "    coco.pop('categories', None)\n",
    "    coco.pop('captions', None)\n",
    "    return coco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# working_dir = \"/mnt/ssd1/processed/industry-data/project-max/ml/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dict = instance_to_caption(json.load(open(working_dir+\"training.json\",\"r\")))\n",
    "# with open(working_dir+\"train-nb.json\",\"w\") as f:\n",
    "#     json.dump(train_dict,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_dict = instance_to_caption(json.load(open(working_dir+\"/5-class-val-cf.json\",\"r\")))\n",
    "# with open(working_dir+\"val-nb.json\",\"w\") as f:\n",
    "#     json.dump(val_dict,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### CONFIGURE ALL VARIABLES IN THIS CELL ########################\n",
    "# num_classes assumed from caption_map entries\n",
    "# image_dims = (224, 224, 3) # Preset for Mobilenet\n",
    "image_dims = (480, 640, 3) # Preset for InceptionV3\n",
    "batch_size = 1\n",
    "NN_DTYPE = np.float32\n",
    "\n",
    "# maps caption strings to class numbers (ensure minimal set of class numbers)\n",
    "# eg use {0, 1, 2} not {4, 7, 8}\n",
    "\n",
    "# Caption type can be either \"single\" or \"multi\".\n",
    "# This sets up various parameters in the system.\n",
    "# If conversion between single and multi is required this should be done explicitly and presented\n",
    "# in a separate json file. The internal representation of all the labels is one-hot encoding.\n",
    "caption_type = \"multi\" \n",
    "caption_map = {\n",
    "    \"IP\":0,\n",
    "    \"ED\":1,\n",
    "    \"JD\":2\n",
    "}\n",
    "\n",
    "num_classes = len(caption_map)\n",
    "caption_map_r = {val: key for key, val in caption_map.items()}\n",
    "\n",
    "class AbyssCaptionTranslator(ClassificationDataset.AnnotationTranslator):\n",
    "    '''Translates the CloudFactory labels into a form that works with this script'''\n",
    "    def filter(self, annotation):\n",
    "        return 'caption' in annotation and 'type' in annotation and annotation['type'] == 'class_labels'\n",
    "    def translate(self, annotation):\n",
    "        return annotation['caption'].split(\",\")\n",
    "    \n",
    "\n",
    "translator = AbyssCaptionTranslator()\n",
    "\n",
    "working_dir = \"/mnt/ssd1/processed/industry-data/project-max/ml/cloud-factory-data/all-classes-no-bg/val-nb.json\"\n",
    "dataset_name = \"cf-forward-validation-test\"\n",
    "\n",
    "coco_val = ClassificationDataset(\n",
    "    caption_map, translator, False,\n",
    "    working_dir)\n",
    "\n",
    "val_gen = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_map_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"val\", set([tuple(coco_val.load_caption(image['id'])) for image in coco_val.imgs.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_val_set(coco):\n",
    "    captions = [caption \n",
    "            for ann in coco.anns.values() if 'caption' in ann\n",
    "           for caption in ann['caption'].split(',') if caption != \"background\"]\n",
    "    smallest_caption, smallest_caption_value = min(Counter(captions).items(), key=lambda x: x[1])\n",
    "    \n",
    "    unique_captions = np.unique(captions)\n",
    "#     print(\"unique_captions\", unique_captions)\n",
    "    # Count how many images are in each label\n",
    "    images_in_caption = {\n",
    "        caption: [ann['image_id'] for ann in coco.anns.values() if caption in ann['caption'].split(',')]\n",
    "        for caption in unique_captions}\n",
    "    \n",
    "    for images in images_in_caption.values():\n",
    "        np.random.shuffle(images)\n",
    "    \n",
    "    # Count how many captions are in each image\n",
    "    captions_in_image = {\n",
    "        image_id: ([\n",
    "            caption\n",
    "            for ann in coco.anns.values() if ann['image_id'] == image_id and 'caption' in ann\n",
    "            for caption in ann['caption'].split(',') if caption != \"background\"])\n",
    "        for image_id in coco.imgs}\n",
    "    print(\"captions_in_image\")\n",
    "    print([len(captions) for image_id, captions in captions_in_image.items()])\n",
    "    \n",
    "#     print(\"smallest\", smallest_caption, smallest_caption_value)\n",
    "    balanced = []\n",
    "    out = {caption: [] for caption in unique_captions}\n",
    "    \n",
    "    def add_to_counts(image_id):\n",
    "        # Increment counts for all captions in image\n",
    "        for caption in captions_in_image[image_id]:\n",
    "            out[caption].append(image_id)\n",
    "        # Remove image_id from all images_in_caption\n",
    "        for images in images_in_caption.values():\n",
    "            if image_id in images:\n",
    "                images.pop(images.index(image_id))\n",
    "    \n",
    "    while any([len(out[caption]) < smallest_caption_value for caption in unique_captions]):\n",
    "        least = min(out.items(), key=lambda x: len(x[1]))\n",
    "        image_id = images_in_caption[least[0]].pop()\n",
    "        add_to_counts(image_id)\n",
    "        \n",
    "    out = [j\n",
    "           for i in out.values()\n",
    "          for j in i]\n",
    "    return set(out)\n",
    "\n",
    "\n",
    "balanced_image_ids_val = balanced_val_set(coco_val)\n",
    "print(\"balanced val set size\", len(balanced_image_ids_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image, caption):\n",
    "    image = resize(image, image_dims, preserve_range=True)\n",
    "    return preprocess_input(image.astype(NN_DTYPE), mode='tf'), caption\n",
    "\n",
    "def postprocess(image):\n",
    "    return ((image + 1) * 127.5).astype(np.uint8)\n",
    "     \n",
    "def pipeline(gen, aug_config=None):\n",
    "    return (\n",
    "        augmentation_gen(\n",
    "            multihot_gen(\n",
    "                lambda_gen(\n",
    "                    caption_map_gen(gen, caption_map, background='background', skip_bg=False)\n",
    "                , func=preprocess)\n",
    "            , num_classes=num_classes)\n",
    "        , aug_config, enable=(aug_config is not None))\n",
    "    )\n",
    "\n",
    "\n",
    "aug_config = {\n",
    "    'flip_lr_percentage': 0.5,\n",
    "    'flip_ud_percentage': 0.,\n",
    "    'affine': {\n",
    "        \"order\": 1,\n",
    "        'scale': {\n",
    "            \"x\": (0.8, 1.2),\n",
    "            \"y\": (0.8, 1.2)\n",
    "        },\n",
    "        \"rotate\": (-10, 10),\n",
    "        \"shear\": (-5, 5),\n",
    "        \"mode\": 'constant'\n",
    "    },\n",
    "#     'color': {\n",
    "#         'probability': 1.00,\n",
    "#         'hue': (0, 0),\n",
    "#         'saturation': (0, 0),\n",
    "#         'value': (0, 0)\n",
    "#     }\n",
    "}\n",
    "# aug_config = None # Uncomment to remove augmentation (goes around 50% faster but much worse results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Val Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if val_gen is None:\n",
    "    val_gen = pipeline(coco_val.generator(imgIds=list(balanced_image_ids_val), shuffle_ids=True))\n",
    "\n",
    "# mpl.rcParams['figure.figsize'] = (10,10)\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "plt.rcParams['font.size']=5\n",
    "\n",
    "examples= []\n",
    "width = 5\n",
    "height = 5\n",
    "for i in range(height):\n",
    "    row = []\n",
    "    print(\"- - - Row {} - - - \".format(i+1))\n",
    "    for j in range(width):\n",
    "        data = next(val_gen)\n",
    "        row.append(data)\n",
    "        print(data[0].shape, data[1], (np.min(data[0]), np.max(data[0])))\n",
    "    examples.append(row)\n",
    "    \n",
    "plt.figure()\n",
    "for i in range(height):\n",
    "    for j in range(width):\n",
    "        plt.subplot(height, width, i*width+j+1)\n",
    "        plt.imshow(postprocess(examples[i][j][0]))\n",
    "        plt.title(', '.join([caption_map_r[int(cap_id)] for cap_id in np.argwhere(examples[i][j][1])]), y=0.9)\n",
    "        plt.axis('off')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_instance = \"/home/users/jmc/pide/workspace/test-project/models/5-class/5-class-forward/\"\n",
    "model_instance = \"/mnt/ssd1/processed/industry-data/project-max/ml/cloud-factory-data/models/\"\n",
    "# model_best_weight = \"best.300-0.7164.h5\"\n",
    "# model_initial_learning_rate = 1e-7 ##### IMPORTANT TO SET THIS FROM TENSORBOARD\n",
    "# num_epoch = 200\n",
    "class_weights = None # Can't currently resume training with imbalance data #TODO\n",
    "\n",
    "# model_weights_in_path = os.path.join(search_output_dir, model_instance, \"models\", model_best_weight)\n",
    "# log_dir = os.path.join(search_output_dir, model_instance, \"continued\")\n",
    "# best_path = os.path.join(log_dir, \"models/best.{epoch:03d}-{val_loss:.4f}.h5\")\n",
    "# model_def_path = os.path.join(log_dir, \"model_def.json\")\n",
    "# model_info_path = os.path.join(log_dir, \"model.json\")\n",
    "\n",
    "K.clear_session()\n",
    "model = Inference(os.path.join(model_instance, \"model.json\")).model\n",
    "model.load_weights(os.path.join(model_instance, \"model_weights.h5\"))\n",
    "\n",
    "model.compile( # TODO, load this from JSON, manually change this if you are doing single label\n",
    "    Nadam(clipnorm=1),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy'])\n",
    "\n",
    "print(\"Found model with {:d} layers\".format(len([1 for layer in model.layers if layer.trainable])))\n",
    "# print(log_dir)\n",
    "\n",
    "# callbacks = setup_callbacks(log_dir, hist=None)\n",
    "# K.set_value(model.optimizer.lr, model_initial_learning_rate)\n",
    "# history_data[experiment_name] = go(num_epoch, class_weights, initial_epoch=0)\n",
    "# (Y_true, Y_pred, TP) = evaluate_model(model, test_data, thresh=0.5)\n",
    "# display_performance(Y_true, Y_pred, TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific for the 5->3 example\n",
    "def pool_preds(Y_pred, max_pool = True):\n",
    "    Y_pred_pooled = []\n",
    "    for i in range(len(Y_pred)):\n",
    "        orig = Y_pred[i]\n",
    "        \n",
    "        if max_pool:\n",
    "            out_pool = np.asarray([max(orig[0],orig[2]), orig[4], max(orig[1],orig[3])])\n",
    "        else:\n",
    "            out = [1 if x > 0.5 else 0 for x in orig ]\n",
    "            out_pool = np.asarray([1 if (out[0] or out[2]) else 0, 1 if (out[4]) else 0, 1 if (out[1] or out[3]) else 0 ])\n",
    "        Y_pred_pooled.append(out_pool)\n",
    "    #     print(orig, out, out_pool)\n",
    "\n",
    "    return np.asarray(Y_pred_pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_gen(model, val_gen, thresh=0.5, num=100):\n",
    "    def multi_label_decision(y_true, y_pred):\n",
    "        return [(y_true > thresh) == (y_pred > thresh)]\n",
    "    def single_label_decision(y_true, y_pred):\n",
    "        return np.argmax(y_true, axis=-1) == np.argmax(y_pred, axis=-1)\n",
    "    decision_function = single_label_decision if caption_type == 'single' else multi_label_decision\n",
    "\n",
    "    val_data = gen_dump_data(val_gen,num)\n",
    "    test_data = val_data\n",
    "    Y_true = test_data[1]\n",
    "    print(Y_true.shape)\n",
    "    Y_pred = model.predict(test_data[0])\n",
    "    print(Y_pred.shape)\n",
    "    Y_pred = pool_preds(Y_pred)\n",
    "    print(Y_pred.shape)\n",
    "    TP = decision_function(Y_true, Y_pred)\n",
    "#     print(TP[0], np.asarray(TP[0]).shape)\n",
    "    acc = np.count_nonzero(TP[0]) / TP[0].size\n",
    "    \n",
    "    print(\"Test using {:d} samples:\".format(len(test_data[0])))\n",
    "    print(\"accuracy\", acc)\n",
    "    return Y_true, Y_pred, TP[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_performance(Y_true, Y_pred, TP):\n",
    "    \n",
    "    plt.rcParams['figure.dpi'] = 100\n",
    "    \n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    thresholds = dict()\n",
    "    f1 = dict()\n",
    "    average_precision = dict()\n",
    "    \n",
    "    def f1_score(p,r):\n",
    "        return 2*p*r/(p+r)\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        precision[i], recall[i], thresholds[i] = precision_recall_curve(Y_true[:, i],\n",
    "                                                            Y_pred[:, i])\n",
    "        f1[i] = [f1_score(precision[i][x],recall[i][x]) for x in range(len(precision[i]))]\n",
    "#         print(i, f1[i])\n",
    "        average_precision[i] = average_precision_score(Y_true[:, i], Y_pred[:, i])\n",
    "\n",
    "    # A \"micro-average\": quantifying score on all classes jointly\n",
    "    precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(Y_true.ravel(),\n",
    "        Y_pred.ravel())\n",
    "    average_precision[\"micro\"] = average_precision_score(Y_true, Y_pred,\n",
    "                                                         average=\"micro\")\n",
    "    print('Average precision score, micro-averaged over all classes: {0:0.2f}'\n",
    "          .format(average_precision[\"micro\"]))\n",
    "\n",
    "    z = np.all((Y_pred > 0.5) == Y_true, axis=1)\n",
    "    acc = np.count_nonzero(z) / z.size\n",
    "    print(\"exact accuracy\", acc)\n",
    "    z = ((Y_pred > 0.5) == Y_true)\n",
    "    acc = np.count_nonzero(z) / z.size\n",
    "    print(\"binary accuracy\", acc)\n",
    "    \n",
    "    # setup plot details\n",
    "    colors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal'])\n",
    "\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "    lines = []\n",
    "    labels = []\n",
    "    for f_score in f_scores:\n",
    "        x = np.linspace(0.01, 1)\n",
    "        y = f_score * x / (2 * x - f_score)\n",
    "        l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "        plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "\n",
    "    lines.append(l)\n",
    "    labels.append('iso-f1 curves')\n",
    "    l, = plt.plot(recall[\"micro\"], precision[\"micro\"], color='gold', lw=2)\n",
    "    lines.append(l)\n",
    "    labels.append('micro-average Precision-recall (area = {0:0.2f})'\n",
    "                  ''.format(average_precision[\"micro\"]))\n",
    "\n",
    "    text_disp = 0.01\n",
    "    for i, color in zip(range(num_classes), colors):\n",
    "        l, = plt.plot(recall[i], precision[i], color=color, lw=2)\n",
    "        best_f = f1[i].index(max(f1[i]))\n",
    "#         print(i, best_f, f1[i], max(f1[i]))\n",
    "        plt.plot(recall[i][best_f],precision[i][best_f],'x',color=color)\n",
    "        plt.text(recall[i][best_f]+text_disp,precision[i][best_f]+text_disp,'{0:.2f}'.format(thresholds[i][best_f]))\n",
    "        \n",
    "        lines.append(l)\n",
    "        labels.append('{0} (area = {1:0.2f})'\n",
    "                      ''.format(caption_map_r[i], average_precision[i]))\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    fig.subplots_adjust(bottom=0.25)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Micro Average Precision vs. Recall')\n",
    "    plt.legend(lines, labels, loc=(0, -.4), prop=dict(size=14))\n",
    "    plt.show()\n",
    "#     plt.savefig(model_plot_path, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# plt.plot(5,5,'x',color='gold')\n",
    "# plt.text(5.1,5,'hello')\n",
    "# works "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Y_true, Y_pred, TP) = evaluate_model_gen(model, val_gen, thresh=0.5,num=100)\n",
    "display_performance(Y_true, Y_pred, TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [0.713375796178344, 0.7051282051282052, 0.6967741935483871, 0.6883116883116884, 0.6797385620915032, 0.6842105263157894, 0.6887417218543047, 0.6933333333333335, 0.6979865771812079, 0.7027027027027025, 0.6938775510204082, 0.6986301369863013, 0.7034482758620689, 0.7083333333333334, 0.6993006993006994, 0.6901408450704225, 0.6950354609929078, 0.7000000000000001, 0.7050359712230215, 0.7101449275362318, 0.7153284671532847, 0.7058823529411764, 0.7111111111111111, 0.7164179104477612, 0.706766917293233, 0.7121212121212123, 0.7175572519083969, 0.7076923076923075, 0.7131782945736433, 0.7187499999999999, 0.7244094488188976, 0.73015873015873, 0.7200000000000001, 0.7096774193548386, 0.6991869918699187, 0.7049180327868853, 0.7107438016528925, 0.7, 0.6890756302521008, 0.6949152542372881, 0.6837606837606838, 0.6724137931034482, 0.6608695652173914, 0.6666666666666666, 0.6725663716814159, 0.6607142857142857, 0.6666666666666666, 0.6545454545454545, 0.6605504587155964, 0.6481481481481483, 0.6355140186915887, 0.6415094339622641, 0.6285714285714286, 0.6153846153846153, 0.6213592233009708, 0.6274509803921569, 0.6138613861386139, 0.6, 0.6060606060606061, 0.5918367346938775, 0.5773195876288659, 0.5833333333333334, 0.5894736842105263, 0.5744680851063829, 0.5591397849462365, 0.5652173913043479, 0.5714285714285714, 0.5555555555555557, 0.5393258426966292, 0.5454545454545454, 0.528735632183908, 0.5348837209302325, 0.5176470588235293, 0.5, 0.5060240963855422, 0.48780487804878053, 0.4691358024691359, 0.45000000000000007, 0.430379746835443, 0.43589743589743585, 0.41558441558441556, 0.3947368421052632, 0.4, 0.3783783783783784, 0.35616438356164387, 0.3611111111111111, 0.3380281690140845, 0.3142857142857143, 0.2898550724637681, 0.29411764705882354, 0.2686567164179105, 0.24242424242424243, 0.2153846153846154, 0.1875, 0.15873015873015872, 0.12903225806451613, 0.13114754098360656, 0.13333333333333333, 0.10169491525423728, 0.0689655172413793, 0.03508771929824561, 0.0]\n",
    "len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_predictions(val_gen, thresh = 0.5):\n",
    "    plt.rcParams['figure.dpi'] = 150\n",
    "    plt.rcParams['font.size']=5\n",
    "\n",
    "    examples= []\n",
    "    width = 5\n",
    "    height = 5\n",
    "\n",
    "    val_data = gen_dump_data(val_gen, width*height-1)\n",
    "    test_data = val_data\n",
    "    Y_true = test_data[1]\n",
    "    images = test_data[0]\n",
    "    print(Y_true.shape)\n",
    "    Y_pred = model.predict(test_data[0])\n",
    "    print(Y_pred.shape)\n",
    "    Y_pred = pool_preds(Y_pred)\n",
    "    print(Y_pred.shape)\n",
    "\n",
    "    plt.figure()\n",
    "    for i in range(height):\n",
    "        for j in range(width):\n",
    "\n",
    "            index = i*width+j\n",
    "            image = images[index]\n",
    "            true_caption = Y_true[index]\n",
    "            pred_caption = Y_pred[index]\n",
    "            plt.subplot(height, width, index+1)\n",
    "\n",
    "            plt.imshow(postprocess(image))\n",
    "            plt.title(\"{:s} -> {:s}\".format(\n",
    "                ', '.join([caption_map_r[int(cap_id)] for cap_id in np.argwhere(true_caption > thresh)]),\n",
    "                ', '.join([caption_map_r[int(cap_id)] for cap_id in np.argwhere(pred_caption > thresh)])\n",
    "                ), y=0.9)\n",
    "            plt.axis('off')\n",
    "            \n",
    "compare_predictions(val_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_true = None\n",
    "Y_pred = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_stats(val_gen, num, thresh=0.5):\n",
    "    val_data = gen_dump_data(val_gen,num-1)\n",
    "    test_data = val_data\n",
    "    Y_true = test_data[1]\n",
    "#     images = test_data[0]\n",
    "    print(Y_true.shape)\n",
    "    Y_pred = model.predict(test_data[0])\n",
    "    print(Y_pred.shape)\n",
    "    Y_pred = pool_preds(Y_pred)\n",
    "    print(Y_pred.shape)\n",
    "    \n",
    "    error_dict = {}\n",
    "    \n",
    "    for i,fault in enumerate(caption_map):\n",
    "        error_dict[fault] = {'TP':0,'TN':0,'FP':0,'FN':0}\n",
    "        f = error_dict[fault]\n",
    "        for j in range(num):\n",
    "            if Y_pred[j,i]>thresh:\n",
    "                if Y_true[j,i]:\n",
    "                    f['TP'] += 1\n",
    "                else:\n",
    "                    f['FP'] += 1\n",
    "            else:\n",
    "                if Y_true[j,i]:\n",
    "                    f['FN'] += 1\n",
    "                else:\n",
    "                    f['TN'] += 1\n",
    "                    \n",
    "    \n",
    "    return error_dict\n",
    "        \n",
    "bs = basic_stats(val_gen,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_basic_stats(bs_dict):\n",
    "    for key in bs_dict:\n",
    "#         print(\"___________________________________\")\n",
    "        print(\"{:<8}| {:<15}\".format(key,'Predicted'))\n",
    "        print(\"{:<8}| {:<15} {:<10}\".format('Truth','T','F'))\n",
    "        print(\"________|__________________________\")\n",
    "        print(\"{:<8}| {:<15} {:<10}\".format('T',bs_dict[key]['TP'],bs_dict[key]['FN']))\n",
    "        print(\"{:<8}| {:<15} {:<10}\\n\".format('F',bs_dict[key]['FP'],bs_dict[key]['TN']))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_basic_stats(bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# plt.table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FP_stats(val_gen, num, thresh=0.5, gen_new=False,verbose=False):\n",
    "    if  gen_new or Y_true is None or Y_pred is None:\n",
    "        val_data = gen_dump_data(val_gen,num-1)\n",
    "        test_data = val_data\n",
    "        Y_true = test_data[1]\n",
    "    #     images = test_data[0]\n",
    "        print(Y_true.shape)\n",
    "        Y_pred = model.predict(test_data[0])\n",
    "        print(Y_pred.shape)\n",
    "        Y_pred = pool_preds(Y_pred)\n",
    "        print(Y_pred.shape)\n",
    "    \n",
    "    error_dict = {}\n",
    "    \n",
    "    for i,_ in enumerate(caption_map):\n",
    "        fault = caption_map_r[i]\n",
    "        error_dict[fault] = {}\n",
    "        f = error_dict[fault]\n",
    "        for j in range(num):\n",
    "            if Y_pred[j,i]>thresh and not Y_true[j,i]:\n",
    "                # Is a False Positive\n",
    "                if verbose:\n",
    "                    print(i,fault, Y_pred[j,:],Y_true[j,:],[caption_map_r[int(cap_id)] for cap_id in np.argwhere(Y_true[j,:] > thresh)])\n",
    "                s = ', '.join([caption_map_r[int(cap_id)] for cap_id in np.argwhere(Y_true[j,:] > thresh)]),\n",
    "                if s in f:\n",
    "                    f[s] += 1\n",
    "                else:\n",
    "                    f[s] = 1\n",
    "                    \n",
    "    \n",
    "    return error_dict\n",
    "        \n",
    "fp = FP_stats(val_gen,100,gen_new=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_F_stats(f_dict,error_type='FN'):\n",
    "    for key in f_dict:\n",
    "        sub_keys = f_dict[key].keys()\n",
    "#         print(sub_keys)\n",
    "        sub_keys_str = ''.join(['{:<15}'.format(','.join(x)) for x in sub_keys])\n",
    "#         print(sub_keys_str)\n",
    "        sub_keys_vals = ''.join(['{:<15}'.format(f_dict[key][x]) for x in sub_keys])\n",
    "#         print(sub_keys_vals)\n",
    "        print(\"{:<8}| {}\".format(error_type,sub_keys_str))\n",
    "#         print(\"{:<8}| {:<15} {:<10}\".format('Truth','T','F'))\n",
    "        \n",
    "        print(\"________|{}\".format(''.join(['_______________' for _ in  sub_keys])))\n",
    "        print(\"{:<8}| {}\".format(key,sub_keys_vals))\n",
    "        print(\"\\n\")\n",
    "#         print(\"{:<8}| {:<15} {:<10}\\n\".format('F',bs_dict[key]['FP'],bs_dict[key]['TN']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp['ED'][('JD',)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_F_stats(fp, 'FP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FN_stats(val_gen, num, thresh=0.5,gen_new=False,verbose=False):\n",
    "    if gen_new or Y_true is None or Y_pred is None:\n",
    "        val_data = gen_dump_data(val_gen,num-1)\n",
    "        test_data = val_data\n",
    "        Y_true = test_data[1]\n",
    "    #     images = test_data[0]\n",
    "        print(Y_true.shape)\n",
    "        Y_pred = model.predict(test_data[0])\n",
    "        print(Y_pred.shape)\n",
    "        Y_pred = pool_preds(Y_pred)\n",
    "        print(Y_pred.shape)\n",
    "    \n",
    "    error_dict = {}\n",
    "    \n",
    "    for i,_ in enumerate(caption_map):\n",
    "        fault = caption_map_r[i]\n",
    "        error_dict[fault] = {}\n",
    "        f = error_dict[fault]\n",
    "        for j in range(num):\n",
    "            if Y_pred[j,i]<thresh and Y_true[j,i]:\n",
    "                # Is a False Positive\n",
    "                if verbose:\n",
    "                    print(i,fault, Y_pred[j,:],Y_true[j,:],[caption_map_r[int(cap_id)] for cap_id in np.argwhere(Y_true[j,:] > thresh)])\n",
    "                s = ', '.join([caption_map_r[int(cap_id)] for cap_id in np.argwhere(Y_true[j,:] > thresh)]),\n",
    "                if s in f:\n",
    "                    f[s]+=1\n",
    "                else:\n",
    "                    f[s] = 1\n",
    "                    \n",
    "    \n",
    "    return error_dict\n",
    "        \n",
    "fn = FN_stats(val_gen,100,gen_new=True,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(val_gen, num, thresh = 0.5):\n",
    "    val_data = gen_dump_data(val_gen,num-1)\n",
    "    test_data = val_data\n",
    "    Y_true = test_data[1]\n",
    "#     images = test_data[0]\n",
    "    print(Y_true.shape)\n",
    "    Y_pred = model.predict(test_data[0])\n",
    "    print(Y_pred.shape)\n",
    "    Y_pred = pool_preds(Y_pred)\n",
    "    print(Y_pred.shape)\n",
    "    \n",
    "    error_dict = {}\n",
    "    label_set = set()\n",
    "    for j in range(num):\n",
    "        yp = Y_pred[j,:]\n",
    "        yt = Y_true[j,:]\n",
    "        yp_str = ','.join([caption_map_r[int(cap_id)] for cap_id in np.argwhere(yp > thresh)])\n",
    "        yt_str = ','.join([caption_map_r[int(cap_id)] for cap_id in np.argwhere(yt > thresh)])\n",
    "        print(yp,yt)\n",
    "        print(yp_str,yt_str)\n",
    "        label_set.add(yp_str)\n",
    "        label_set.add(yt_str)\n",
    "        if yp_str in error_dict:\n",
    "            if yt_str in error_dict[yp_str]:\n",
    "                error_dict[yp_str][yt_str] += 1\n",
    "            else:\n",
    "                error_dict[yp_str][yt_str] = 1\n",
    "        else:\n",
    "            error_dict[yp_str] = {yt_str: 1}\n",
    "     \n",
    "    \n",
    "    \n",
    "#         sub_keys = f_dict[key].keys()\n",
    "# #         print(sub_keys)\n",
    "    sub_keys_str = ''.join(['{:<8}'.format(','.join(x)) for x in label_set])\n",
    "#         print(sub_keys_str)\n",
    "        \n",
    "    full_confusion_dict = {}\n",
    "    for key in label_set:\n",
    "        full_confusion_dict[key] = {}\n",
    "        for sub_key in label_set:\n",
    "            full_confusion_dict[key][sub_key] = 0\n",
    "            if key in error_dict and sub_key in error_dict[key]:\n",
    "                full_confusion_dict[key][sub_key] = error_dict[key][sub_key]\n",
    "            \n",
    "    \n",
    "#         print(sub_keys_vals)\n",
    "    print(\"{:<8}| {}\".format(error_type,sub_keys_str))\n",
    "    for key in label_set:\n",
    "        sub_keys_vals = ''.join(['{:<8}'.format(full_confusion_dict[key][x]) for x in label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(val_gen, num, thresh = 0.5,print_text=False):\n",
    "    val_data = gen_dump_data(val_gen,num-1)\n",
    "    test_data = val_data\n",
    "    Y_true = test_data[1]\n",
    "#     images = test_data[0]\n",
    "    print(Y_true.shape)\n",
    "    Y_pred = model.predict(test_data[0])\n",
    "    print(Y_pred.shape)\n",
    "    Y_pred = pool_preds(Y_pred)\n",
    "    print(Y_pred.shape)\n",
    "    \n",
    "    error_dict = {}\n",
    "    label_set = set()\n",
    "    for j in range(num):\n",
    "        yp = Y_pred[j,:]\n",
    "        yt = Y_true[j,:]\n",
    "        yp_str = ','.join([caption_map_r[int(cap_id)] for cap_id in np.argwhere(yp > thresh)])\n",
    "        yt_str = ','.join([caption_map_r[int(cap_id)] for cap_id in np.argwhere(yt > thresh)])\n",
    "#         yp_str = yp_str if yp_str != '' else 'none'\n",
    "#         yt_str = yt_str if yt_str != '' else 'none'\n",
    "#         print(yp,yt)\n",
    "#         print(yp_str,yt_str)\n",
    "        label_set.add(yp_str)\n",
    "        label_set.add(yt_str)\n",
    "        if yp_str in error_dict:\n",
    "            if yt_str in error_dict[yp_str]:\n",
    "                error_dict[yp_str][yt_str] += 1\n",
    "            else:\n",
    "                error_dict[yp_str][yt_str] = 1\n",
    "        else:\n",
    "            error_dict[yp_str] = {yt_str: 1}\n",
    "     \n",
    "    \n",
    "    \n",
    "#         sub_keys = f_dict[key].keys()\n",
    "# #         print(sub_keys)\n",
    "    sub_keys_str = ''.join(['{:<15}'.format(x) for x in label_set])\n",
    "#         print(sub_keys_str)\n",
    "        \n",
    "    full_confusion_dict = {}\n",
    "    for key in label_set:\n",
    "        full_confusion_dict[key] = {}\n",
    "        for sub_key in label_set:\n",
    "            full_confusion_dict[key][sub_key] = 0\n",
    "            if key in error_dict and sub_key in error_dict[key]:\n",
    "                full_confusion_dict[key][sub_key] = error_dict[key][sub_key]\n",
    "            \n",
    "    \n",
    "    if print_text:\n",
    "        print(\"{:<15}| {}\".format('Confuse',sub_keys_str))\n",
    "        print(\"_______________|{}\".format(''.join(['_______________' for _ in  label_set])))\n",
    "        for key in label_set:\n",
    "            sub_keys_vals = ''.join(['{:<15}'.format(full_confusion_dict[key][x]) for x in label_set])\n",
    "            print(\"{:<15}| {}\".format(key,sub_keys_vals))\n",
    "        print(\"\\n\")\n",
    "    df = pd.DataFrame.from_dict(full_confusion_dict)            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(val_gen, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_F_stats(fn, 'FN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if val_gen is None:\n",
    "    val_gen = pipeline(coco_val.generator(imgIds=list(balanced_image_ids_val), shuffle_ids=True))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Y_true, Y_pred, TP) = evaluate_model_gen(model, val_gen, thresh=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_performance(Y_true, Y_pred, TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
